"""
NotebookLMìš© URL ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ì±… ì œëª©ì„ ë°›ì•„ì„œ í•œê¸€/ì˜ì–´ ìë£Œë¥¼ ë°˜ë°˜ì”© ìˆ˜ì§‘í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
from typing import List, Dict, Tuple
from pathlib import Path

# src ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from duckduckgo_search import DDGS
from bs4 import BeautifulSoup
import requests
from dotenv import load_dotenv

# YouTube API import
try:
    from googleapiclient.discovery import build
    YOUTUBE_API_AVAILABLE = True
except ImportError:
    YOUTUBE_API_AVAILABLE = False

load_dotenv()

class NotebookLMURLCollector:
    """NotebookLMìš© URL ìˆ˜ì§‘ í´ë˜ìŠ¤ (í•œê¸€/ì˜ì–´ ë¶„ë¦¬ ìˆ˜ì§‘)"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.urls = []
        self.ddgs = DDGS()  # DuckDuckGo ê²€ìƒ‰ ì¸ìŠ¤í„´ìŠ¤
        
        # YouTube API ì´ˆê¸°í™”
        self.youtube = None
        if YOUTUBE_API_AVAILABLE:
            youtube_api_key = os.getenv("YOUTUBE_API_KEY") or os.getenv("GOOGLE_BOOKS_API_KEY")
            if youtube_api_key:
                try:
                    self.youtube = build('youtube', 'v3', developerKey=youtube_api_key)
                    print("âœ… YouTube API ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    print(f"âš ï¸ YouTube API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def search_urls_bilingual(
        self, 
        book_title: str, 
        author: str = None, 
        total_results: int = 30,
        en_title: str = None
    ) -> Tuple[List[str], List[str]]:
        """
        í•œê¸€/ì˜ì–´ ìë£Œë¥¼ ë°˜ë°˜ì”© ìˆ˜ì§‘
        
        Args:
            book_title: ì±… ì œëª© (í•œê¸€)
            author: ì €ì ì´ë¦„
            total_results: ì´ ìˆ˜ì§‘í•  URL ê°œìˆ˜
            translate_title: ì˜ì–´ ì œëª© ìë™ ë²ˆì—­ ì—¬ë¶€
            
        Returns:
            (í•œê¸€_urls, ì˜ì–´_urls) íŠœí”Œ
        """
        ko_count = total_results // 2
        en_count = total_results - ko_count
        
        print(f"ğŸ” '{book_title}' ê´€ë ¨ URL ìˆ˜ì§‘ ì¤‘...")
        print(f"   ëª©í‘œ: í•œê¸€ {ko_count}ê°œ + ì˜ì–´ {en_count}ê°œ = ì´ {total_results}ê°œ\n")
        
        # ì˜ì–´ ì œëª© ì„¤ì •
        if not en_title:
            en_title = self._translate_title(book_title)
        
        # í•œê¸€ URL ìˆ˜ì§‘
        print("=" * 60)
        print("ğŸ“š í•œê¸€ ìë£Œ ìˆ˜ì§‘ ì¤‘...")
        print("=" * 60)
        ko_urls = self._search_korean_urls(book_title, author, ko_count)
        
        # ì˜ì–´ URL ìˆ˜ì§‘
        print("\n" + "=" * 60)
        print("ğŸ“š English Resources Collection...")
        print("=" * 60)
        en_urls = self._search_english_urls(book_title, author, en_count, en_title)
        
        print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ:")
        print(f"   í•œê¸€: {len(ko_urls)}ê°œ")
        print(f"   ì˜ì–´: {len(en_urls)}ê°œ")
        print(f"   ì´ê³„: {len(ko_urls) + len(en_urls)}ê°œ\n")
        
        return ko_urls, en_urls
    
    def _search_korean_urls(self, book_title: str, author: str = None, num_results: int = 15) -> List[str]:
        """í•œê¸€ ìë£Œ ìˆ˜ì§‘"""
        query = book_title
        if author:
            query = f"{book_title} {author}"
        
        all_urls = []
        
        # YouTube ì§ì ‘ ê²€ìƒ‰ (ìš°ì„ )
        youtube_urls = self._search_youtube_direct(query, book_title, author, max_results=5)
        all_urls.extend(youtube_urls)
        
        # ë‚˜ë¨¸ì§€ ì†ŒìŠ¤ëŠ” DuckDuckGoë¡œ ê²€ìƒ‰
        search_queries = [
            f"{query} site:ko.wikipedia.org",  # ìœ„í‚¤ë°±ê³¼
            f"{query} site:kyobobook.co.kr",   # êµë³´ë¬¸ê³ 
            f"{query} site:yes24.com",         # ì˜ˆìŠ¤24
            f"{query} site:aladin.co.kr",      # ì•Œë¼ë”˜
            f"{query} site:hani.co.kr ë¦¬ë·°",   # í•œê²¨ë ˆ
            f"{query} site:khan.co.kr ë¦¬ë·°",   # ê²½í–¥ì‹ ë¬¸
            f"{query} site:joongang.co.kr ë¦¬ë·°", # ì¤‘ì•™ì¼ë³´
            f"{query} ì„œí‰ ë¦¬ë·°",              # ì¼ë°˜ ì„œí‰/ë¦¬ë·°
            f"{query} ì±… ì†Œê°œ",                # ì±… ì†Œê°œ
            f"{query} ì‘ê°€ ì¸í„°ë·°",            # ì‘ê°€ ì¸í„°ë·°
            f"{query} ê°•ì˜",                   # ê°•ì˜/ê°•ì—°
            f"{query} íŒŸìºìŠ¤íŠ¸",               # íŒŸìºìŠ¤íŠ¸
        ]
        
        remaining_count = max(0, num_results - len(all_urls))
        if remaining_count > 0:
            other_urls = self._execute_search(search_queries, remaining_count, lang='ko')
            all_urls.extend(other_urls)
        
        return all_urls[:num_results]
    
    def _search_youtube_direct(self, query: str, book_title: str, author: str = None, max_results: int = 5, region: str = 'KR') -> List[str]:
        """YouTube Data APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ê²€ìƒ‰"""
        if not self.youtube:
            # YouTube APIê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return []
        
        youtube_urls = []
        seen_video_ids = set()
        
        # YouTube ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        youtube_queries = [
            f"{query} ë¦¬ë·°",
            f"{query} ì„œí‰",
            f"{query} ì±…",
            f"{query} ì‘ê°€ ì¸í„°ë·°",
            f"{query} ê°•ì˜",
        ]
        
        try:
            for search_query in youtube_queries:
                if len(youtube_urls) >= max_results:
                    break
                
                try:
                    search_response = self.youtube.search().list(
                        q=search_query,
                        part='id,snippet',
                        type='video',
                        maxResults=3,
                        order='relevance',
                        regionCode=region
                    ).execute()
                    
                    for item in search_response.get('items', []):
                        if len(youtube_urls) >= max_results:
                            break
                        
                        video_id = item['id']['videoId']
                        if video_id not in seen_video_ids:
                            seen_video_ids.add(video_id)
                            video_url = f"https://www.youtube.com/watch?v={video_id}"
                            youtube_urls.append(video_url)
                            print(f"    âœ“ YouTube: {item['snippet']['title'][:50]}...")
                    
                    time.sleep(0.5)  # API ì œí•œ ë°©ì§€
                    
                except Exception as e:
                    print(f"  âš ï¸ YouTube ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                    continue
                    
        except Exception as e:
            print(f"  âš ï¸ YouTube API ì˜¤ë¥˜: {e}")
        
        return youtube_urls
    
    def _translate_title(self, book_title: str) -> str:
        """ì±… ì œëª©ì„ ì˜ì–´ë¡œ ë²ˆì—­ (ê°„ë‹¨í•œ ë§¤í•‘)"""
        translations = {
            "ë…¸ë¥´ì›¨ì´ì˜ ìˆ²": "Norwegian Wood",
            "ë…¸ë¥´ì›¨ì´ì˜_ìˆ²": "Norwegian Wood",
            "1984": "1984",
            "ì‚¬í”¼ì—”ìŠ¤": "Sapiens",
            "21ì„¸ê¸°ë¥¼ ìœ„í•œ 21ê°€ì§€ ì œì–¸": "21 Lessons for the 21st Century",
            "í˜¸ëª¨ ë°ìš°ìŠ¤": "Homo Deus",
            "1Q84": "1Q84",
            "í•´ë³€ì˜ ì¹´í”„ì¹´": "Kafka on the Shore",
            "ë™ë¬¼ë†ì¥": "Animal Farm",
            "ë…¸ì¸ê³¼ ë°”ë‹¤": "The Old Man and the Sea",
            "ìœ„ëŒ€í•œ ê°œì¸ ë¹„": "The Great Gatsby",
            "í˜¸ë°€ë°­ì˜ íŒŒìˆ˜ê¾¼": "The Catcher in the Rye",
            "ì•µë¬´ìƒˆ ì£½ì´ê¸°": "To Kill a Mockingbird",
            "ì˜¤ë§Œê³¼ í¸ê²¬": "Pride and Prejudice",
            "ì „ìŸê³¼ í‰í™”": "War and Peace",
            "ì•ˆë‚˜ ì¹´ë ˆë‹ˆë‚˜": "Anna Karenina",
        }
        # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜í•œ ë²„ì „ë„ í™•ì¸
        book_title_underscore = book_title.replace(' ', '_')
        if book_title_underscore in translations:
            return translations[book_title_underscore]
        # ë²ˆì—­ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜ (ì£¼ì œì˜ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        return translations.get(book_title, book_title)
    
    def _search_english_urls(self, book_title: str, author: str = None, num_results: int = 15, en_title: str = None) -> List[str]:
        """ì˜ì–´ ìë£Œ ìˆ˜ì§‘"""
        # ì˜ì–´ ì œëª© ë³€í™˜
        if not en_title:
            en_title = self._translate_title(book_title)
        en_query = en_title
        if author:
            en_query = f"{en_title} {author}"
        
        all_urls = []
        
        # YouTube ì§ì ‘ ê²€ìƒ‰ (ìš°ì„ )
        youtube_urls = self._search_youtube_direct(en_query, en_title, author, max_results=5, region='US')
        all_urls.extend(youtube_urls)
        
        # ë‚˜ë¨¸ì§€ ì†ŒìŠ¤ëŠ” DuckDuckGoë¡œ ê²€ìƒ‰
        search_queries = [
            f"{en_query} site:en.wikipedia.org",  # Wikipedia
            f"{en_query} site:goodreads.com",     # Goodreads
            f"{en_query} site:amazon.com review", # Amazon reviews
            f"{en_query} site:nytimes.com review", # NY Times
            f"{en_query} site:theguardian.com review", # The Guardian
            f"{en_query} book review",              # General reviews
            f"{en_query} book summary",            # Book summary
            f"{en_query} author interview",        # Author interview
            f"{en_query} lecture",                  # Lecture
            f"{en_query} podcast",                  # Podcast
            f"{en_query} analysis",                 # Analysis
            f"{en_query} discussion",               # Discussion
        ]
        
        remaining_count = max(0, num_results - len(all_urls))
        if remaining_count > 0:
            other_urls = self._execute_search(search_queries, remaining_count, lang='en')
            all_urls.extend(other_urls)
        
        return all_urls[:num_results]
    
    def _execute_search(self, search_queries: List[str], num_results: int, lang: str = 'ko') -> List[str]:
        """ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ (DuckDuckGo ì‚¬ìš©)"""
        all_urls = []
        seen_urls = set()
        
        for search_query in search_queries:
            if len(all_urls) >= num_results:
                break
                
            try:
                print(f"  ê²€ìƒ‰ ì¤‘: {search_query[:60]}...")
                # DuckDuckGo ê²€ìƒ‰ (ê° ì¿¼ë¦¬ë‹¹ ìµœëŒ€ 5ê°œ ê²°ê³¼)
                results = list(self.ddgs.text(search_query, max_results=5))
                
                for result in results:
                    if len(all_urls) >= num_results:
                        break
                    url = result.get('href', '')
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        all_urls.append(url)
                        print(f"    âœ“ {url}")
                
                time.sleep(1)  # ìš”ì²­ ê°„ ëŒ€ê¸° (DuckDuckGoëŠ” ëœ ì—„ê²©í•¨)
                
            except Exception as e:
                print(f"  âš ï¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        return all_urls[:num_results]
    
    def search_urls(self, book_title: str, author: str = None, num_results: int = 25) -> List[str]:
        """
        ì±… ê´€ë ¨ URL ìˆ˜ì§‘ (YouTube ì˜ìƒ í¬í•¨)
        
        Args:
            book_title: ì±… ì œëª©
            author: ì €ì ì´ë¦„
            num_results: ìˆ˜ì§‘í•  URL ê°œìˆ˜
        """
        query = book_title
        if author:
            query = f"{book_title} {author}"
        
        print(f"ğŸ” '{book_title}' ê´€ë ¨ URL ìˆ˜ì§‘ ì¤‘...")
        print(f"   ëª©í‘œ: {num_results}ê°œ ì´ìƒ\n")
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (YouTube í¬í•¨)
        search_queries = [
            f"{query} site:youtube.com",  # YouTube ì˜ìƒ
            f"{query} site:youtu.be",     # YouTube ë‹¨ì¶• URL
            f"{query} site:ko.wikipedia.org",  # ìœ„í‚¤ë°±ê³¼
            f"{query} site:kyobobook.co.kr",   # êµë³´ë¬¸ê³ 
            f"{query} site:yes24.com",         # ì˜ˆìŠ¤24
            f"{query} site:aladin.co.kr",      # ì•Œë¼ë”˜
            f"{query} site:hani.co.kr ë¦¬ë·°",   # í•œê²¨ë ˆ
            f"{query} site:khan.co.kr ë¦¬ë·°",   # ê²½í–¥ì‹ ë¬¸
            f"{query} site:joongang.co.kr ë¦¬ë·°", # ì¤‘ì•™ì¼ë³´
            f"{query} ì„œí‰ ë¦¬ë·°",              # ì¼ë°˜ ì„œí‰/ë¦¬ë·°
            f"{query} ì±… ì†Œê°œ",                # ì±… ì†Œê°œ
            f"{query} ì‘ê°€ ì¸í„°ë·°",            # ì‘ê°€ ì¸í„°ë·°
            f"{query} ê°•ì˜",                   # ê°•ì˜/ê°•ì—°
            f"{query} íŒŸìºìŠ¤íŠ¸",               # íŒŸìºìŠ¤íŠ¸
        ]
        
        all_urls = []
        seen_urls = set()
        
        for search_query in search_queries:
            try:
                print(f"  ê²€ìƒ‰ ì¤‘: {search_query[:50]}...")
                # DuckDuckGo ê²€ìƒ‰
                results = list(self.ddgs.text(search_query, max_results=5))
                
                for result in results:
                    url = result.get('href', '')
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        all_urls.append(url)
                        print(f"    âœ“ {url}")
                
                time.sleep(1)  # ìš”ì²­ ê°„ ëŒ€ê¸°
                
            except Exception as e:
                print(f"  âš ï¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"\nâœ… ì´ {len(all_urls)}ê°œì˜ URLì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n")
        return all_urls[:num_results]
    
    def validate_url(self, url: str) -> Dict[str, any]:
        """URL ìœ íš¨ì„± ê²€ì¦"""
        try:
            response = self.session.get(url, timeout=10, allow_redirects=True)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            title = None
            if soup.title:
                title = soup.title.string.strip()
            elif soup.find('meta', property='og:title'):
                title = soup.find('meta', property='og:title')['content']
            
            description = None
            if soup.find('meta', property='og:description'):
                description = soup.find('meta', property='og:description')['content']
            elif soup.find('meta', attrs={'name': 'description'}):
                description = soup.find('meta', attrs={'name': 'description'})['content']
            
            return {
                'url': url,
                'valid': True,
                'title': title,
                'description': description,
                'status_code': response.status_code
            }
        except Exception as e:
            return {
                'url': url,
                'valid': False,
                'error': str(e)
            }
    
    def save_urls_bilingual(
        self, 
        book_title: str, 
        ko_urls: List[str], 
        en_urls: List[str],
        author: str = None,
        validate: bool = False
    ) -> Dict[str, str]:
        """
        í•œê¸€/ì˜ì–´ URLì„ íŒŒì¼ë¡œ ì €ì¥ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹ í¬í•¨)
        
        Args:
            book_title: ì±… ì œëª©
            ko_urls: í•œê¸€ URL ë¦¬ìŠ¤íŠ¸
            en_urls: ì˜ì–´ URL ë¦¬ìŠ¤íŠ¸
            author: ì €ì ì´ë¦„
            validate: URL ìœ íš¨ì„± ê²€ì¦ ì—¬ë¶€
        """
        safe_title = "".join(c for c in book_title if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_title = safe_title.replace(' ', '_')
        
        output_dir = Path("assets/urls")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        md_path = output_dir / f"{safe_title}_notebooklm.md"
        
        total_urls = ko_urls + en_urls
        
        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ (í•œê¸€/ì˜ì–´ ì„¹ì…˜ êµ¬ë¶„)
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# {book_title} - NotebookLM ì†ŒìŠ¤ URL\n\n")
            if author:
                f.write(f"**ì±…**: {book_title}  \n")
                f.write(f"**ì €ì**: {author}  \n")
            f.write(f"**ì´ {len(total_urls)}ê°œì˜ URL (í•œê¸€ {len(ko_urls)}ê°œ + ì˜ì–´ {len(en_urls)}ê°œ)**\n\n")
            f.write("> âœ… **í•œê¸€/ì˜ì–´ ìë£Œ ë°˜ë°˜ ìˆ˜ì§‘**: í•œê¸€ ìë£Œì™€ ì˜ì–´ ìë£Œë¥¼ ê· í˜•ìˆê²Œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.  \n")
            f.write("> âœ… **ë‹¤ì–‘í•œ ì†ŒìŠ¤**: ìœ„í‚¤ë°±ê³¼, ì˜¨ë¼ì¸ ì„œì , ë‰´ìŠ¤ ë¦¬ë·°, YouTube ì˜ìƒ ë“± ë‹¤ì–‘í•œ ì¶œì²˜ë¥¼ í¬í•¨í–ˆìŠµë‹ˆë‹¤.\n\n")
            
            f.write("## ğŸ“‹ URL ë¦¬ìŠ¤íŠ¸\n\n")
            f.write("ì•„ë˜ URLë“¤ì„ ë³µì‚¬í•˜ì—¬ NotebookLMì— ì†ŒìŠ¤ë¡œ ì¶”ê°€í•˜ì„¸ìš”.\n\n")
            
            # í•œê¸€ ì„¹ì…˜
            f.write("### ğŸ“š í•œê¸€ ìë£Œ\n\n")
            f.write("```\n")
            for url in ko_urls:
                f.write(f"{url}\n")
            f.write("```\n\n")
            
            # ì˜ì–´ ì„¹ì…˜
            f.write("### ğŸ“š English Resources\n\n")
            f.write("```\n")
            for url in en_urls:
                f.write(f"{url}\n")
            f.write("```\n\n")
            
            # ì „ì²´ URL (ë³µì‚¬ìš©)
            f.write("### ğŸ“‹ ì „ì²´ URL (ë³µì‚¬ìš©)\n\n")
            f.write("```\n")
            for url in total_urls:
                f.write(f"{url}\n")
            f.write("```\n\n")
            
            f.write("## ğŸ“ ì‚¬ìš© ë°©ë²•\n\n")
            f.write("1. ìœ„ URL ë¸”ë¡ì„ ì „ì²´ ì„ íƒ (Cmd+A / Ctrl+A)\n")
            f.write("2. ë³µì‚¬ (Cmd+C / Ctrl+C)\n")
            f.write("3. NotebookLMì—ì„œ 'ì†ŒìŠ¤ ì¶”ê°€' > 'URL' ì„ íƒ\n")
            f.write("4. ë¶™ì—¬ë„£ê¸° (ê° URLì´ ìë™ìœ¼ë¡œ ì¸ì‹ë©ë‹ˆë‹¤)\n\n")
            f.write("ğŸ’¡ **íŒ**: í•œê¸€ ìë£Œì™€ ì˜ì–´ ìë£Œë¥¼ ëª¨ë‘ í¬í•¨í•˜ë©´ ë” í’ë¶€í•œ ì½˜í…ì¸ ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n")
        
        print(f"\nğŸ’¾ URL ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤:")
        print(f"   - MD (NotebookLM ë³µì‚¬ìš©): {md_path}")
        
        return {'md_path': str(md_path)}
    
    def save_urls(self, book_title: str, urls: List[str], validate: bool = False) -> Dict[str, str]:
        """
        URLì„ íŒŒì¼ë¡œ ì €ì¥ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹ í¬í•¨) - ê¸°ì¡´ ë©”ì„œë“œ (í•˜ìœ„ í˜¸í™˜ì„±)
        
        Args:
            book_title: ì±… ì œëª©
            urls: URL ë¦¬ìŠ¤íŠ¸
            validate: URL ìœ íš¨ì„± ê²€ì¦ ì—¬ë¶€
        """
        safe_title = "".join(c for c in book_title if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_title = safe_title.replace(' ', '_')
        
        output_dir = Path("assets/urls")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        md_path = output_dir / f"{safe_title}_notebooklm.md"
        
        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ (NotebookLM ë³µì‚¬ìš©)
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# {book_title} - NotebookLM ì†ŒìŠ¤ URL\n\n")
            f.write(f"**ì´ {len(urls)}ê°œì˜ URL**\n\n")
            f.write("## ğŸ“‹ URL ë¦¬ìŠ¤íŠ¸\n\n")
            f.write("ì•„ë˜ URLë“¤ì„ ë³µì‚¬í•˜ì—¬ NotebookLMì— ì†ŒìŠ¤ë¡œ ì¶”ê°€í•˜ì„¸ìš”.\n\n")
            f.write("```\n")
            for i, url in enumerate(urls, 1):
                f.write(f"{url}\n")
            f.write("```\n\n")
            f.write("## ğŸ“ ì‚¬ìš© ë°©ë²•\n\n")
            f.write("1. ìœ„ URL ë¸”ë¡ì„ ì „ì²´ ì„ íƒ (Cmd+A / Ctrl+A)\n")
            f.write("2. ë³µì‚¬ (Cmd+C / Ctrl+C)\n")
            f.write("3. NotebookLMì—ì„œ 'ì†ŒìŠ¤ ì¶”ê°€' > 'URL' ì„ íƒ\n")
            f.write("4. ë¶™ì—¬ë„£ê¸° (ê° URLì´ ìë™ìœ¼ë¡œ ì¸ì‹ë©ë‹ˆë‹¤)\n\n")
        
        print(f"\nğŸ’¾ URL ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤:")
        print(f"   - MD (NotebookLM ë³µì‚¬ìš©): {md_path}")
        
        return {'md_path': str(md_path)}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='NotebookLMìš© URL ìˆ˜ì§‘ (í•œê¸€/ì˜ì–´ ë°˜ë°˜)')
    parser.add_argument('--title', type=str, help='ì±… ì œëª© (í•œê¸€)')
    parser.add_argument('--author', type=str, help='ì €ì ì´ë¦„')
    parser.add_argument('--en-title', type=str, help='ì±… ì œëª© (ì˜ì–´, ì„ íƒì‚¬í•­ - ìë™ ë²ˆì—­ ì‹œë„)')
    parser.add_argument('--num', type=int, default=30, help='ì´ ìˆ˜ì§‘í•  URL ê°œìˆ˜ (ê¸°ë³¸ê°’: 30, í•œê¸€/ì˜ì–´ ë°˜ë°˜)')
    parser.add_argument('--validate', action='store_true', help='URL ìœ íš¨ì„± ê²€ì¦ ìˆ˜í–‰')
    parser.add_argument('--bilingual', action='store_true', default=True, help='í•œê¸€/ì˜ì–´ ë°˜ë°˜ ìˆ˜ì§‘ (ê¸°ë³¸ê°’: True)')
    
    args = parser.parse_args()
    
    collector = NotebookLMURLCollector()
    
    if args.title:
        book_title = args.title
        author = args.author
        en_title = args.en_title
    else:
        # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
        print("=" * 60)
        print("ğŸ“š NotebookLMìš© URL ìˆ˜ì§‘ê¸° (í•œê¸€/ì˜ì–´ ë°˜ë°˜)")
        print("=" * 60)
        print()
        
        book_title = input("ì±… ì œëª©ì„ ì…ë ¥í•˜ì„¸ìš” (í•œê¸€): ").strip()
        if not book_title:
            print("âŒ ì±… ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        author = input("ì €ì ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒì‚¬í•­): ").strip() or None
        en_title = input("ì±… ì œëª© (ì˜ì–´, ì„ íƒì‚¬í•­ - ìë™ ë²ˆì—­ ì‹œë„): ").strip() or None
        num_results = input("ì´ ìˆ˜ì§‘í•  URL ê°œìˆ˜ (ê¸°ë³¸ê°’: 30, í•œê¸€/ì˜ì–´ ë°˜ë°˜): ").strip()
        args.num = int(num_results) if num_results.isdigit() else 30
        args.validate = input("URL ìœ íš¨ì„± ê²€ì¦ì„ ìˆ˜í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: n): ").strip().lower() == 'y'
        args.bilingual = input("í•œê¸€/ì˜ì–´ ë°˜ë°˜ ìˆ˜ì§‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: y): ").strip().lower() != 'n'
    
    print()
    
    # URL ìˆ˜ì§‘
    if args.bilingual:
        # í•œê¸€/ì˜ì–´ ë°˜ë°˜ ìˆ˜ì§‘
        en_title = args.en_title if hasattr(args, 'en_title') and args.en_title else None
        ko_urls, en_urls = collector.search_urls_bilingual(book_title, author, args.num, en_title=en_title)
        
        if ko_urls or en_urls:
            # URL ì €ì¥
            result = collector.save_urls_bilingual(
                book_title, 
                ko_urls, 
                en_urls, 
                author=author,
                validate=args.validate
            )
            
            print()
            print("=" * 60)
            print("âœ… URL ìˆ˜ì§‘ ì™„ë£Œ!")
            print("=" * 60)
            print(f"\nğŸ“„ NotebookLMìš© íŒŒì¼: {result['md_path']}")
            print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
            print(f"   1. {result['md_path']} íŒŒì¼ì„ ì—½ë‹ˆë‹¤")
            print(f"   2. 'ì „ì²´ URL (ë³µì‚¬ìš©)' ì„¹ì…˜ì˜ URLë“¤ì„ ë³µì‚¬í•©ë‹ˆë‹¤")
            print(f"   3. NotebookLM (https://notebooklm.google.com)ì— ì ‘ì†í•©ë‹ˆë‹¤")
            print(f"   4. ìƒˆ ì†ŒìŠ¤ ì¶”ê°€ > URLì—ì„œ ë¶™ì—¬ë„£ê¸°í•©ë‹ˆë‹¤")
            print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
            print(f"   - í•œê¸€ ìë£Œ: {len(ko_urls)}ê°œ")
            print(f"   - ì˜ì–´ ìë£Œ: {len(en_urls)}ê°œ")
            print(f"   - ì´ê³„: {len(ko_urls) + len(en_urls)}ê°œ")
            print()
        else:
            print("âŒ ìˆ˜ì§‘ëœ URLì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ê¸°ì¡´ ë°©ì‹ (í•œê¸€ë§Œ)
        urls = collector.search_urls(book_title, author, args.num)
        
        if urls:
            # URL ì €ì¥
            result = collector.save_urls(book_title, urls, validate=args.validate)
            
            print()
            print("=" * 60)
            print("âœ… URL ìˆ˜ì§‘ ì™„ë£Œ!")
            print("=" * 60)
            print(f"\nğŸ“„ NotebookLMìš© íŒŒì¼: {result['md_path']}")
            print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
            print(f"   1. {result['md_path']} íŒŒì¼ì„ ì—½ë‹ˆë‹¤")
            print(f"   2. URLë“¤ì„ ë³µì‚¬í•©ë‹ˆë‹¤")
            print(f"   3. NotebookLM (https://notebooklm.google.com)ì— ì ‘ì†í•©ë‹ˆë‹¤")
            print(f"   4. ìƒˆ ì†ŒìŠ¤ ì¶”ê°€ > URLì—ì„œ ë¶™ì—¬ë„£ê¸°í•©ë‹ˆë‹¤")
            print()
        else:
            print("âŒ ìˆ˜ì§‘ëœ URLì´ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

