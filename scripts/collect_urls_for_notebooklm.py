"""
NotebookLMìš© URL ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ì±… ì œëª©ì„ ë°›ì•„ì„œ YouTube ì˜ìƒ í¬í•¨ 20ê°œ ì´ìƒì˜ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
from typing import List, Dict
from pathlib import Path

# src ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from googlesearch import search
from bs4 import BeautifulSoup
import requests
from dotenv import load_dotenv

load_dotenv()

class NotebookLMURLCollector:
    """NotebookLMìš© URL ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.urls = []
    
    def search_urls(self, book_title: str, author: str = None, num_results: int = 25) -> List[str]:
        """
        ì±… ê´€ë ¨ URL ìˆ˜ì§‘ (YouTube ì˜ìƒ í¬í•¨)
        
        Args:
            book_title: ì±… ì œëª©
            author: ì €ì ì´ë¦„
            num_results: ìˆ˜ì§‘í•  URL ê°œìˆ˜
        """
        query = book_title
        if author:
            query = f"{book_title} {author}"
        
        print(f"ğŸ” '{book_title}' ê´€ë ¨ URL ìˆ˜ì§‘ ì¤‘...")
        print(f"   ëª©í‘œ: {num_results}ê°œ ì´ìƒ\n")
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (YouTube í¬í•¨)
        search_queries = [
            f"{query} site:youtube.com",  # YouTube ì˜ìƒ
            f"{query} site:youtu.be",     # YouTube ë‹¨ì¶• URL
            f"{query} site:ko.wikipedia.org",  # ìœ„í‚¤ë°±ê³¼
            f"{query} site:kyobobook.co.kr",   # êµë³´ë¬¸ê³ 
            f"{query} site:yes24.com",         # ì˜ˆìŠ¤24
            f"{query} site:aladin.co.kr",      # ì•Œë¼ë”˜
            f"{query} site:hani.co.kr ë¦¬ë·°",   # í•œê²¨ë ˆ
            f"{query} site:khan.co.kr ë¦¬ë·°",   # ê²½í–¥ì‹ ë¬¸
            f"{query} site:joongang.co.kr ë¦¬ë·°", # ì¤‘ì•™ì¼ë³´
            f"{query} ì„œí‰ ë¦¬ë·°",              # ì¼ë°˜ ì„œí‰/ë¦¬ë·°
            f"{query} ì±… ì†Œê°œ",                # ì±… ì†Œê°œ
            f"{query} ì‘ê°€ ì¸í„°ë·°",            # ì‘ê°€ ì¸í„°ë·°
            f"{query} ê°•ì˜",                   # ê°•ì˜/ê°•ì—°
            f"{query} íŒŸìºìŠ¤íŠ¸",               # íŒŸìºìŠ¤íŠ¸
        ]
        
        all_urls = []
        seen_urls = set()
        
        for search_query in search_queries:
            try:
                print(f"  ê²€ìƒ‰ ì¤‘: {search_query[:50]}...")
                results = search(search_query, num_results=5, lang='ko')
                
                for url in results:
                    if url not in seen_urls:
                        seen_urls.add(url)
                        all_urls.append(url)
                        print(f"    âœ“ {url}")
                
                time.sleep(2)  # ìš”ì²­ ê°„ ëŒ€ê¸°
                
            except Exception as e:
                print(f"  âš ï¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"\nâœ… ì´ {len(all_urls)}ê°œì˜ URLì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\n")
        return all_urls[:num_results]
    
    def validate_url(self, url: str) -> Dict[str, any]:
        """URL ìœ íš¨ì„± ê²€ì¦"""
        try:
            response = self.session.get(url, timeout=10, allow_redirects=True)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            title = None
            if soup.title:
                title = soup.title.string.strip()
            elif soup.find('meta', property='og:title'):
                title = soup.find('meta', property='og:title')['content']
            
            description = None
            if soup.find('meta', property='og:description'):
                description = soup.find('meta', property='og:description')['content']
            elif soup.find('meta', attrs={'name': 'description'}):
                description = soup.find('meta', attrs={'name': 'description'})['content']
            
            return {
                'url': url,
                'valid': True,
                'title': title,
                'description': description,
                'status_code': response.status_code
            }
        except Exception as e:
            return {
                'url': url,
                'valid': False,
                'error': str(e)
            }
    
    def save_urls(self, book_title: str, urls: List[str], validate: bool = False) -> Dict[str, str]:
        """
        URLì„ íŒŒì¼ë¡œ ì €ì¥ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹ í¬í•¨)
        
        Args:
            book_title: ì±… ì œëª©
            urls: URL ë¦¬ìŠ¤íŠ¸
            validate: URL ìœ íš¨ì„± ê²€ì¦ ì—¬ë¶€
        """
        safe_title = "".join(c for c in book_title if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_title = safe_title.replace(' ', '_')
        
        output_dir = Path("assets/urls")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        txt_path = output_dir / f"{safe_title}_notebooklm.txt"
        md_path = output_dir / f"{safe_title}_notebooklm.md"
        json_path = output_dir / f"{safe_title}_notebooklm.json"
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ (NotebookLMìš© - í•œ ì¤„ì— í•˜ë‚˜ì”©)
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(f"# {book_title} - NotebookLMìš© URL ë¦¬ìŠ¤íŠ¸\n")
            f.write(f"# ì´ {len(urls)}ê°œì˜ URL\n\n")
            for url in urls:
                f.write(f"{url}\n")
        
        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥ (NotebookLM ë³µì‚¬ìš©)
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# {book_title} - NotebookLM ì†ŒìŠ¤ URL\n\n")
            f.write(f"**ì´ {len(urls)}ê°œì˜ URL**\n\n")
            f.write("## ğŸ“‹ URL ë¦¬ìŠ¤íŠ¸\n\n")
            f.write("ì•„ë˜ URLë“¤ì„ ë³µì‚¬í•˜ì—¬ NotebookLMì— ì†ŒìŠ¤ë¡œ ì¶”ê°€í•˜ì„¸ìš”.\n\n")
            f.write("```\n")
            for i, url in enumerate(urls, 1):
                f.write(f"{url}\n")
            f.write("```\n\n")
            f.write("## ğŸ“ ì‚¬ìš© ë°©ë²•\n\n")
            f.write("1. ìœ„ URL ë¸”ë¡ì„ ì „ì²´ ì„ íƒ (Cmd+A / Ctrl+A)\n")
            f.write("2. ë³µì‚¬ (Cmd+C / Ctrl+C)\n")
            f.write("3. NotebookLMì—ì„œ 'ì†ŒìŠ¤ ì¶”ê°€' > 'URL' ì„ íƒ\n")
            f.write("4. ë¶™ì—¬ë„£ê¸° (ê° URLì´ ìë™ìœ¼ë¡œ ì¸ì‹ë©ë‹ˆë‹¤)\n\n")
        
        # JSON íŒŒì¼ ì €ì¥ (ìƒì„¸ ì •ë³´)
        url_data = {
            'book_title': book_title,
            'collected_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_urls': len(urls),
            'urls': []
        }
        
        if validate:
            print("ğŸ” URL ìœ íš¨ì„± ê²€ì¦ ì¤‘...")
            for i, url in enumerate(urls, 1):
                print(f"  [{i}/{len(urls)}] {url}")
                url_info = self.validate_url(url)
                url_data['urls'].append(url_info)
                time.sleep(0.5)
        else:
            url_data['urls'] = [{'url': url, 'valid': None} for url in urls]
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(url_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ URL ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤:")
        print(f"   - TXT: {txt_path}")
        print(f"   - MD (NotebookLM ë³µì‚¬ìš©): {md_path}")
        print(f"   - JSON (ìƒì„¸ ì •ë³´): {json_path}")
        
        return {'txt_path': str(txt_path), 'md_path': str(md_path), 'json_path': str(json_path)}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='NotebookLMìš© URL ìˆ˜ì§‘')
    parser.add_argument('--title', type=str, help='ì±… ì œëª©')
    parser.add_argument('--author', type=str, help='ì €ì ì´ë¦„')
    parser.add_argument('--num', type=int, default=25, help='ìˆ˜ì§‘í•  URL ê°œìˆ˜ (ê¸°ë³¸ê°’: 25)')
    parser.add_argument('--validate', action='store_true', help='URL ìœ íš¨ì„± ê²€ì¦ ìˆ˜í–‰')
    
    args = parser.parse_args()
    
    collector = NotebookLMURLCollector()
    
    if args.title:
        book_title = args.title
        author = args.author
    else:
        # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
        print("=" * 60)
        print("ğŸ“š NotebookLMìš© URL ìˆ˜ì§‘ê¸°")
        print("=" * 60)
        print()
        
        book_title = input("ì±… ì œëª©ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not book_title:
            print("âŒ ì±… ì œëª©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        author = input("ì €ì ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒì‚¬í•­): ").strip() or None
        num_results = input("ìˆ˜ì§‘í•  URL ê°œìˆ˜ (ê¸°ë³¸ê°’: 25): ").strip()
        args.num = int(num_results) if num_results.isdigit() else 25
        args.validate = input("URL ìœ íš¨ì„± ê²€ì¦ì„ ìˆ˜í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: n): ").strip().lower() == 'y'
    
    print()
    
    # URL ìˆ˜ì§‘
    urls = collector.search_urls(book_title, author, args.num)
    
    if urls:
        # URL ì €ì¥
        result = collector.save_urls(book_title, urls, validate=args.validate)
        
        print()
        print("=" * 60)
        print("âœ… URL ìˆ˜ì§‘ ì™„ë£Œ!")
        print("=" * 60)
        print(f"\nğŸ“„ NotebookLMìš© íŒŒì¼: {result['txt_path']}")
        print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"   1. {result['txt_path']} íŒŒì¼ì„ ì—½ë‹ˆë‹¤")
        print(f"   2. URLë“¤ì„ ë³µì‚¬í•©ë‹ˆë‹¤")
        print(f"   3. NotebookLM (https://notebooklm.google.com)ì— ì ‘ì†í•©ë‹ˆë‹¤")
        print(f"   4. ìƒˆ ì†ŒìŠ¤ ì¶”ê°€ > URLì—ì„œ ë¶™ì—¬ë„£ê¸°í•©ë‹ˆë‹¤")
        print()
    else:
        print("âŒ ìˆ˜ì§‘ëœ URLì´ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

