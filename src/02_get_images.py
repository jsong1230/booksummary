"""
ì±… í‘œì§€ ë° ë¬´ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
- Google Books APIë¡œ ì±… í‘œì§€ ë‹¤ìš´ë¡œë“œ
- Unsplash/Pexels APIë¡œ ë¬´ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (5~10ì¥)
"""

import os
import json
import time
import base64
import requests
from pathlib import Path
from typing import List, Dict, Optional
import concurrent.futures
import random
from dotenv import load_dotenv
try:
    from utils.retry_utils import retry_with_backoff
except ImportError:
    from src.utils.retry_utils import retry_with_backoff

try:
    from utils.logger import get_logger
except ImportError:
    from src.utils.logger import get_logger

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    from googleapiclient.discovery import build
    GOOGLE_BOOKS_AVAILABLE = True
except ImportError:
    GOOGLE_BOOKS_AVAILABLE = False

try:
    from pexels_api import API as PexelsAPI
    PEXELS_AVAILABLE = True
except ImportError:
    PEXELS_AVAILABLE = False

load_dotenv()


class ImageDownloader:
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
        # API í‚¤ ë¡œë“œ
        self.google_books_api_key = os.getenv("GOOGLE_BOOKS_API_KEY")
        self.pexels_api_key = os.getenv("PEXELS_API_KEY")
        self.unsplash_access_key = os.getenv("UNSPLASH_ACCESS_KEY")
        self.pixabay_api_key = os.getenv("PIXABAY_API_KEY")
        
        # Google Books API ì´ˆê¸°í™”
        self.books_service = None
        if GOOGLE_BOOKS_AVAILABLE and self.google_books_api_key:
            try:
                self.books_service = build('books', 'v1', developerKey=self.google_books_api_key)
            except Exception as e:
                self.logger.warning(f"Google Books API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Pexels API ì´ˆê¸°í™”
        self.pexels = None
        if PEXELS_AVAILABLE and self.pexels_api_key:
            try:
                self.pexels = PexelsAPI(self.pexels_api_key)
            except Exception as e:
                self.logger.warning(f"Pexels API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # AI API í‚¤ ë¡œë“œ
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.claude_api_key = os.getenv("CLAUDE_API_KEY")
    
    @retry_with_backoff(retries=3, backoff_in_seconds=1.0)
    def _download_single_image(self, url: str, output_path: Path) -> str:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        with open(output_path, 'wb') as f:
            f.write(response.content)
        
        return str(output_path)

    @retry_with_backoff(retries=3, backoff_in_seconds=2.0)
    def _make_request(self, url: str, headers: Dict = None, params: Dict = None) -> Dict:
        """API ìš”ì²­ ìˆ˜í–‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        return response.json()

    @retry_with_backoff(retries=3, backoff_in_seconds=2.0)
    def _search_pexels(self, keyword: str, page: int, results_per_page: int) -> Dict:
        """Pexels ê²€ìƒ‰ ìˆ˜í–‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        if not self.pexels:
            raise ValueError("Pexels API not initialized")
        
        # ë‹¤ì–‘ì„±ì„ ìœ„í•´ ëœë¤í•˜ê²Œ í˜ì´ì§€ ì„ íƒ (1~3í˜ì´ì§€)
        if page == 1:
            page = random.randint(1, 3)
            
        return self.pexels.search(keyword, page=page, results_per_page=results_per_page)
    
    def download_book_cover(self, book_title: str, author: str = None, output_dir: Path = None, skip_image: bool = False) -> Optional[str]:
        """
        Google Books APIë¡œ ì±… í‘œì§€ ë‹¤ìš´ë¡œë“œ ë° book_info.json ìƒì„±
        
        âš ï¸ ì£¼ì˜: ì±… í‘œì§€ ì´ë¯¸ì§€ëŠ” ì €ì‘ê¶Œì´ ìˆì–´ YouTube ë“±ì— ì‚¬ìš© ì‹œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        í‘œì§€ ì´ë¯¸ì§€ëŠ” ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ë‹¤ìš´ë¡œë“œí•˜ë©°, ì‹¤ì œ ì˜ìƒ ì œì‘ì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        
        Args:
            book_title: ì±… ì œëª©
            author: ì €ì ì´ë¦„
            output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            skip_image: ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œëŠ” ê±´ë„ˆë›°ê³  book_info.jsonë§Œ ìƒì„±
            
        Returns:
            ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ (skip_image=Trueë©´ None)
        """
        if not self.books_service:
            self.logger.warning("Google Books APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        self.logger.info(f"ğŸ“š ì±… í‘œì§€ ê²€ìƒ‰ ì¤‘: {book_title}")
        if author:
            self.logger.info(f"   ì €ì: {author}")
        
        try:
            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (ì €ì í¬í•¨í•˜ì—¬ ì •í™•ë„ í–¥ìƒ)
            query = f'intitle:"{book_title}"'
            if author:
                query += f' inauthor:"{author}"'
            
            # ì–¸ì–´ ê°ì§€: ì œëª©ì— í•œê¸€ì´ ìˆìœ¼ë©´ í•œêµ­ì–´, ì—†ìœ¼ë©´ ì˜ì–´ë¡œ ê²€ìƒ‰
            has_korean = any('\uAC00' <= c <= '\uD7A3' for c in book_title)
            lang_restrict = 'ko' if has_korean else 'en'
            
            self.logger.info(f"   ê²€ìƒ‰ ì–¸ì–´: {lang_restrict}")
            
            # Google Books API ê²€ìƒ‰
            results = self.books_service.volumes().list(
                q=query,
                maxResults=10,  # ë” ë§ì€ ê²°ê³¼ í™•ì¸
                langRestrict=lang_restrict
            ).execute()
            
            if not results.get('items'):
                # ì–¸ì–´ ì œí•œ ì—†ì´ ì¬ì‹œë„
                self.logger.warning("ì–¸ì–´ ì œí•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì–¸ì–´ ì œí•œ ì—†ì´ ì¬ì‹œë„...")
                results = self.books_service.volumes().list(
                    q=query,
                    maxResults=10
                ).execute()
            
            if not results.get('items'):
                self.logger.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼ ì„ íƒ (ì €ìëª…ë„ í™•ì¸)
            best_book = None
            for book in results['items']:
                volume_info = book.get('volumeInfo', {})
                book_authors = volume_info.get('authors', [])
                book_title_found = volume_info.get('title', '').lower()
                
                # ì €ìëª…ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                author_match = False
                if author:
                    author_lower = author.lower()
                    for book_author in book_authors:
                        if author_lower in book_author.lower() or book_author.lower() in author_lower:
                            author_match = True
                            break
                else:
                    author_match = True  # ì €ì ì •ë³´ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ê²°ê³¼ í—ˆìš©
                
                # ì œëª©ë„ ë¹„ìŠ·í•œì§€ í™•ì¸
                title_match = book_title.lower() in book_title_found or book_title_found in book_title.lower()
                
                if author_match and title_match:
                    best_book = book
                    self.logger.info(f"âœ… ë§¤ì¹­ëœ ì±… ë°œê²¬: {volume_info.get('title')} - {', '.join(book_authors)}")
                    break
            
            if not best_book:
                # ë§¤ì¹­ë˜ëŠ” ê²Œ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©
                self.logger.warning("ì •í™•í•œ ë§¤ì¹­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                best_book = results['items'][0]
            
            book = best_book
            volume_info = book.get('volumeInfo', {})
            
            # ì €ì¥ ê²½ë¡œ
            if output_dir is None:
                try:
                    from utils.file_utils import get_standard_safe_title
                except ImportError:
                    from src.utils.file_utils import get_standard_safe_title
                safe_title_str = get_standard_safe_title(book_title)
                output_dir = Path("assets/images") / safe_title_str
            else:
                output_dir = Path(output_dir)
            
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (skip_imageê°€ Falseì¸ ê²½ìš°ë§Œ)
            image_url = None
            output_path = None
            if not skip_image:
                # ì´ë¯¸ì§€ ë§í¬ ì°¾ê¸°
                image_links = volume_info.get('imageLinks', {})
                if not image_links:
                    self.logger.warning("í‘œì§€ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    # ê°€ì¥ í° ì´ë¯¸ì§€ ì„ íƒ
                    image_url = image_links.get('large') or image_links.get('medium') or image_links.get('small') or image_links.get('thumbnail')
                    
                    if image_url:
                        try:
                            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                            response = requests.get(image_url, timeout=10)
                            response.raise_for_status()
                            
                            output_path = output_dir / "cover.jpg"
                            
                            # íŒŒì¼ ì €ì¥
                            with open(output_path, 'wb') as f:
                                f.write(response.content)
                            
                            self.logger.info(f"âœ… í‘œì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_path}")
                        except Exception as e:
                            self.logger.warning(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                            image_url = None
                    else:
                        self.logger.warning("ì´ë¯¸ì§€ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                # skip_image=Trueì¸ ê²½ìš°ì—ë„ image_urlì€ book_infoì— í¬í•¨í•˜ê¸° ìœ„í•´ ê°€ì ¸ì˜¤ê¸°
                image_links = volume_info.get('imageLinks', {})
                if image_links:
                    image_url = image_links.get('large') or image_links.get('medium') or image_links.get('small') or image_links.get('thumbnail')
                self.logger.info("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œëŠ” ê±´ë„ˆë›°ê³  ì±… ì •ë³´ë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
            
            # ISBN ì¶”ì¶œ (industryIdentifiers í•„ë“œ)
            isbn_13 = ''
            isbn_10 = ''
            for identifier in volume_info.get('industryIdentifiers', []):
                id_type = identifier.get('type', '')
                id_value = identifier.get('identifier', '')
                if id_type == 'ISBN_13':
                    isbn_13 = id_value
                elif id_type == 'ISBN_10':
                    isbn_10 = id_value

            # ê²€ìƒ‰ëœ íŒë³¸ì˜ ì–¸ì–´ì— ë”°ë¼ ISBNì„ í•œêµ­ì–´/ì˜ì–´ë¡œ ë¶„ë¥˜
            book_lang = volume_info.get('language', 'ko')
            isbn_13_ko = isbn_13 if book_lang == 'ko' else ''
            isbn_10_ko = isbn_10 if book_lang == 'ko' else ''
            isbn_13_en = isbn_13 if book_lang != 'ko' else ''
            isbn_10_en = isbn_10 if book_lang != 'ko' else ''

            # ê¸°ì¡´ book_info.jsonì´ ìˆìœ¼ë©´ ë‹¤ë¥¸ ì–¸ì–´ ISBNì„ ë³´ì¡´
            book_info_path = output_dir / "book_info.json"
            if book_info_path.exists():
                try:
                    with open(book_info_path, 'r', encoding='utf-8') as f:
                        existing = json.load(f)
                    if not isbn_13_ko:
                        isbn_13_ko = existing.get('isbn_13_ko', '')
                    if not isbn_10_ko:
                        isbn_10_ko = existing.get('isbn_10_ko', '')
                    if not isbn_13_en:
                        isbn_13_en = existing.get('isbn_13_en', '')
                    if not isbn_10_en:
                        isbn_10_en = existing.get('isbn_10_en', '')
                except Exception:
                    pass

            # ì±… ì •ë³´ ì €ì¥ (ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ í•­ìƒ ì €ì¥)
            book_info = {
                'title': volume_info.get('title', book_title),
                'authors': volume_info.get('authors', [author] if author else []),
                'publisher': volume_info.get('publisher', ''),
                'publishedDate': volume_info.get('publishedDate', ''),
                'description': volume_info.get('description', ''),
                'pageCount': volume_info.get('pageCount', 0),
                'categories': volume_info.get('categories', []),
                'language': book_lang,
                'google_books_id': book.get('id', ''),
                'image_url': image_url if image_url else '',
                'isbn_13_ko': isbn_13_ko,
                'isbn_10_ko': isbn_10_ko,
                'isbn_13_en': isbn_13_en,
                'isbn_10_en': isbn_10_en
            }
            
            book_info_path = output_dir / "book_info.json"
            with open(book_info_path, 'w', encoding='utf-8') as f:
                json.dump(book_info, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"âœ… ì±… ì •ë³´ ì €ì¥ ì™„ë£Œ: {book_info_path}")
            
            return str(output_path) if output_path else None
            
        except Exception as e:
            self.logger.error(f"ì˜¤ë¥˜: {e}")
            return None
    
    @retry_with_backoff(retries=3, backoff_in_seconds=2.0)
    def download_mood_images_unsplash(self, keywords: List[str], num_images: int = 100, output_dir: Path = None, max_per_keyword_override: Optional[int] = None) -> List[str]:
        """
        Unsplash APIë¡œ ë¬´ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            num_images: ë‹¤ìš´ë¡œë“œí•  ì´ë¯¸ì§€ ê°œìˆ˜
            output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            max_per_keyword_override: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ (Noneì´ë©´ ìë™ ê³„ì‚°)
            
        Returns:
            ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.unsplash_access_key:
            self.logger.warning("Unsplash API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        downloaded = []
        # ê° í‚¤ì›Œë“œì—ì„œ ê°€ì ¸ì˜¬ ì´ë¯¸ì§€ ìˆ˜ (ë‹¤ì–‘ì„±ì„ ìœ„í•´ ì œí•œ)
        # í‚¤ì›Œë“œê°€ ë§ìœ¼ë©´ ì ê²Œ, ì ìœ¼ë©´ ë§ì´ ê°€ì ¸ì˜´ (ìµœì†Œ 2ê°œ, ìµœëŒ€ 5ê°œ)
        # ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ì‹œì—ëŠ” ì œí•œì„ ì™„í™”í•˜ì—¬ ë¹ ë¥´ê²Œ ë‹¤ìš´ë¡œë“œ
        if max_per_keyword_override is not None:
            max_per_keyword = max_per_keyword_override
        else:
            max_per_keyword = max(2, min(5, num_images // (len(keywords) or 1)))
        
        # í‚¤ì›Œë“œ ìˆœì„œ ì„ê¸° (ë§¤ë²ˆ ê°™ì€ í‚¤ì›Œë“œë§Œ ì‚¬ìš©ë˜ì§€ ì•Šë„ë¡)
        shuffled_keywords = keywords.copy()
        random.shuffle(shuffled_keywords)
        
        # ë‹¤ìš´ë¡œë“œ ì‘ì—… ë¦¬ìŠ¤íŠ¸
        download_tasks = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            for keyword in shuffled_keywords:
                if len(downloaded) + len(download_tasks) >= num_images:
                    break
                
                try:
                    self.logger.info(f"ğŸ” ê²€ìƒ‰: {keyword}")
                    
                    # Unsplash API ê²€ìƒ‰
                    url = "https://api.unsplash.com/search/photos"
                    headers = {
                        "Authorization": f"Client-ID {self.unsplash_access_key}"
                    }
                    # ë‹¤ì–‘ì„±ì„ ìœ„í•´ ëœë¤í•˜ê²Œ í˜ì´ì§€ ì„ íƒ (1~3í˜ì´ì§€)
                    page = random.randint(1, 3)
                    
                    # ê° í‚¤ì›Œë“œì—ì„œ ìµœëŒ€ max_per_keywordê°œë§Œ ê°€ì ¸ì˜¤ê¸°
                    remaining = num_images - (len(downloaded) + len(download_tasks))
                    params = {
                        "query": keyword,
                        "page": page,
                        "per_page": min(max_per_keyword, remaining, 15),
                        "orientation": "landscape"
                    }
                    
                    data = self._make_request(url, headers=headers, params=params)
                    results = data.get('results', [])
                    
                    if not results:
                        self.logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {keyword}")
                        continue
                    
                    for photo in results:
                        if len(downloaded) + len(download_tasks) >= num_images:
                            break
                        
                        # ê³ í™”ì§ˆ ì´ë¯¸ì§€ URL
                        image_url = photo['urls'].get('regular') or photo['urls'].get('full')
                        
                        if not image_url:
                            continue
                        
                        # ì €ì¥ ê²½ë¡œ ì„¤ì •
                        filename = f"mood_{len(downloaded) + len(download_tasks) + 1:02d}_{keyword.replace(' ', '_')}.jpg"
                        output_path = output_dir / filename
                        
                        # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì‘ì—… ì¶”ê°€
                        future = executor.submit(self._download_single_image, image_url, output_path)
                        download_tasks.append((future, filename))
                        
                        time.sleep(0.1)  # API rate limit ë°©ì§€ (ìµœì†Œí•œì˜ ì§€ì—°)
                    
                except Exception as e:
                    self.logger.error(f"ì˜¤ë¥˜: {e}")
                    continue
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future, filename in download_tasks:
                try:
                    result = future.result()
                    if result:
                        downloaded.append(result)
                        self.logger.info(f"âœ… {filename}")
                except Exception as e:
                    self.logger.error(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({filename}): {e}")
        
        return downloaded
    
    @retry_with_backoff(retries=3, backoff_in_seconds=2.0)
    def download_mood_images_pexels(self, keywords: List[str], num_images: int = 100, output_dir: Path = None, max_per_keyword_override: Optional[int] = None) -> List[str]:
        """
        Pexels APIë¡œ ë¬´ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            num_images: ë‹¤ìš´ë¡œë“œí•  ì´ë¯¸ì§€ ê°œìˆ˜
            output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            max_per_keyword_override: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ (Noneì´ë©´ ìë™ ê³„ì‚°)
            
        Returns:
            ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.pexels:
            self.logger.warning("Pexels APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        downloaded = []
        # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ ì œí•œ (ë‹¤ì–‘ì„± í™•ë³´)
        # ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ì‹œì—ëŠ” ì œí•œì„ ì™„í™”í•˜ì—¬ ë¹ ë¥´ê²Œ ë‹¤ìš´ë¡œë“œ
        if max_per_keyword_override is not None:
            max_per_keyword = max_per_keyword_override
        else:
            max_per_keyword = max(2, min(5, num_images // (len(keywords) or 1)))
        
        # í‚¤ì›Œë“œ ìˆœì„œ ì„ê¸°
        shuffled_keywords = keywords.copy()
        random.shuffle(shuffled_keywords)
        
        download_tasks = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            for keyword in shuffled_keywords:
                if len(downloaded) + len(download_tasks) >= num_images:
                    break
                
                try:
                    self.logger.info(f"ğŸ” ê²€ìƒ‰: {keyword}")
                    
                    # Pexels API ê²€ìƒ‰
                    try:
                        remaining = num_images - (len(downloaded) + len(download_tasks))
                        # _search_pexels ë‚´ë¶€ì—ì„œ ì´ë¯¸ í˜ì´ì§€ ëœë¤í™” ì²˜ë¦¬ë¨
                        search_results = self._search_pexels(keyword, page=1, results_per_page=min(max_per_keyword, remaining))
                    except Exception as e:
                        self.logger.error(f"Pexels ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                        continue
                    
                    if not search_results.get('photos'):
                        self.logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {keyword}")
                        continue
                    
                    for photo in search_results['photos']:
                        if len(downloaded) + len(download_tasks) >= num_images:
                            break
                        
                        # ê³ í™”ì§ˆ ì´ë¯¸ì§€ URL
                        image_url = photo.get('src', {}).get('large') or photo.get('src', {}).get('original')
                        
                        if not image_url:
                            continue
                        
                        # ì €ì¥ ê²½ë¡œ ì„¤ì •
                        filename = f"mood_{len(downloaded) + len(download_tasks) + 1:02d}_{keyword.replace(' ', '_')}.jpg"
                        output_path = output_dir / filename
                        
                        # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì‘ì—… ì¶”ê°€
                        future = executor.submit(self._download_single_image, image_url, output_path)
                        download_tasks.append((future, filename))
                        
                        time.sleep(0.1)  # API rate limit ë°©ì§€
                    
                except Exception as e:
                    self.logger.error(f"ì˜¤ë¥˜: {e}")
                    continue
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future, filename in download_tasks:
                try:
                    result = future.result()
                    if result:
                        downloaded.append(result)
                        self.logger.info(f"âœ… {filename}")
                except Exception as e:
                    self.logger.error(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({filename}): {e}")
        
        return downloaded
    
    @retry_with_backoff(retries=3, backoff_in_seconds=2.0)
    def download_mood_images_pixabay(self, keywords: List[str], num_images: int = 100, output_dir: Path = None, max_per_keyword_override: Optional[int] = None) -> List[str]:
        """
        Pixabay APIë¡œ ë¬´ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        
        Args:
            keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            num_images: ë‹¤ìš´ë¡œë“œí•  ì´ë¯¸ì§€ ê°œìˆ˜
            output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
            max_per_keyword_override: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ ì˜¤ë²„ë¼ì´ë“œ (Noneì´ë©´ ìë™ ê³„ì‚°)
            
        Returns:
            ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.pixabay_api_key:
            self.logger.warning("Pixabay API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        downloaded = []
        # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ ì œí•œ
        # ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ì‹œì—ëŠ” ì œí•œì„ ì™„í™”í•˜ì—¬ ë¹ ë¥´ê²Œ ë‹¤ìš´ë¡œë“œ
        if max_per_keyword_override is not None:
            max_per_keyword = max_per_keyword_override
        else:
            max_per_keyword = max(2, min(5, num_images // (len(keywords) or 1)))
        
        # í‚¤ì›Œë“œ ìˆœì„œ ì„ê¸°
        shuffled_keywords = keywords.copy()
        random.shuffle(shuffled_keywords)
        
        base_url = "https://pixabay.com/api/"
        download_tasks = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            for keyword in shuffled_keywords:
                if len(downloaded) + len(download_tasks) >= num_images:
                    break
                
                try:
                    self.logger.info(f"ğŸ” ê²€ìƒ‰: {keyword}")
                    
                    # Pixabay API ê²€ìƒ‰
                    # ë‹¤ì–‘ì„±ì„ ìœ„í•´ ëœë¤í•˜ê²Œ í˜ì´ì§€ ì„ íƒ
                    page = random.randint(1, 3)
                    
                    params = {
                        'key': self.pixabay_api_key,
                        'q': keyword,
                        'image_type': 'photo',
                        'orientation': 'horizontal',
                        'safesearch': 'true',
                        'page': page,
                        'per_page': min(max_per_keyword, num_images - (len(downloaded) + len(download_tasks)))
                    }
                    
                    data = self._make_request(base_url, params=params)
                    hits = data.get('hits', [])
                    
                    if not hits:
                        self.logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {keyword}")
                        continue
                    
                    for hit in hits:
                        if len(downloaded) + len(download_tasks) >= num_images:
                            break
                        
                        # ê³ í™”ì§ˆ ì´ë¯¸ì§€ URL (largeImageURL ìš°ì„ , ì—†ìœ¼ë©´ webformatURL)
                        image_url = hit.get('largeImageURL') or hit.get('webformatURL')
                        
                        if not image_url:
                            continue
                        
                        # ì €ì¥ ê²½ë¡œ ì„¤ì •
                        filename = f"mood_{len(downloaded) + len(download_tasks) + 1:02d}_{keyword.replace(' ', '_')}.jpg"
                        output_path = output_dir / filename
                        
                        # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì‘ì—… ì¶”ê°€
                        future = executor.submit(self._download_single_image, image_url, output_path)
                        download_tasks.append((future, filename))
                        
                        time.sleep(0.1)  # API rate limit ë°©ì§€
                    
                except Exception as e:
                    self.logger.error(f"ì˜¤ë¥˜: {e}")
                    continue
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for future, filename in download_tasks:
                try:
                    result = future.result()
                    if result:
                        downloaded.append(result)
                        self.logger.info(f"âœ… {filename}")
                except Exception as e:
                    self.logger.error(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({filename}): {e}")
        
        return downloaded
    
    def validate_images_with_ai(self, image_dir: Path, book_title: str, author: str = None, target_count: int = 100) -> List[Path]:
        """
        GPT-4o Visionìœ¼ë¡œ ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€ì˜ ì±… ê´€ë ¨ì„±ì„ ê²€ì¦í•˜ê³  ìƒìœ„ ì´ë¯¸ì§€ë§Œ ìœ ì§€.

        Args:
            image_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            book_title: ì±… ì œëª©
            author: ì €ì ì´ë¦„
            target_count: ìµœì¢… ìœ ì§€í•  ì´ë¯¸ì§€ ìˆ˜ (ê¸°ë³¸: 100)

        Returns:
            ê²€ì¦ í›„ ìœ ì§€ëœ ì´ë¯¸ì§€ ê²½ë¡œ ëª©ë¡
        """
        if not OPENAI_AVAILABLE or not self.openai_api_key:
            self.logger.warning("OpenAI API í‚¤ê°€ ì—†ì–´ ì´ë¯¸ì§€ ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return list(image_dir.glob("mood_*.jpg"))[:target_count]

        all_images = sorted(image_dir.glob("mood_*.jpg"))
        if not all_images:
            return []

        self.logger.info(f"ğŸ” AI ì´ë¯¸ì§€ ê²€ì¦ ì‹œì‘: {len(all_images)}ê°œ ì´ë¯¸ì§€ â†’ ìƒìœ„ {target_count}ê°œ ì„ ë³„")

        author_str = f" by {author}" if author else ""
        scored_images = []
        batch_size = 10

        client_oa = openai.OpenAI(api_key=self.openai_api_key)

        for i in range(0, len(all_images), batch_size):
            batch = all_images[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(all_images) + batch_size - 1) // batch_size
            self.logger.info(f"  ë°°ì¹˜ {batch_num}/{total_batches} ê²€ì¦ ì¤‘... ({len(batch)}ê°œ)")

            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            image_contents = []
            valid_batch = []
            for img_path in batch:
                try:
                    with open(img_path, 'rb') as f:
                        img_data = base64.b64encode(f.read()).decode('utf-8')
                    image_contents.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{img_data}", "detail": "low"}
                    })
                    valid_batch.append(img_path)
                except Exception as e:
                    self.logger.warning(f"ì´ë¯¸ì§€ ì½ê¸° ì‹¤íŒ¨ ({img_path.name}): {e}")

            if not valid_batch:
                continue

            prompt_text = (
                f"You are evaluating {len(valid_batch)} images for use in a video about the book "
                f'"{book_title}"{author_str}.\n\n'
                f"For EACH image (numbered 1 to {len(valid_batch)}), rate how relevant it is to this specific book's "
                f"content, setting, themes, or atmosphere on a scale of 1-10.\n\n"
                f"Scoring guide:\n"
                f"- 8-10: Directly matches the book's setting, characters, or key themes\n"
                f"- 5-7: Loosely related to the book's mood or general era/location\n"
                f"- 1-4: Generic stock photo with no clear connection to this book\n\n"
                f"Respond with ONLY {len(valid_batch)} numbers separated by commas (e.g., '7,3,9,5,...').\n"
                f"No explanations."
            )

            messages = [{"type": "text", "text": prompt_text}] + image_contents

            try:
                response = client_oa.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": messages}],
                    max_tokens=100
                )
                scores_text = response.choices[0].message.content or ""
                scores_raw = [s.strip() for s in scores_text.split(',')]
                scores = []
                for s in scores_raw:
                    try:
                        scores.append(int(float(s)))
                    except ValueError:
                        scores.append(5)  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì¤‘ê°„ ì ìˆ˜

                # ì ìˆ˜ ê¸¸ì´ ë§ì¶”ê¸°
                while len(scores) < len(valid_batch):
                    scores.append(5)
                scores = scores[:len(valid_batch)]

                for img_path, score in zip(valid_batch, scores):
                    scored_images.append((score, img_path))

            except Exception as e:
                self.logger.warning(f"ë°°ì¹˜ {batch_num} ê²€ì¦ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ì¤‘ê°„ ì ìˆ˜ë¡œ ì²˜ë¦¬
                for img_path in valid_batch:
                    scored_images.append((5, img_path))

            time.sleep(0.5)  # API rate limit ë°©ì§€

        if not scored_images:
            self.logger.warning("ê²€ì¦ ê²°ê³¼ ì—†ìŒ - ì›ë³¸ ì´ë¯¸ì§€ ëª©ë¡ ë°˜í™˜")
            return list(all_images)[:target_count]

        # ì ìˆ˜ìˆœ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        scored_images.sort(key=lambda x: x[0], reverse=True)

        # ì ìˆ˜ ë¶„í¬ ë¡œê¹…
        score_counts = {i: sum(1 for s, _ in scored_images if s == i) for i in range(1, 11)}
        self.logger.info(f"ğŸ“Š ì ìˆ˜ ë¶„í¬: {score_counts}")

        kept = [p for _, p in scored_images[:target_count]]
        removed = [p for _, p in scored_images[target_count:]]

        # ì ìˆ˜ ë‚®ì€ ì´ë¯¸ì§€ ì‚­ì œ
        deleted_count = 0
        for img_path in removed:
            try:
                img_path.unlink()
                deleted_count += 1
            except Exception as e:
                self.logger.warning(f"ì´ë¯¸ì§€ ì‚­ì œ ì‹¤íŒ¨ ({img_path.name}): {e}")

        # ìœ ì§€ëœ ì´ë¯¸ì§€ ì¤‘ ì ìˆ˜ ë‚®ì€ ê²ƒ(1-4ì ) ê°œìˆ˜ ë¡œê¹…
        low_score_kept = sum(1 for s, _ in scored_images[:target_count] if s <= 4)
        self.logger.info(
            f"âœ… ê²€ì¦ ì™„ë£Œ: {len(kept)}ê°œ ìœ ì§€ (ì €ì ìˆ˜ í¬í•¨ {low_score_kept}ê°œ), {deleted_count}ê°œ ì‚­ì œ"
        )

        return kept

    def download_all(self, book_title: str, author: str = None, keywords: List[str] = None, num_mood_images: int = 100, skip_cover: bool = False, skip_validation: bool = False) -> Dict:
        """
        ì±… í‘œì§€ì™€ ë¬´ë“œ ì´ë¯¸ì§€ ëª¨ë‘ ë‹¤ìš´ë¡œë“œ

        Args:
            book_title: ì±… ì œëª©
            author: ì €ì ì´ë¦„
            keywords: ë¬´ë“œ ì´ë¯¸ì§€ ê²€ìƒ‰ í‚¤ì›Œë“œ (Noneì´ë©´ ìë™ ìƒì„±)
            num_mood_images: ìµœì¢… ìœ ì§€í•  ë¬´ë“œ ì´ë¯¸ì§€ ê°œìˆ˜ (ê¸°ë³¸: 100)
            skip_cover: í‘œì§€ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê±´ë„ˆë›°ê¸°
            skip_validation: AI ê²€ì¦ ê±´ë„ˆë›°ê¸° (ê¸°ë³¸: False, ê²€ì¦ ìˆ˜í–‰)

        Returns:
            ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        self.logger.info("=" * 60)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        try:
            from utils.file_utils import get_standard_safe_title
        except ImportError:
            from src.utils.file_utils import get_standard_safe_title
        safe_title_str = get_standard_safe_title(book_title)
        output_dir = Path("assets/images") / safe_title_str
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. ì±… í‘œì§€ ë‹¤ìš´ë¡œë“œ ë° book_info.json ìƒì„± (ì„ íƒì‚¬í•­)
        # âš ï¸ ì£¼ì˜: ì±… í‘œì§€ëŠ” ì €ì‘ê¶Œì´ ìˆì–´ ì˜ìƒì— ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        # í‘œì§€ëŠ” ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ë‹¤ìš´ë¡œë“œí•˜ë©°, ì‹¤ì œ ì˜ìƒì—ëŠ” ì €ì‘ê¶Œ ì—†ëŠ” ë¬´ë“œ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        # skip_cover=Trueì—¬ë„ book_info.jsonì€ ìƒì„±í•©ë‹ˆë‹¤.
        cover_path = None
        if skip_cover:
            self.logger.info("ì±… í‘œì§€ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œëŠ” ê±´ë„ˆë›°ì§€ë§Œ, ì±… ì •ë³´(book_info.json)ëŠ” ìƒì„±í•©ë‹ˆë‹¤.")
            self.download_book_cover(book_title, author, output_dir, skip_image=True)
        else:
            self.logger.warning("ì±… í‘œì§€ ì´ë¯¸ì§€ëŠ” ì €ì‘ê¶Œ ë¬¸ì œë¡œ ì˜ìƒì— ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            self.logger.info("í‘œì§€ëŠ” ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
            cover_path = self.download_book_cover(book_title, author, output_dir, skip_image=False)
        
        # 2. í‚¤ì›Œë“œ ìƒì„± (ì—†ìœ¼ë©´) - AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì±… ë‚´ìš© ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„±
        if keywords is None:
            self.logger.info("ğŸ“ AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì±… ë‚´ìš© ê¸°ë°˜ ì´ë¯¸ì§€ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„± ì¤‘...")
            keywords = self.generate_keywords_with_ai(book_title, author, output_dir)
            self.logger.info(f"âœ… ìƒì„±ëœ í‚¤ì›Œë“œ: {', '.join(keywords[:10])}")
        
        self.logger.info(f"ğŸ¨ ë¬´ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘... (í‚¤ì›Œë“œ: {', '.join(keywords)})")
        
        # 3. ë¬´ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ (Pexels â†’ Pixabay â†’ Unsplash ìˆœì„œ)
        # ê¸°ì¡´ ì´ë¯¸ì§€ í™•ì¸
        existing_images = list(output_dir.glob("mood_*.jpg"))
        existing_count = len(existing_images)
        
        if existing_count >= num_mood_images:
            self.logger.info(f"âœ… ê¸°ì¡´ ì´ë¯¸ì§€ ë°œê²¬: {existing_count}ê°œ (ëª©í‘œ: {num_mood_images}ê°œ)")
            self.logger.info("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {
                'cover_path': str(cover_path) if cover_path else None,
                'mood_images': [str(img) for img in existing_images[:num_mood_images]],
                'total_mood_images': existing_count
            }

        # AI ê²€ì¦ì„ ìœ„í•´ ì—¬ìœ ë¶„(30ê°œ)ì„ í¬í•¨í•˜ì—¬ ë” ë§ì´ ë‹¤ìš´ë¡œë“œ
        # skip_validationì´ë©´ ëª©í‘œ ìˆ˜ë§Œí¼ë§Œ ë‹¤ìš´ë¡œë“œ
        download_target = num_mood_images if skip_validation else max(num_mood_images + 30, 130)
        self.logger.info(f"ğŸ“Š ê¸°ì¡´ ì´ë¯¸ì§€: {existing_count}ê°œ, ë‹¤ìš´ë¡œë“œ ëª©í‘œ: {download_target}ê°œ (ê²€ì¦ í›„ {num_mood_images}ê°œ ìœ ì§€)")

        # ì´ë¯¸ì§€ë¥¼ í™•ì‹¤íˆ ë‹¤ìš´ë¡œë“œí•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ í‚¤ì›Œë“œì—ì„œ ì¶©ë¶„íˆ ìˆ˜ì§‘
        mood_images = existing_images.copy()  # ê¸°ì¡´ ì´ë¯¸ì§€ í¬í•¨
        target_count = download_target
        
        # Pexelsì—ì„œ ë‹¤ìš´ë¡œë“œ (1ìˆœìœ„)
        if len(mood_images) < target_count and self.pexels:
            remaining = target_count - len(mood_images)
            self.logger.info(f"ğŸ“¸ Pexelsì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘... (ëª©í‘œ: {remaining}ê°œ)")
            additional = self.download_mood_images_pexels(keywords, remaining, output_dir)
            mood_images.extend(additional)
            self.logger.info(f"âœ… Pexels: {len(additional)}ê°œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        
        # Pixabayì—ì„œ ì¶”ê°€ ë‹¤ìš´ë¡œë“œ (2ìˆœìœ„)
        if len(mood_images) < target_count and self.pixabay_api_key:
            remaining = target_count - len(mood_images)
            self.logger.info(f"ğŸ“¸ Pixabayì—ì„œ ì¶”ê°€ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘... (ëª©í‘œ: {remaining}ê°œ)")
            additional = self.download_mood_images_pixabay(keywords, remaining, output_dir)
            mood_images.extend(additional)
            self.logger.info(f"âœ… Pixabay: {len(additional)}ê°œ ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        
        # Unsplashì—ì„œ ì¶”ê°€ ë‹¤ìš´ë¡œë“œ (3ìˆœìœ„)
        if len(mood_images) < target_count and self.unsplash_access_key:
            remaining = target_count - len(mood_images)
            self.logger.info(f"ğŸ“¸ Unsplashì—ì„œ ì¶”ê°€ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘... (ëª©í‘œ: {remaining}ê°œ)")
            additional = self.download_mood_images_unsplash(keywords, remaining, output_dir)
            mood_images.extend(additional)
            self.logger.info(f"âœ… Unsplash: {len(additional)}ê°œ ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        
        # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ í‚¤ì›Œë“œë¥¼ ìˆœí™˜í•˜ë©° ì¶”ê°€ ë‹¤ìš´ë¡œë“œ
        # ê°œì„ : í•œ ë²ˆì— ë” ë§ì€ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì˜¤ê³ , ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìµœì í™”í•˜ì—¬ ì§€ì—° ìµœì†Œí™”
        if len(mood_images) < target_count:
            remaining = target_count - len(mood_images)
            self.logger.info(f"ğŸ”„ ì¶”ê°€ í‚¤ì›Œë“œë¡œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘... (ëª©í‘œ: {remaining}ê°œ)")
            
            # í‚¤ì›Œë“œ ìˆœì„œ ì„ê¸°
            shuffled_keywords = keywords.copy()
            random.shuffle(shuffled_keywords)
            
            # ì¶”ê°€ ë‹¤ìš´ë¡œë“œ: í•œ ë²ˆì— ë” ë§ì€ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ê°œì„ 
            # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê°œìˆ˜ë¥¼ ëŠ˜ë¦¬ê³ , ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìµœì í™”
            keyword_cycle = 0
            max_cycles = 3  # ìµœëŒ€ 3ë²ˆ ìˆœí™˜ (ê¸°ì¡´ len(keywords) * 2ì—ì„œ ì¶•ì†Œ)
            
            while len(mood_images) < target_count and keyword_cycle < max_cycles:
                remaining = target_count - len(mood_images)
                if remaining <= 0:
                    break
                
                # í•œ ë²ˆì— ë” ë§ì€ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê°œìˆ˜ ì¦ê°€
                # ë‚¨ì€ ê°œìˆ˜ê°€ ì ìœ¼ë©´ í•œ ë²ˆì— ì²˜ë¦¬
                batch_size = min(remaining, 10)  # í•œ ë²ˆì— ìµœëŒ€ 10ê°œì”© ì²˜ë¦¬
                
                # Pexelsì—ì„œ ì¶”ê°€ ì‹œë„ (1ìˆœìœ„) - ìš°ì„ ì ìœ¼ë¡œ ë” ë§ì´ ê°€ì ¸ì˜¤ê¸°
                if len(mood_images) < target_count and self.pexels:
                    remaining = target_count - len(mood_images)
                    try:
                        # ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ì‹œ í‚¤ì›Œë“œë‹¹ ì œí•œì„ ì™„í™” (ìµœëŒ€ 10ê°œê¹Œì§€ í—ˆìš©)
                        # ë‚¨ì€ ê°œìˆ˜ê°€ ì ìœ¼ë©´ í•œ ë²ˆì— ë¹ ë¥´ê²Œ ì²˜ë¦¬
                        additional = self.download_mood_images_pexels(
                            shuffled_keywords[:5], 
                            remaining, 
                            output_dir,
                            max_per_keyword_override=min(10, remaining)  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 10ê°œê¹Œì§€ í—ˆìš©
                        )
                        mood_images.extend(additional)
                        if len(mood_images) >= target_count:
                            break
                    except Exception as e:
                        self.logger.warning(f"Pexels ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                # Pixabayì—ì„œ ì¶”ê°€ ì‹œë„ (2ìˆœìœ„)
                if len(mood_images) < target_count and self.pixabay_api_key:
                    remaining = target_count - len(mood_images)
                    try:
                        # ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ì‹œ í‚¤ì›Œë“œë‹¹ ì œí•œì„ ì™„í™”
                        additional = self.download_mood_images_pixabay(
                            shuffled_keywords[:5], 
                            remaining, 
                            output_dir,
                            max_per_keyword_override=min(10, remaining)  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 10ê°œê¹Œì§€ í—ˆìš©
                        )
                        mood_images.extend(additional)
                        if len(mood_images) >= target_count:
                            break
                    except Exception as e:
                        self.logger.warning(f"Pixabay ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                # Unsplashì—ì„œ ì¶”ê°€ ì‹œë„ (3ìˆœìœ„)
                if len(mood_images) < target_count and self.unsplash_access_key:
                    remaining = target_count - len(mood_images)
                    try:
                        # ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ì‹œ í‚¤ì›Œë“œë‹¹ ì œí•œì„ ì™„í™”
                        additional = self.download_mood_images_unsplash(
                            shuffled_keywords[:5], 
                            remaining, 
                            output_dir,
                            max_per_keyword_override=min(10, remaining)  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 10ê°œê¹Œì§€ í—ˆìš©
                        )
                        mood_images.extend(additional)
                        if len(mood_images) >= target_count:
                            break
                    except Exception as e:
                        self.logger.warning(f"Unsplash ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                keyword_cycle += 1
                if len(mood_images) >= target_count:
                    break
        
        self.logger.info("=" * 60)
        self.logger.info("âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}")
        self.logger.info(f"ğŸ“š í‘œì§€: {'âœ…' if cover_path else 'âŒ'}")
        self.logger.info(f"ğŸ¨ ë¬´ë“œ ì´ë¯¸ì§€: {len(mood_images)}ê°œ")

        # 4. AI ê²€ì¦ ë‹¨ê³„: ê´€ë ¨ì„± ë‚®ì€ ì´ë¯¸ì§€ ì‚­ì œ, ìƒìœ„ num_mood_imagesê°œ ìœ ì§€
        if not skip_validation and len(mood_images) > num_mood_images:
            self.logger.info(f"ğŸ” AI ê²€ì¦ ì‹œì‘: {len(mood_images)}ê°œ â†’ {num_mood_images}ê°œ ì„ ë³„")
            validated = self.validate_images_with_ai(output_dir, book_title, author, target_count=num_mood_images)
            mood_images = validated
        else:
            if skip_validation:
                self.logger.info("â© AI ê²€ì¦ ê±´ë„ˆëœ€ (--skip-validation)")
            mood_images = mood_images[:num_mood_images]

        # mood_imagesê°€ Path ê°ì²´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
        mood_images_str = [str(img) if isinstance(img, Path) else img for img in mood_images]

        return {
            'cover_path': str(cover_path) if cover_path else None,
            'mood_images': mood_images_str,
            'output_dir': str(output_dir),
            'total_mood_images': len(mood_images_str)
        }
    
    def _generate_keywords(self, book_title: str, author: str = None) -> List[str]:
        """
        ì±…ê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œ ìƒì„± (ì €ì‘ê¶Œ ì—†ëŠ” ì´ë¯¸ì§€ ê²€ìƒ‰ìš©)
        - ê´€ë ¨ ì˜í™”, ì‘ê°€, ì±… í…Œë§ˆ ë“±
        """
        keywords = []
        author_lower = author.lower() if author else ""
        title_lower = book_title.lower() if book_title else ""
        
        # ì‘ê°€ ê´€ë ¨ í‚¤ì›Œë“œ
        if author:
            # ë¬´ë¼ì¹´ë¯¸ í•˜ë£¨í‚¤ ê´€ë ¨
            if any(n in author_lower for n in ["ë¬´ë¼ì¹´ë¯¸", "í•˜ë£¨í‚¤", "murakami", "haruki"]):
                keywords.extend([
                    "japanese literature", "tokyo cityscape", "japanese culture",
                    "norwegian wood movie", "japanese novel", "murakami books",
                    "surrealism art", "jazz bar atmosphere", "well in forest"
                ])
            # ì˜¤ë² ë¼ëŠ” ë‚¨ì / í”„ë ˆë“œë¦­ ë°°í¬ë§Œ ê´€ë ¨
            elif any(n in author_lower or n in title_lower for n in ["ì˜¤ë² ", "ë°°í¬ë§Œ", "backman", "ove"]):
                keywords.extend([
                    "swedish small town", "old neighborhood houses", "saab car vintage",
                    "grumpy old man", "lonely figure park bench", "neighborhood community",
                    "winter in sweden", "melancholic atmosphere", "warm interior cottage",
                    "cat in neighborhood", "toolbox and tools", "blue overalls"
                ])
            # ì¼ë°˜ì ì¸ ì‘ê°€ëª… ì¶”ê°€
            keywords.append(author_lower.replace(" ", ""))
        
        # ì±… ì œëª© í…Œë§ˆ ê´€ë ¨ í‚¤ì›Œë“œ
        if any(n in title_lower for n in ["ë…¸ë¥´ì›¨ì´", "norwegian", "ìƒì‹¤", "loss"]):
            keywords.extend([
                "norway forest", "norwegian landscape", "forest nature",
                "scandinavian nature", "1960s japan", "tokyo 1960s",
                "loss and grief", "japanese youth 1960s", "tokyo university"
            ])
        
        # ë²”ìš© í…Œë§ˆ í‚¤ì›Œë“œëŠ” ì œê±° - ì±… ë‚´ìš©ê³¼ ë¬´ê´€í•œ ì´ë¯¸ì§€ê°€ í¬í•¨ë˜ëŠ” ì›ì¸
        
        # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 30ê°œ ë°˜í™˜ (ê¸°ì¡´ 10ê°œì—ì„œ ìƒí–¥)
        unique_keywords = []
        seen = set()
        for kw in keywords:
            kw_clean = kw.lower().strip()
            if kw_clean and kw_clean not in seen:
                seen.add(kw_clean)
                unique_keywords.append(kw_clean)
        
        return unique_keywords[:30]
    
    def generate_keywords_with_ai(self, book_title: str, author: str = None, image_dir: Path = None) -> List[str]:
        """
        AIë¥¼ ì‚¬ìš©í•˜ì—¬ ì±… ë‚´ìš© ê¸°ë°˜ ì´ë¯¸ì§€ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
        - ì±…ì˜ ë‚´ìš©, ì£¼ì œ, ë°°ê²½, ê°ì •, ì£¼ìš” ì¥ë©´ ë“±ì„ ë¶„ì„í•˜ì—¬ êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ìƒì„±
        """
        try:
            from utils.file_utils import get_standard_safe_title
        except ImportError:
            from src.utils.file_utils import get_standard_safe_title
        
        safe_title_str = get_standard_safe_title(book_title)
        
        # ì±… ì •ë³´ ë¡œë“œ ì‹œë„
        book_info = None
        if image_dir:
            book_info_path = image_dir / "book_info.json"
            if book_info_path.exists():
                try:
                    with open(book_info_path, 'r', encoding='utf-8') as f:
                        book_info = json.load(f)
                except:
                    pass
        
        # Summary íŒŒì¼ ë¡œë“œ ì‹œë„ (ë” ì •í™•í•œ í‚¤ì›Œë“œ ìƒì„±ì„ ìœ„í•´)
        summary_text = None
        # .txtì™€ .md ëª¨ë‘ ì§€ì›
        summary_paths = [
            Path("assets/summaries") / f"{safe_title_str}_summary_kr.md",
            Path("assets/summaries") / f"{safe_title_str}_summary_en.md",
            # í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ ê²½ë¡œë“¤
            Path("assets/summaries") / f"{safe_title_str}_summary_ko.md",
            Path("assets/summaries") / f"{safe_title_str}_summary_ko.txt",
            Path("assets/summaries") / f"{safe_title_str}_summary_en.txt"
        ]
        
        for sp in summary_paths:
            if sp.exists():
                try:
                    with open(sp, 'r', encoding='utf-8') as f:
                        summary_text = f.read()[:4000]  # ì²˜ìŒ 4000ì ì‚¬ìš© (ë” ë§ì€ ì¥ë©´ ì •ë³´)
                    break
                except:
                    continue
        
        prompt = f"""Role: You are an expert visual director and historian specializing in book-to-visual adaptation.
Task: Generate 60 specific English image search keywords for the book "{book_title}" by "{author}".

CRITICAL RULES:
1. **Book-Specific ONLY**: Every keyword must directly reflect THIS book's actual content, scenes, settings, or themes.
2. **NO Generic Photography Terms**: FORBIDDEN - "dramatic lighting", "cinematic landscape", "moody atmosphere", "vintage photography", "golden hour", "soft bokeh", "minimalist composition", "symbolic object", "classic novel vibe", "literary atmosphere". These are banned.
3. **Geographical Accuracy**: Strictly follow the story's actual setting. Do NOT use "Korea" unless the book is set there.
4. **Scene-Based**: Extract specific scenes, locations, objects, and characters from the book's actual content.
5. **Visual Diversity**: Include wide establishing shots, close-up textures, and character moments - all book-specific.

Content to Analyze:
"""

        # Summary ë‚´ìš© ì¶”ê°€ (ê°€ì¥ ì¤‘ìš”)
        if summary_text:
            prompt += f"\n[Book Summary]\n{summary_text}\n"

        if book_info:
            if book_info.get('description'):
                prompt += f"\n[Book Description]\n{book_info['description'][:800]}\n"
            if book_info.get('categories'):
                prompt += f"[Categories]\n{', '.join(book_info['categories'])}\n"

        prompt += """
Keywords Categories (Provide 10-12 per category):
1. **Book-Specific Atmosphere**: ONLY the unique emotional tone and atmosphere of THIS book (e.g., "nazi concentration camp despair", "1960s tokyo melancholy", "austrian mountains isolation")
2. **Setting & Architecture**: Actual locations from the book (e.g., "hagia sophia interior", "auschwitz barracks", "1960s tokyo university dormitory", "norwegian forest autumn")
3. **Objects & Symbols**: Actual objects that appear in the book or symbolize its themes (e.g., "prisoner uniform stripes", "vintage japanese record player", "worn leather journal")
4. **Textures & Close-ups**: Physical details that evoke the book's world (e.g., "barbed wire close-up", "old tatami mat texture", "yellowed wartime document")
5. **Characters/Scenes**: Specific scenes or character types from the book (e.g., "prisoner working in nazi camp", "japanese college student 1960s", "lonely man in snowy park")

Constraints:
- Keywords must be in **ENGLISH**.
- **NO** text overlays or typography keywords.
- **NO** generic stock photo terms: "dramatic lighting", "cinematic", "vintage photography", "golden hour", "bokeh", "minimalist", "storytelling visual", "emotional scene", "literary".
- **NO** generic terms like "book", "reading", "illustration", "nature landscape" unless specific to the book.
- **Strictly exclude** modern elements if the book is historical.
- **Strictly exclude** Korean elements for non-Korean stories.

Format:
- Return ONLY a list of keywords separated by commas.
- No numbering or explanations.
"""



        try:
            # Claude API ìš°ì„  ì‚¬ìš©
            if ANTHROPIC_AVAILABLE and self.claude_api_key:
                client = anthropic.Anthropic(api_key=self.claude_api_key)
                response = client.messages.create(
                    model="claude-sonnet-4-6",
                    max_tokens=1000,
                    messages=[{
                        "role": "user",
                        "content": prompt
                    }]
                )
                keywords_text = response.content[0].text
            # OpenAI API ì‚¬ìš©
            elif OPENAI_AVAILABLE and self.openai_api_key:
                openai.api_key = self.openai_api_key
                client_oa = openai.OpenAI(api_key=self.openai_api_key)
                response = client_oa.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that generates image search keywords based on book content."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=1000
                )
                keywords_text = response.choices[0].message.content
            else:
                self.logger.warning("AI API í‚¤ê°€ ì—†ì–´ ê¸°ë³¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return self._generate_keywords(book_title, author)
            
            # í‚¤ì›Œë“œ íŒŒì‹± ë° í•„í„°ë§
            keywords = []
            # ê¸ˆì§€ëœ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ ëª©ë¡
            banned_keywords = {
                'aesthetic', 'beautiful', 'nice', 'pretty', 'art', 'design', 'style',
                'book', 'reading', 'literature', 'novel', 'story', 'fiction',
                'image', 'photo', 'picture', 'illustration', 'graphic', 'visual',
                'bookstore', 'bookshop', 'library'
            }
            
            # ì‰¼í‘œ(,)ì™€ ì¤„ë°”ê¿ˆ(\n)ì„ ëª¨ë‘ ì²˜ë¦¬í•˜ì—¬ í‚¤ì›Œë“œ ë¶„ë¦¬
            raw_keywords = []
            for part in keywords_text.split(','):
                for line in part.split('\n'):
                    raw_keywords.append(line.strip())
            
            for line in raw_keywords:
                # ë²ˆí˜¸ë‚˜ ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
                if line and not line.startswith('#') and not line.startswith('-'):
                    # ë²ˆí˜¸ ì œê±° (1. 2. ë“±)
                    line = line.lstrip('0123456789. -')
                    # ë”°ì˜´í‘œ ì œê±°
                    line = line.strip('"\'')
                    # ë‹¨ì¼ ë¬¸ìë‚˜ ë„ˆë¬´ ì§§ì€ í‚¤ì›Œë“œ ì œì™¸
                    words = line.split()
                    if words and len(words) >= 1 and len(words) <= 5:
                        # ê° ë‹¨ì–´ê°€ ìµœì†Œ 2ê¸€ì ì´ìƒì´ì–´ì•¼ í•¨ (ë‹¨, 'saab' ê°™ì€ ì§§ì€ ìœ íš¨ ë‹¨ì–´ í—ˆìš©)
                        if all(len(w) >= 2 for w in words):
                            keyword = ' '.join(words).lower()
                            # ê¸ˆì§€ëœ í‚¤ì›Œë“œ í•„í„°ë§
                            keyword_words = set(keyword.split())
                            if not keyword_words.intersection(banned_keywords):
                                keywords.append(keyword)
            
            if not keywords:
                self.logger.warning("AI í‚¤ì›Œë“œ íŒŒì‹± ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return self._generate_keywords(book_title, author)
            
            # AI í‚¤ì›Œë“œê°€ ì¶©ë¶„í•˜ë©´ basic í‚¤ì›Œë“œ ë³‘í•©í•˜ì§€ ì•ŠìŒ (ì±… ê´€ë ¨ì„± ìœ ì§€)
            # AI í‚¤ì›Œë“œê°€ 30ê°œ ë¯¸ë§Œì¼ ë•Œë§Œ ì‘ê°€/ì œëª©ë³„ í•˜ë“œì½”ë”© í‚¤ì›Œë“œë¡œ ë³´ì¶©
            if len(keywords) >= 30:
                self.logger.info(f"ğŸ“ AI í‚¤ì›Œë“œ {len(keywords)}ê°œ ì¶©ë¶„ - basic í‚¤ì›Œë“œ ë³‘í•© ìƒëµ (ê´€ë ¨ì„± ìœ ì§€)")
                all_keywords = keywords
            else:
                self.logger.info(f"ğŸ“ AI í‚¤ì›Œë“œ {len(keywords)}ê°œ ë¶€ì¡± - basic í‚¤ì›Œë“œë¡œ ë³´ì¶©")
                basic_keywords = self._generate_keywords(book_title, author)
                all_keywords = keywords + basic_keywords
            
            # ì¤‘ë³µ ì œê±° ë° ê¸ˆì§€ í‚¤ì›Œë“œ ì¬í•„í„°ë§
            seen = set()
            unique_keywords = []
            # ì¶”ê°€ ê¸ˆì§€ í‚¤ì›Œë“œ (ì „ì²´ í‚¤ì›Œë“œ ë¬¸ìì—´ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì œì™¸)
            additional_banned = ['bookstore', 'bookshop', 'japanese bookstore', 'japanese bookshop']
            
            for kw in all_keywords:
                kw_clean = kw.lower().strip()
                kw_words = set(kw_clean.split())
                
                # ê¸ˆì§€ëœ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì¶”ê°€
                if kw_clean and kw_clean not in seen and not kw_words.intersection(banned_keywords):
                    # ì¶”ê°€ ê¸ˆì§€ í‚¤ì›Œë“œ ì²´í¬ (ì „ì²´ ë¬¸ìì—´ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì œì™¸)
                    if not any(banned in kw_clean for banned in additional_banned):
                        seen.add(kw_clean)
                        unique_keywords.append(kw_clean)
            
            self.logger.info(f"ğŸ“ í•„í„°ë§ëœ í‚¤ì›Œë“œ: {len(unique_keywords)}ê°œ (ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ ì œì™¸)")
            # 100ê°œ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê¸° ìœ„í•´ ì¶©ë¶„í•œ í‚¤ì›Œë“œ ë°˜í™˜
            return unique_keywords[:50]  # ìµœëŒ€ 50ê°œ í‚¤ì›Œë“œ
            
        except Exception as e:
            self.logger.warning(f"AI í‚¤ì›Œë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            self.logger.info("ê¸°ë³¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._generate_keywords(book_title, author)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ì±… í‘œì§€ ë° ë¬´ë“œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ')
    parser.add_argument('--title', type=str, required=True, help='ì±… ì œëª©')
    parser.add_argument('--author', type=str, help='ì €ì ì´ë¦„')
    parser.add_argument('--keywords', type=str, nargs='+', help='ë¬´ë“œ ì´ë¯¸ì§€ ê²€ìƒ‰ í‚¤ì›Œë“œ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)')
    parser.add_argument('--num-mood', type=int, default=100, help='ë¬´ë“œ ì´ë¯¸ì§€ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)')
    parser.add_argument('--skip-cover', action='store_true', help='í‘œì§€ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-validation', action='store_true', help='AI ì´ë¯¸ì§€ ê²€ì¦ ê±´ë„ˆë›°ê¸° (ê¸°ë³¸: ê²€ì¦ ìˆ˜í–‰)')

    args = parser.parse_args()

    downloader = ImageDownloader()
    result = downloader.download_all(
        book_title=args.title,
        author=args.author,
        keywords=args.keywords,
        num_mood_images=args.num_mood,
        skip_cover=args.skip_cover,
        skip_validation=args.skip_validation
    )
    
    logger = get_logger(__name__)
    if result['cover_path']:
        logger.info(f"âœ… í‘œì§€: {result['cover_path']}")
    if result['mood_images']:
        logger.info(f"âœ… ë¬´ë“œ ì´ë¯¸ì§€: {len(result['mood_images'])}ê°œ")


if __name__ == "__main__":
    main()

