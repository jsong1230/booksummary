"""
ìœ íŠœë¸Œ ë¡±í¼ ë¶íŠœë¸Œë¥¼ ìœ„í•œ ê¹Šì´ ìˆëŠ” ìë£Œ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
{ì‘ê°€}ì˜ ã€{ì±…ì œëª©}ã€ì— ëŒ€í•´ 30~60ë¶„ì§œë¦¬ í•´ì„¤/ë¶„ì„ ìœ íŠœë¸Œ ì˜ìƒì„ ë§Œë“¤ ì˜ˆì •.
NotebookLMì— ë„£ì„ ìë£Œë¡œ ì“¸ ìˆ˜ ìˆë„ë¡, ì´ ì±…ì„ ê¹Šì´ ìˆê²Œ ë‹¤ë£¨ëŠ” URL 30ê°œë¥¼ ìˆ˜ì§‘.
"""

import os
import sys
import csv
import time
import re
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from urllib.parse import urlparse

# src ë””ë ‰í† ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

from duckduckgo_search import DDGS
from bs4 import BeautifulSoup
import requests
from dotenv import load_dotenv

# YouTube API import
try:
    from googleapiclient.discovery import build
    YOUTUBE_API_AVAILABLE = True
except ImportError:
    YOUTUBE_API_AVAILABLE = False

load_dotenv()


class DeepURLCollector:
    """ê¹Šì´ ìˆëŠ” ìë£Œ ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    # ìµœìš°ì„  ê²€ìƒ‰í•  YouTube ì±„ë„ (ëª¨ë“  ì–¸ì–´)
    TOP_PRIORITY_CHANNEL = '@1DANG100'  # ì¼ë‹¹ë°± - ìµœìš°ì„ 
    
    # ìš°ì„  ê²€ìƒ‰í•  YouTube ì±„ë„ (í•œê¸€)
    PRIORITY_KO_CHANNELS = [
        '@thewinterbookstore',  # ê²¨ìš¸ì„œì 
        '@chaegiljji',  # ì±…ì½ì°Œë¼
        '@humanitylearning',  # ì¸ë¬¸í•™TV íœ´ì‹ê°™ì€ ì§€ì‹
        '@mkkimtv',  # ê¹€ë¯¸ê²½TV
    ]
    
    # ì¶”ê°€ ê²€ìƒ‰í•  YouTube ì±„ë„ (í•œê¸€)
    ADDITIONAL_KO_CHANNELS = [
        '@Gwana',  # ê³¼ë‚˜
        '@jachung',  # ë¼ì´í”„í•´ì»¤ ìì²­
        '@channelyes24',  # ì±„ë„ì˜ˆìŠ¤
    ]
    
    # ìš°ì„  ê²€ìƒ‰í•  YouTube ì±„ë„ (ì˜ì–´)
    PRIORITY_EN_CHANNELS = [
        '@BTFC',  # Better Than Food
        '@ClimbtheStacks',  # Climb The Stacks
        '@JackEdwards',  # Jack Edwards
        '@arielbissett',  # Ariel Bissett
    ]
    
    # ì¶”ê°€ ê²€ìƒ‰í•  YouTube ì±„ë„ (ì˜ì–´)
    ADDITIONAL_EN_CHANNELS = [
        '@withcindy',  # Read with Cindy
        '@thebookleo',  # The Book Leo
        '@InsightJunkie',  # Insight Junkie
    ]
    
    # ì œì™¸í•  ë„ë©”ì¸ íŒ¨í„´
    EXCLUDED_DOMAINS = [
        'kyobobook.co.kr', 'yes24.com', 'aladin.co.kr', 'interpark.com',
        'amazon.com', 'amazon.co.kr', 'amazon.co.uk',
        'naver.com/shopping', 'coupang.com', '11st.co.kr',
        'gmarket.co.kr', 'auction.co.kr',
        'ko.wikipedia.org', 'namu.wiki', 'en.wikipedia.org',
        'wikidata.org', 'wikipedia.org',
    ]
    
    # ì œì™¸í•  URL íŒ¨í„´
    EXCLUDED_PATTERNS = [
        r'/product/', r'/goods/', r'/item/', r'/shop/',
        r'/isbn/', r'/book/', r'/detail/',
        r'/search\?', r'/category/',
    ]
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.ddgs = DDGS()
        
        # YouTube API ì´ˆê¸°í™”
        self.youtube = None
        if YOUTUBE_API_AVAILABLE:
            youtube_api_key = os.getenv("YOUTUBE_API_KEY") or os.getenv("GOOGLE_BOOKS_API_KEY")
            if youtube_api_key:
                try:
                    self.youtube = build('youtube', 'v3', developerKey=youtube_api_key)
                    print("âœ… YouTube API ì´ˆê¸°í™” ì™„ë£Œ")
                except Exception as e:
                    print(f"âš ï¸ YouTube API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def is_excluded_url(self, url: str) -> bool:
        """URLì´ ì œì™¸ ëŒ€ìƒì¸ì§€ í™•ì¸"""
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # ë„ë©”ì¸ ì²´í¬
        for excluded_domain in self.EXCLUDED_DOMAINS:
            if excluded_domain in domain:
                return True
        
        # URL íŒ¨í„´ ì²´í¬
        for pattern in self.EXCLUDED_PATTERNS:
            if re.search(pattern, url, re.IGNORECASE):
                return True
        
        return False
    
    def validate_url_content(self, url: str, book_title: str, author: str = None, strict: bool = True) -> Dict[str, any]:
        """URL ë‚´ìš© ê²€ì¦ - ì±… ì œëª©ì´ ì‹¤ì œë¡œ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        
        Args:
            url: ê²€ì¦í•  URL
            book_title: ì±… ì œëª©
            author: ì €ì ì´ë¦„
            strict: ì—„ê²©í•œ ê²€ì¦ ì—¬ë¶€ (Falseë©´ ë” ê´€ëŒ€í•˜ê²Œ ê²€ì¦)
        """
        try:
            response = self.session.get(url, timeout=10, allow_redirects=True)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = None
            if soup.title:
                title = soup.title.string.strip()
            elif soup.find('meta', property='og:title'):
                title = soup.find('meta', property='og:title').get('content', '').strip()
            
            # ì„¤ëª… ì¶”ì¶œ
            description = None
            if soup.find('meta', property='og:description'):
                description = soup.find('meta', property='og:description').get('content', '').strip()
            elif soup.find('meta', attrs={'name': 'description'}):
                description = soup.find('meta', attrs={'name': 'description'}).get('content', '').strip()
            
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°„ë‹¨íˆ)
            body_text = ''
            if soup.body:
                # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ ì œê±°
                for script in soup.body(["script", "style"]):
                    script.decompose()
                body_text = soup.body.get_text(separator=' ', strip=True)[:2000]  # ì²˜ìŒ 2000ìë§Œ
            
            # ê²€ì¦: ì±… ì œëª©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€
            combined_text = f"{title} {description} {body_text}".lower()
            book_title_lower = book_title.lower()
            
            # ì±… ì œëª©ì˜ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ (ê¸´ ì œëª©ì˜ ê²½ìš°)
            book_title_words = [w for w in book_title_lower.split() if len(w) > 2]
            # ì£¼ìš” í‚¤ì›Œë“œ (ì˜ˆ: "21ì„¸ê¸°ë¥¼ ìœ„í•œ 21ê°€ì§€ ì œì–¸" -> "21ì„¸ê¸°", "21ê°€ì§€", "ì œì–¸")
            key_words = [w for w in book_title_lower.split() if len(w) >= 3]
            
            # ì±… ì œëª©ì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ì œì™¸
            title_found = book_title_lower in combined_text
            key_words_found = any(word in combined_text for word in key_words) if key_words else False
            
            if not title_found and not key_words_found:
                # ì €ì ì´ë¦„ë„ í™•ì¸
                if author:
                    author_lower = author.lower()
                    author_parts = [w for w in author_lower.split() if len(w) > 2]
                    author_found = any(part in combined_text for part in author_parts) if author_parts else False
                    if not author_found:
                        if strict:
                            return {
                                'valid': False,
                                'reason': 'book_title_not_found',
                                'title': title,
                                'description': description[:200] if description else None
                            }
                        # ì—„ê²©í•˜ì§€ ì•Šìœ¼ë©´ ì €ì ì´ë¦„ë§Œ ìˆì–´ë„ í†µê³¼
                else:
                    if strict:
                        return {
                            'valid': False,
                            'reason': 'book_title_not_found',
                            'title': title,
                            'description': description[:200] if description else None
                        }
            
            # ë„ˆë¬´ ì§§ì€ ì½˜í…ì¸  ì²´í¬ (ì—„ê²© ëª¨ë“œì¼ ë•Œë§Œ, ë³¸ë¬¸ì´ 100ì ë¯¸ë§Œì´ë©´ ì œì™¸ - ì™„í™”)
            if strict and len(body_text) < 100 and not url.startswith('https://www.youtube.com'):
                return {
                    'valid': False,
                    'reason': 'content_too_short',
                    'title': title,
                    'description': description[:200] if description else None
                }
            
            return {
                'valid': True,
                'title': title,
                'description': description[:200] if description else None,
                'status_code': response.status_code
            }
            
        except requests.exceptions.Timeout:
            # íƒ€ì„ì•„ì›ƒì´ì–´ë„ URL ìì²´ëŠ” ìœ íš¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì œëª©ë§Œìœ¼ë¡œ íŒë‹¨
            return {'valid': True, 'title': None, 'description': None, 'reason': 'timeout_but_accepted'}
        except requests.exceptions.RequestException as e:
            # 403, 404 ë“± ì˜¤ë¥˜ëŠ” ì œì™¸í•˜ë˜, ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ì¼ë‹¨ í†µê³¼
            if '403' in str(e) or '404' in str(e) or 'Forbidden' in str(e):
                return {'valid': False, 'reason': str(e)}
            # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” ì¼ë‹¨ í†µê³¼ (ê²€ì¦ ì™„í™”)
            return {'valid': True, 'title': None, 'description': None, 'reason': 'request_error_but_accepted'}
        except Exception as e:
            # ê¸°íƒ€ ì˜ˆì™¸ëŠ” ì¼ë‹¨ í†µê³¼ (ê²€ì¦ ì™„í™”)
            return {'valid': True, 'title': None, 'description': None, 'reason': 'exception_but_accepted'}
    
    def search_channel_videos(self, channel_handle: str, book_title: str, author: str = None) -> List[Dict]:
        """íŠ¹ì • YouTube ì±„ë„ì—ì„œ ì±… ê´€ë ¨ ì˜ìƒ ê²€ìƒ‰"""
        if not self.youtube:
            return []
        
        videos = []
        
        try:
            # ì±„ë„ í•¸ë“¤ë¡œ ì±„ë„ ID ì°¾ê¸°
            channel_response = self.youtube.search().list(
                q=channel_handle,
                part='id,snippet',
                type='channel',
                maxResults=1
            ).execute()
            
            if not channel_response.get('items'):
                return []
            
            channel_id = channel_response['items'][0]['id']['channelId']
            
            # ì±„ë„ì˜ ì—…ë¡œë“œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ID ê°€ì ¸ì˜¤ê¸°
            channel_details = self.youtube.channels().list(
                part='contentDetails',
                id=channel_id
            ).execute()
            
            if not channel_details.get('items'):
                return []
            
            upload_playlist_id = channel_details['items'][0]['contentDetails']['relatedPlaylists']['uploads']
            
            # í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ì—ì„œ ì˜ìƒ ê²€ìƒ‰
            search_query = book_title
            if author:
                search_query = f"{book_title} {author}"
            
            # ì±„ë„ì˜ ëª¨ë“  ì˜ìƒì—ì„œ ê²€ìƒ‰
            playlist_items = []
            next_page_token = None
            
            while len(playlist_items) < 50:  # ìµœëŒ€ 50ê°œ ì˜ìƒ í™•ì¸
                try:
                    request_params = {
                        'part': 'snippet,contentDetails',
                        'playlistId': upload_playlist_id,
                        'maxResults': 50
                    }
                    if next_page_token:
                        request_params['pageToken'] = next_page_token
                    
                    response = self.youtube.playlistItems().list(**request_params).execute()
                    playlist_items.extend(response.get('items', []))
                    
                    next_page_token = response.get('nextPageToken')
                    if not next_page_token:
                        break
                except:
                    break
            
            # ì±… ì œëª©ê³¼ ê´€ë ¨ëœ ì˜ìƒ í•„í„°ë§
            for item in playlist_items:
                video_title = item['snippet']['title']
                video_description = item['snippet'].get('description', '')
                combined = f"{video_title} {video_description}".lower()
                
                # ì±… ì œëª©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if book_title.lower() not in combined:
                    if author:
                        author_lower = author.lower()
                        if not any(part in combined for part in author_lower.split() if len(part) > 2):
                            continue
                    else:
                        continue
                
                video_id = item['contentDetails']['videoId']
                
                # ì˜ìƒ ê¸¸ì´ í™•ì¸ (30ë¶„ ì´ìƒ)
                try:
                    video_response = self.youtube.videos().list(
                        part='contentDetails',
                        id=video_id
                    ).execute()
                    
                    duration_str = video_response['items'][0]['contentDetails']['duration']
                    duration_seconds = self._parse_duration(duration_str)
                    
                    if duration_seconds < 1800:  # 30ë¶„ ë¯¸ë§Œì´ë©´ ì œì™¸
                        continue
                except:
                    pass  # ê¸¸ì´ ì •ë³´ë¥¼ ëª» ê°€ì ¸ì™€ë„ ê³„ì† ì§„í–‰
                
                videos.append({
                    'url': f"https://www.youtube.com/watch?v={video_id}",
                    'title': video_title,
                    'description': video_description[:200],
                    'type': 'youtube',
                    'channel': '@1DANG100'
                })
            
        except Exception as e:
            print(f"  âš ï¸ ì±„ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        return videos
    
    def search_youtube_videos(self, book_title: str, author: str = None, max_results: int = 10, lang: str = 'ko') -> List[Dict]:
        """YouTubeì—ì„œ ê¸´ ë¦¬ë·°/í•´ì„¤/ê°•ì˜ ì˜ìƒ ê²€ìƒ‰"""
        if not self.youtube:
            return []
        
        videos = []
        seen_video_ids = set()
        
        # 1. ì¼ë‹¹ë°±(@1DANG100) ì±„ë„ ìµœìš°ì„  ê²€ìƒ‰
        print(f"  ğŸ“º [{self.TOP_PRIORITY_CHANNEL}] ì¼ë‹¹ë°± ì±„ë„ ìµœìš°ì„  ê²€ìƒ‰ ì¤‘...")
        channel_videos = self.search_channel_videos(self.TOP_PRIORITY_CHANNEL, book_title, author)
        for video in channel_videos:
            video_id = video['url'].split('v=')[-1].split('&')[0]
            if video_id not in seen_video_ids:
                seen_video_ids.add(video_id)
                videos.append(video)
                print(f"    âœ“ [ì¼ë‹¹ë°± ìµœìš°ì„ ] {video['title'][:60]}...")
        
        # 2. ìš°ì„ ìˆœìœ„ ì±„ë„ ê²€ìƒ‰
        priority_channels = self.PRIORITY_KO_CHANNELS if lang == 'ko' else self.PRIORITY_EN_CHANNELS
        print(f"  ğŸ“º ìš°ì„ ìˆœìœ„ ì±„ë„ì—ì„œ ê²€ìƒ‰ ì¤‘ ({len(priority_channels)}ê°œ ì±„ë„)...")
        for channel_handle in priority_channels:
            if len(videos) >= max_results:
                break
            channel_videos = self.search_channel_videos(channel_handle, book_title, author)
            for video in channel_videos:
                video_id = video['url'].split('v=')[-1].split('&')[0]
                if video_id not in seen_video_ids:
                    seen_video_ids.add(video_id)
                    videos.append(video)
                    channel_name = channel_handle.replace('@', '')
                    print(f"    âœ“ [{channel_name}] {video['title'][:60]}...")
                    if len(videos) >= max_results:
                        break
        
        # 3. ì¶”ê°€ ì±„ë„ ê²€ìƒ‰ (ì—¬ìœ ê°€ ìˆì„ ë•Œ)
        if len(videos) < max_results:
            additional_channels = self.ADDITIONAL_KO_CHANNELS if lang == 'ko' else self.ADDITIONAL_EN_CHANNELS
            print(f"  ğŸ“º ì¶”ê°€ ì±„ë„ì—ì„œ ê²€ìƒ‰ ì¤‘ ({len(additional_channels)}ê°œ ì±„ë„)...")
            for channel_handle in additional_channels:
                if len(videos) >= max_results:
                    break
                channel_videos = self.search_channel_videos(channel_handle, book_title, author)
                for video in channel_videos:
                    video_id = video['url'].split('v=')[-1].split('&')[0]
                    if video_id not in seen_video_ids:
                        seen_video_ids.add(video_id)
                        videos.append(video)
                        channel_name = channel_handle.replace('@', '')
                        print(f"    âœ“ [{channel_name}] {video['title'][:60]}...")
                        if len(videos) >= max_results:
                            break
        
        # 4. ì¼ë°˜ ê²€ìƒ‰ (ì±„ë„ ê²€ìƒ‰ìœ¼ë¡œ ë¶€ì¡±í•  ë•Œ)
        if len(videos) < max_results:
            print("  ğŸ“º ì¼ë°˜ YouTube ê²€ìƒ‰ ì¤‘...")
            # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            if lang == 'ko':
                queries = [
                    f"{book_title} {author} í•´ì„¤" if author else f"{book_title} í•´ì„¤",
                    f"{book_title} {author} ë¶„ì„" if author else f"{book_title} ë¶„ì„",
                    f"{book_title} {author} ê°•ì˜" if author else f"{book_title} ê°•ì˜",
                    f"{book_title} {author} ê°•ì—°" if author else f"{book_title} ê°•ì—°",
                    f"{book_title} {author} ë¶í† í¬" if author else f"{book_title} ë¶í† í¬",
                    f"{book_title} {author} ë¦¬ë·°" if author else f"{book_title} ë¦¬ë·°",
                ]
            else:
                queries = [
                    f"{book_title} {author} analysis" if author else f"{book_title} analysis",
                    f"{book_title} {author} lecture" if author else f"{book_title} lecture",
                    f"{book_title} {author} review" if author else f"{book_title} review",
                    f"{book_title} {author} discussion" if author else f"{book_title} discussion",
                    f"{book_title} {author} book talk" if author else f"{book_title} book talk",
                ]
            
            region = 'KR' if lang == 'ko' else 'US'
        
        for query in queries:
            if len(videos) >= max_results:
                break
            
            try:
                search_response = self.youtube.search().list(
                    q=query,
                    part='id,snippet',
                    type='video',
                    maxResults=5,
                    order='relevance',
                    regionCode=region,
                    videoDuration='long'  # ê¸´ ì˜ìƒë§Œ (20ë¶„ ì´ìƒ, ì‹¤ì œë¡œëŠ” 30ë¶„ ì´ìƒ í•„í„°ë§)
                ).execute()
                
                for item in search_response.get('items', []):
                    if len(videos) >= max_results:
                        break
                    
                    video_id = item['id']['videoId']
                    if video_id in seen_video_ids:
                        continue
                    
                    video_title = item['snippet']['title']
                    video_description = item['snippet'].get('description', '')
                    
                    # ì±… ì œëª©ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    combined = f"{video_title} {video_description}".lower()
                    if book_title.lower() not in combined:
                        if author:
                            author_lower = author.lower()
                            if not any(part in combined for part in author_lower.split() if len(part) > 2):
                                continue
                        else:
                            continue
                    
                    # ì˜ìƒ ê¸¸ì´ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì„ íƒì )
                    try:
                        video_response = self.youtube.videos().list(
                            part='contentDetails',
                            id=video_id
                        ).execute()
                        
                        duration_str = video_response['items'][0]['contentDetails']['duration']
                        # PT15M30S í˜•ì‹ì„ íŒŒì‹±
                        duration_seconds = self._parse_duration(duration_str)
                        
                        # 30ë¶„(1800ì´ˆ) ë¯¸ë§Œì´ë©´ ì œì™¸ (ë¡±í¼ ì½˜í…ì¸ ë§Œ)
                        if duration_seconds < 1800:
                            continue
                    except:
                        pass  # ê¸¸ì´ ì •ë³´ë¥¼ ëª» ê°€ì ¸ì™€ë„ ê³„ì† ì§„í–‰
                    
                    seen_video_ids.add(video_id)
                    video_url = f"https://www.youtube.com/watch?v={video_id}"
                    videos.append({
                        'url': video_url,
                        'title': video_title,
                        'description': video_description[:200],
                        'type': 'youtube'
                    })
                    print(f"    âœ“ YouTube: {video_title[:60]}...")
                
                time.sleep(0.5)  # API ì œí•œ ë°©ì§€
                
            except Exception as e:
                print(f"  âš ï¸ YouTube ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        return videos
    
    def _parse_duration(self, duration_str: str) -> int:
        """ISO 8601 duration í˜•ì‹ íŒŒì‹± (PT15M30S -> ì´ˆ)"""
        import re
        pattern = r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?'
        match = re.match(pattern, duration_str)
        if match:
            hours = int(match.group(1) or 0)
            minutes = int(match.group(2) or 0)
            seconds = int(match.group(3) or 0)
            return hours * 3600 + minutes * 60 + seconds
        return 0
    
    def search_web_urls(self, book_title: str, author: str = None, max_results: int = 50, lang: str = 'ko') -> List[Dict]:
        """ì›¹ì—ì„œ ê¹Šì´ ìˆëŠ” ìë£Œ ê²€ìƒ‰ (PDF, ë…¼ë¬¸, í•™ìˆ  ì‚¬ì´íŠ¸ í¬í•¨)"""
        urls = []
        seen_urls = set()
        
        # í•™ìˆ  ì‚¬ì´íŠ¸ ë° íŠ¹ì • ì‚¬ì´íŠ¸ ëª©ë¡
        academic_sites_ko = [
            'site:academia.edu',
            'site:researchgate.net',
            'site:jstor.org',
            'site:scholar.google.com',
            'site:dbpia.co.kr',
            'site:kci.go.kr',
            'site:riss.kr',
            'site:brunch.co.kr',
            'site:medium.com',
            'site:blog.naver.com',
            'site:blog.daum.net',
            'site:post.naver.com',
        ]
        
        academic_sites_en = [
            'site:academia.edu',
            'site:researchgate.net',
            'site:jstor.org',
            'site:scholar.google.com',
            'site:medium.com',
            'site:theguardian.com',
            'site:nytimes.com',
            'site:newyorker.com',
            'site:lrb.co.uk',  # London Review of Books
            'site:nybooks.com',  # New York Review of Books
        ]
        
        if lang == 'ko':
            # ì¼ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬
            queries = [
                f'"{book_title}" {author} í•´ì„¤' if author else f'"{book_title}" í•´ì„¤',
                f'"{book_title}" {author} ë¶„ì„' if author else f'"{book_title}" ë¶„ì„',
                f'"{book_title}" {author} ë¹„í‰' if author else f'"{book_title}" ë¹„í‰',
                f'"{book_title}" {author} ë…¼ë¬¸' if author else f'"{book_title}" ë…¼ë¬¸',
                f'"{book_title}" {author} ì—ì„¸ì´' if author else f'"{book_title}" ì—ì„¸ì´',
                f'"{book_title}" {author} ê°•ì˜ìë£Œ' if author else f'"{book_title}" ê°•ì˜ìë£Œ',
                f'"{book_title}" {author} ë…í›„ê°' if author else f'"{book_title}" ë…í›„ê°',
                f'"{book_title}" {author} ì„œí‰' if author else f'"{book_title}" ì„œí‰',
                f'"{book_title}" {author} ë¦¬ë·°' if author else f'"{book_title}" ë¦¬ë·°',
            ]
            
            # PDF íŒŒì¼ ê²€ìƒ‰
            pdf_queries = [
                f'"{book_title}" {author} filetype:pdf' if author else f'"{book_title}" filetype:pdf',
                f'"{book_title}" {author} ë…¼ë¬¸ pdf' if author else f'"{book_title}" ë…¼ë¬¸ pdf',
                f'"{book_title}" {author} ê°•ì˜ìë£Œ pdf' if author else f'"{book_title}" ê°•ì˜ìë£Œ pdf',
            ]
            
            # í•™ìˆ  ì‚¬ì´íŠ¸ ê²€ìƒ‰
            for site in academic_sites_ko:
                queries.append(f'"{book_title}" {author} {site}' if author else f'"{book_title}" {site}')
        else:
            # ì¼ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬
            queries = [
                f'"{book_title}" {author} analysis' if author else f'"{book_title}" analysis',
                f'"{book_title}" {author} review' if author else f'"{book_title}" review',
                f'"{book_title}" {author} essay' if author else f'"{book_title}" essay',
                f'"{book_title}" {author} critique' if author else f'"{book_title}" critique',
                f'"{book_title}" {author} lecture' if author else f'"{book_title}" lecture',
                f'"{book_title}" {author} discussion' if author else f'"{book_title}" discussion',
            ]
            
            # PDF íŒŒì¼ ê²€ìƒ‰
            pdf_queries = [
                f'"{book_title}" {author} filetype:pdf' if author else f'"{book_title}" filetype:pdf',
                f'"{book_title}" {author} paper pdf' if author else f'"{book_title}" paper pdf',
                f'"{book_title}" {author} essay pdf' if author else f'"{book_title}" essay pdf',
            ]
            
            # í•™ìˆ  ì‚¬ì´íŠ¸ ê²€ìƒ‰
            for site in academic_sites_en:
                queries.append(f'"{book_title}" {author} {site}' if author else f'"{book_title}" {site}')
        
        # PDF ê²€ìƒ‰ ì¿¼ë¦¬ ì¶”ê°€
        queries.extend(pdf_queries)
        
        for query in queries:
            if len(urls) >= max_results:
                break
            
            try:
                print(f"  ê²€ìƒ‰ ì¤‘: {query[:60]}...")
                # ë” ë§ì€ ê²°ê³¼ ìˆ˜ì§‘ (ìµœëŒ€ 20ê°œ)
                results = list(self.ddgs.text(query, max_results=20))
                
                for result in results:
                    if len(urls) >= max_results:
                        break
                    
                    url = result.get('href', '')
                    if not url or url in seen_urls:
                        continue
                    
                    # ì œì™¸ URL ì²´í¬
                    if self.is_excluded_url(url):
                        continue
                    
                    # PDF íŒŒì¼ì€ ê²€ì¦ ì™„í™” (ì œëª©ë§Œ í™•ì¸)
                    is_pdf = url.lower().endswith('.pdf') or '/pdf' in url.lower() or url.lower().endswith('.pdf?')
                    if is_pdf:
                        # PDFëŠ” ì œëª©ë§Œ ê°„ë‹¨íˆ í™•ì¸
                        title = result.get('title', '').lower()
                        body = result.get('body', '').lower()
                        combined = f"{title} {body}".lower()
                        if book_title.lower() not in combined:
                            if author:
                                author_lower = author.lower()
                                if not any(part in combined for part in author_lower.split() if len(part) > 2):
                                    continue
                            else:
                                continue
                        seen_urls.add(url)
                        urls.append({
                            'url': url,
                            'title': result.get('title', ''),
                            'description': result.get('body', '')[:200] if result.get('body') else '',
                            'type': 'pdf'
                        })
                        print(f"    âœ“ PDF: {url[:80]}...")
                        continue
                    
                    # í•™ìˆ  ì‚¬ì´íŠ¸ëŠ” ê²€ì¦ ì™„í™”
                    is_academic = any(site in url.lower() for site in [
                        'academia.edu', 'researchgate.net', 'jstor.org', 
                        'scholar.google.com', 'dbpia.co.kr', 'kci.go.kr', 'riss.kr'
                    ])
                    
                    # ë¸”ë¡œê·¸/ë¯¸ë””ì—„ ë“±ì€ ê²€ì¦ ë” ì™„í™”
                    is_blog = any(site in url.lower() for site in [
                        'blog.naver.com', 'blog.daum.net', 'post.naver.com',
                        'medium.com', 'brunch.co.kr', 'tistory.com'
                    ])
                    
                    # URL ê²€ì¦ (í•™ìˆ  ì‚¬ì´íŠ¸ì™€ ë¸”ë¡œê·¸ëŠ” ë” ê´€ëŒ€í•˜ê²Œ)
                    validation = self.validate_url_content(url, book_title, author, strict=not (is_academic or is_blog))
                    if not validation.get('valid'):
                        # ë””ë²„ê¹…: ì™œ ì œì™¸ë˜ì—ˆëŠ”ì§€ ë¡œê·¸ (ì²˜ìŒ ëª‡ ê°œë§Œ)
                        if len(urls) < 5:
                            reason = validation.get('reason', 'unknown')
                            print(f"    â­ï¸ ì œì™¸ë¨ ({reason}): {url[:60]}...")
                        continue
                    
                    seen_urls.add(url)
                    urls.append({
                        'url': url,
                        'title': validation.get('title', ''),
                        'description': validation.get('description', ''),
                        'type': 'web'
                    })
                    print(f"    âœ“ {url[:80]}...")
                
                time.sleep(0.5)  # ìš”ì²­ ê°„ ëŒ€ê¸° (ì†ë„ ê°œì„ )
                
            except Exception as e:
                print(f"  âš ï¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                continue
        
        return urls
    
    def collect_urls(self, book_title: str, author: str = None, total_results: int = 30) -> Tuple[List[str], List[str]]:
        """í•œê¸€/ì˜ì–´ ìë£Œ ìˆ˜ì§‘ (ê°€ëŠ¥í•œ í•œ ë§ì´ ìˆ˜ì§‘, YouTube 50% ëª©í‘œ)"""
        # ìµœì†Œ ëª©í‘œëŠ” 30ê°œì´ì§€ë§Œ, ë” ë§ì´ ìˆ˜ì§‘í•˜ë„ë¡ ë³€ê²½
        min_ko_count = total_results // 2
        min_en_count = total_results - min_ko_count
        
        # YouTube ëª©í‘œ: ì „ì²´ì˜ 50%
        youtube_target_ko = max(15, min_ko_count // 2)
        youtube_target_en = max(15, min_en_count // 2)
        
        print(f"ğŸ” '{book_title}' ê´€ë ¨ ê¹Šì´ ìˆëŠ” ìë£Œ ìˆ˜ì§‘ ì¤‘...")
        if author:
            print(f"   ì‘ê°€: {author}")
        print(f"   ëª©í‘œ: ìµœì†Œ í•œê¸€ {min_ko_count}ê°œ + ì˜ì–´ {min_en_count}ê°œ (ê°€ëŠ¥í•œ í•œ ë§ì´ ìˆ˜ì§‘)")
        print(f"   YouTube ëª©í‘œ: í•œê¸€ {youtube_target_ko}ê°œ, ì˜ì–´ {youtube_target_en}ê°œ (30ë¶„ ì´ìƒ ì˜ìƒ)\n")
        
        # í•œê¸€ ìë£Œ ìˆ˜ì§‘
        print("=" * 60)
        print("ğŸ“š í•œê¸€ ìë£Œ ìˆ˜ì§‘ ì¤‘...")
        print("=" * 60)
        
        ko_urls = []
        seen_ko_urls = set()
        
        # YouTube ì˜ìƒ (í•œê¸€) - 50% ëª©í‘œë¡œ ì ê·¹ ìˆ˜ì§‘
        print("  ğŸ“º YouTube ì˜ìƒ ê²€ìƒ‰ ì¤‘ (30ë¶„ ì´ìƒ)...")
        try:
            youtube_ko = self.search_youtube_videos(book_title, author, max_results=youtube_target_ko * 2, lang='ko')
            youtube_count = 0
            for item in youtube_ko:
                if item['url'] not in seen_ko_urls:
                    seen_ko_urls.add(item['url'])
                    ko_urls.append(item['url'])
                    youtube_count += 1
                    if youtube_count >= youtube_target_ko:
                        break
            print(f"  âœ… YouTube ì˜ìƒ {youtube_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        except Exception as e:
            print(f"  âš ï¸ YouTube ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        
        # ì›¹ ìë£Œ (í•œê¸€) - ë‚˜ë¨¸ì§€ 50% ìˆ˜ì§‘
        print("  ğŸŒ ì›¹ ìë£Œ ê²€ìƒ‰ ì¤‘...")
        web_ko = self.search_web_urls(book_title, author, max_results=50, lang='ko')
        web_count = 0
        for item in web_ko:
            if item['url'] not in seen_ko_urls:
                seen_ko_urls.add(item['url'])
                ko_urls.append(item['url'])
                web_count += 1
                # YouTubeê°€ ë¶€ì¡±í•˜ë©´ ì›¹ ìë£Œë¥¼ ë” ë§ì´ ìˆ˜ì§‘
                if len(ko_urls) >= min_ko_count and web_count >= youtube_count:
                    break
        print(f"  âœ… ì›¹ ìë£Œ {web_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ìµœì†Œ ëª©í‘œëŠ” ë‹¬ì„±í–ˆëŠ”ì§€ í™•ì¸í•˜ê³ , ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ê²€ìƒ‰
        if len(ko_urls) < min_ko_count:
            print(f"  âš ï¸ í•œê¸€ ìë£Œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ ({len(ko_urls)}/{min_ko_count}). ì¶”ê°€ ê²€ìƒ‰ ì¤‘...")
            additional_ko = self.search_web_urls(book_title, author, max_results=min_ko_count - len(ko_urls) + 10, lang='ko')
            for item in additional_ko:
                if item['url'] not in seen_ko_urls:
                    seen_ko_urls.add(item['url'])
                    ko_urls.append(item['url'])
        
        # ì˜ì–´ ìë£Œ ìˆ˜ì§‘
        print("\n" + "=" * 60)
        print("ğŸ“š English Resources Collection...")
        print("=" * 60)
        
        en_urls = []
        seen_en_urls = set()
        
        # YouTube ì˜ìƒ (ì˜ì–´) - 50% ëª©í‘œë¡œ ì ê·¹ ìˆ˜ì§‘
        print("  ğŸ“º YouTube Videos Search (30+ minutes)...")
        try:
            youtube_en = self.search_youtube_videos(book_title, author, max_results=youtube_target_en * 2, lang='en')
            youtube_count = 0
            for item in youtube_en:
                if item['url'] not in seen_en_urls:
                    seen_en_urls.add(item['url'])
                    en_urls.append(item['url'])
                    youtube_count += 1
                    if youtube_count >= youtube_target_en:
                        break
            print(f"  âœ… YouTube Videos {youtube_count} collected")
        except Exception as e:
            print(f"  âš ï¸ YouTube Search Error: {e}")
        
        # ì›¹ ìë£Œ (ì˜ì–´) - ë‚˜ë¨¸ì§€ 50% ìˆ˜ì§‘
        print("  ğŸŒ Web Resources Search...")
        web_en = self.search_web_urls(book_title, author, max_results=50, lang='en')
        web_count = 0
        for item in web_en:
            if item['url'] not in seen_en_urls:
                seen_en_urls.add(item['url'])
                en_urls.append(item['url'])
                web_count += 1
                # YouTubeê°€ ë¶€ì¡±í•˜ë©´ ì›¹ ìë£Œë¥¼ ë” ë§ì´ ìˆ˜ì§‘
                if len(en_urls) >= min_en_count and web_count >= youtube_count:
                    break
        print(f"  âœ… Web Resources {web_count} collected")
        
        # ìµœì†Œ ëª©í‘œëŠ” ë‹¬ì„±í–ˆëŠ”ì§€ í™•ì¸í•˜ê³ , ë¶€ì¡±í•˜ë©´ ì¶”ê°€ ê²€ìƒ‰
        if len(en_urls) < min_en_count:
            print(f"  âš ï¸ English resources insufficient ({len(en_urls)}/{min_en_count}). Additional search...")
            additional_en = self.search_web_urls(book_title, author, max_results=min_en_count - len(en_urls) + 10, lang='en')
            for item in additional_en:
                if item['url'] not in seen_en_urls:
                    seen_en_urls.add(item['url'])
                    en_urls.append(item['url'])
        
        print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ:")
        print(f"   í•œê¸€: {len(ko_urls)}ê°œ")
        print(f"   ì˜ì–´: {len(en_urls)}ê°œ")
        print(f"   ì´ê³„: {len(ko_urls) + len(en_urls)}ê°œ")
        if len(ko_urls) + len(en_urls) >= total_results:
            print(f"   âœ… ëª©í‘œ ë‹¬ì„±! (ëª©í‘œ: {total_results}ê°œ ì´ìƒ)")
        print()
        
        return ko_urls, en_urls
    
    def save_urls(self, book_title: str, ko_urls: List[str], en_urls: List[str], author: str = None) -> str:
        """URLì„ íŒŒì¼ë¡œ ì €ì¥ (NotebookLM í˜•ì‹)"""
        from utils.file_utils import safe_title
        
        safe_title_str = safe_title(book_title)
        output_dir = Path("assets/urls")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        md_path = output_dir / f"{safe_title_str}_notebooklm.md"
        
        total_urls = ko_urls + en_urls
        
        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# {book_title} - NotebookLM ì†ŒìŠ¤ URL\n\n")
            if author:
                f.write(f"**ì‘ê°€**: {author}  \n")
            f.write(f"**ì´ {len(total_urls)}ê°œì˜ URL (í•œê¸€ {len(ko_urls)}ê°œ + ì˜ì–´ {len(en_urls)}ê°œ)**\n\n")
            
            f.write("## ğŸ“‹ URL ë¦¬ìŠ¤íŠ¸\n\n")
            f.write("ì•„ë˜ URLë“¤ì„ ë³µì‚¬í•˜ì—¬ NotebookLMì— ì†ŒìŠ¤ë¡œ ì¶”ê°€í•˜ì„¸ìš”.\n\n")
            
            # URLë§Œ ì¶œë ¥ (ì„¤ëª… ì—†ì´)
            for url in total_urls:
                f.write(f"{url}\n")
        
        print(f"ğŸ’¾ URL ì €ì¥ ì™„ë£Œ: {md_path}")
        return str(md_path)


def load_books_from_csv(csv_path: str = "data/ildangbaek_books.csv") -> List[Tuple[str, str]]:
    """CSVì—ì„œ ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•Šì€ ì±… ëª©ë¡ ë¡œë“œ"""
    books = []
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            title = row.get('title', '').strip()
            author = row.get('author', '').strip()
            status = row.get('status', '').strip()
            
            # ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•Šì€ ì±…ë§Œ (not_processed)
            if status == 'not_processed' and title:
                books.append((title, author))
    
    return books


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ê¹Šì´ ìˆëŠ” ìë£Œ URL ìˆ˜ì§‘ (NotebookLMìš©)')
    parser.add_argument('--title', type=str, help='ì±… ì œëª©')
    parser.add_argument('--author', type=str, help='ì €ì ì´ë¦„')
    parser.add_argument('--csv', action='store_true', help='CSVì—ì„œ ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•Šì€ ì±…ë“¤ ëª¨ë‘ ì²˜ë¦¬')
    parser.add_argument('--num', type=int, default=30, help='ì´ ìˆ˜ì§‘í•  URL ê°œìˆ˜ (ê¸°ë³¸ê°’: 30)')
    
    args = parser.parse_args()
    
    collector = DeepURLCollector()
    
    if args.csv:
        # CSVì—ì„œ ì±… ëª©ë¡ ë¡œë“œ
        books = load_books_from_csv()
        
        if not books:
            print("ğŸ“­ ì²˜ë¦¬í•  ì±…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“š ì´ {len(books)}ê°œì˜ ì±…ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n")
        
        for i, (title, author) in enumerate(books, 1):
            print(f"\n{'='*80}")
            print(f"[{i}/{len(books)}] {title}" + (f" - {author}" if author else ""))
            print(f"{'='*80}\n")
            
            try:
                ko_urls, en_urls = collector.collect_urls(title, author, args.num)
                
                if ko_urls or en_urls:
                    collector.save_urls(title, ko_urls, en_urls, author)
                    print(f"âœ… ì™„ë£Œ: {title} (í•œê¸€ {len(ko_urls)}ê°œ, ì˜ì–´ {len(en_urls)}ê°œ)")
                else:
                    print(f"âš ï¸ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {title}")
                
                # ìš”ì²­ ê°„ ëŒ€ê¸°
                if i < len(books):
                    print("\nâ³ 3ì´ˆ ëŒ€ê¸° ì¤‘...\n")
                    time.sleep(3)
                    
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {title} - {str(e)}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"\n{'='*80}")
        print("âœ… ëª¨ë“  ì±… ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*80}\n")
        
    elif args.title:
        # ë‹¨ì¼ ì±… ì²˜ë¦¬
        ko_urls, en_urls = collector.collect_urls(args.title, args.author, args.num)
        
        if ko_urls or en_urls:
            collector.save_urls(args.title, ko_urls, en_urls, args.author)
            print("\nâœ… URL ìˆ˜ì§‘ ì™„ë£Œ!")
        else:
            print("\nâŒ ìˆ˜ì§‘ëœ URLì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("âŒ --title ë˜ëŠ” --csv ì˜µì…˜ì„ ì§€ì •í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()

