#!/usr/bin/env python3
"""
ì¼ë‹¹ë°± ì—í”¼ì†Œë“œ ì˜ìƒ ë©”íƒ€ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸

Part 1ê³¼ Part 2ë¡œ êµ¬ì„±ëœ ì—í”¼ì†Œë“œ ì˜ìƒì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import sys
import json
import argparse
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Tuple, List

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.file_utils import get_standard_safe_title
from src.utils.logger import setup_logger
from src.utils.translations import translate_book_title, translate_author_name, translate_book_title_to_korean, is_english_title
from src.utils.affiliate_links import generate_affiliate_section

# ë¡œê±° ì„¤ì •
logger = setup_logger(__name__)


def contains_korean(text: str) -> bool:
    """
    í…ìŠ¤íŠ¸ì— í•œêµ­ì–´ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    
    Args:
        text: í™•ì¸í•  í…ìŠ¤íŠ¸
        
    Returns:
        í•œêµ­ì–´ ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ True
    """
    import re
    korean_pattern = re.compile(r'[ê°€-í£]')
    return bool(korean_pattern.search(text))


def remove_korean_from_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í•œêµ­ì–´ ë¬¸ìë¥¼ ì œê±°
    
    Args:
        text: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
        
    Returns:
        í•œêµ­ì–´ê°€ ì œê±°ëœ í…ìŠ¤íŠ¸
    """
    import re
    korean_pattern = re.compile(r'[ê°€-í£]')
    return korean_pattern.sub('', text).strip()


def ensure_english_only(text: str, fallback: str = "") -> str:
    """
    í…ìŠ¤íŠ¸ê°€ ì˜ì–´ë§Œ í¬í•¨í•˜ë„ë¡ ë³´ì¥ (í•œêµ­ì–´ê°€ ìˆìœ¼ë©´ ì œê±°)
    
    Args:
        text: í™•ì¸í•  í…ìŠ¤íŠ¸
        fallback: í•œêµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆê³  ì œê±° í›„ ë¹ˆ ë¬¸ìì—´ì´ ë˜ë©´ ì‚¬ìš©í•  ê¸°ë³¸ê°’
        
    Returns:
        ì˜ì–´ë§Œ í¬í•¨ëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return fallback
    
    if contains_korean(text):
        cleaned = remove_korean_from_text(text)
        if not cleaned.strip():
            return fallback
        return cleaned.strip()
    
    return text


def detect_book_genre(book_title: str, book_info: Optional[Dict] = None) -> Tuple[str, str]:
    """
    ì±…ì˜ ì¥ë¥´ë¥¼ ê°ì§€í•˜ì—¬ í•œê¸€/ì˜ë¬¸ ìš©ì–´ ë°˜í™˜
    
    Args:
        book_title: ì±… ì œëª©
        book_info: ì±… ì •ë³´ ë”•ì…”ë„ˆë¦¬ (ì„ íƒì‚¬í•­)
        
    Returns:
        (í•œê¸€_ìš©ì–´, ì˜ë¬¸_ìš©ì–´) íŠœí”Œ
        ì˜ˆ: ("ì†Œì„¤", "Novel"), ("ì‹œ", "Poetry"), ("ìˆ˜í•„", "Essay"), ("ì‘í’ˆ", "Work")
    """
    title_lower = book_title.lower()
    
    # book_infoì—ì„œ categories í™•ì¸
    if book_info and 'categories' in book_info:
        categories = book_info['categories']
        for category in categories:
            category_lower = category.lower()
            if 'ì†Œì„¤' in category_lower or 'novel' in category_lower or 'fiction' in category_lower:
                return ("ì†Œì„¤", "Novel")
            elif 'ì‹œ' in category_lower or 'poetry' in category_lower or 'poem' in category_lower:
                return ("ì‹œ", "Poetry")
            elif 'ìˆ˜í•„' in category_lower or 'essay' in category_lower:
                return ("ìˆ˜í•„", "Essay")
            elif 'ë…¼í”½ì…˜' in category_lower or 'non-fiction' in category_lower or 'nonfiction' in category_lower:
                return ("ì¸ë¬¸í•™", "Humanities")
            elif 'ì² í•™' in category_lower or 'philosophy' in category_lower:
                return ("ì² í•™", "Philosophy")
            elif 'ê³¼í•™' in category_lower or 'science' in category_lower:
                return ("ê³¼í•™", "Science")
            elif 'ê²½ì œ' in category_lower or 'economy' in category_lower or 'business' in category_lower:
                return ("ê²½ì œê²½ì˜", "Business")
            elif 'ìê¸°ê³„ë°œ' in category_lower or 'self-help' in category_lower:
                return ("ìê¸°ê³„ë°œ", "SelfHelp")
            elif 'ì—­ì‚¬' in category_lower or 'history' in category_lower:
                return ("ì—­ì‚¬", "History")
    
    # ì œëª©ì—ì„œ í‚¤ì›Œë“œë¡œ ì¥ë¥´ ì¶”ì •
    # ì£¼ì˜: "ì†Œì„¤"ì„ ë¨¼ì € ì²´í¬ (ë‹¤ë¥¸ ë‹¨ì–´ì— í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
    # ì˜ˆ: "ê²½ì˜ë¥¼ í‘œí•˜ì‹œì˜¤"ì— "ì‹œ"ê°€ í¬í•¨ë˜ì§€ë§Œ, "ì†Œì„¤"ì´ ë” ëª…í™•í•œ ì¥ë¥´ ì§€í‘œ
    import re
    
    # í•œê¸€ì˜ ê²½ìš° ë‹¨ì–´ ê²½ê³„ë¥¼ ì •í™•íˆ ì²´í¬í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ, ë” ê¸´ íŒ¨í„´ì„ ìš°ì„  ì²´í¬
    # "ì†Œì„¤" ê´€ë ¨ íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
    if re.search(r'ì†Œì„¤', book_title) or 'novel' in title_lower or 'fiction' in title_lower:
        return ("ì†Œì„¤", "Novel")
    # "ì‹œ" ê´€ë ¨ íŒ¨í„´ - "ì‹œì§‘", "ì‹œì¸", "ì‹œì„ " ë“± ëª…í™•í•œ íŒ¨í„´ë§Œ ì²´í¬
    # ë‹¨, "ê²½ì˜", "ì‹œê°", "ì‹œì¥" ë“±ì€ ì œì™¸í•˜ê¸° ìœ„í•´ ë” ê¸´ íŒ¨í„´ ìš°ì„ 
    elif re.search(r'ì‹œì§‘|ì‹œì¸|ì‹œì„ |ì‹œí™”', book_title) or 'poetry' in title_lower or 'poem' in title_lower:
        return ("ì‹œ", "Poetry")
    # "ìˆ˜í•„" ê´€ë ¨ íŒ¨í„´
    elif re.search(r'ìˆ˜í•„', book_title) or 'essay' in title_lower:
        return ("ìˆ˜í•„", "Essay")
    # "ì² í•™" ê´€ë ¨ íŒ¨í„´
    elif re.search(r'ì² í•™', book_title) or 'philosophy' in title_lower:
        return ("ì² í•™", "Philosophy")
    # "ê³¼í•™" ê´€ë ¨ íŒ¨í„´
    elif re.search(r'ê³¼í•™', book_title) or 'science' in title_lower:
        return ("ê³¼í•™", "Science")
    # "ê²½ì œ/ê²½ì˜" ê´€ë ¨ íŒ¨í„´
    elif re.search(r'ê²½ì œ|ê²½ì˜|ë¶€ì|ëˆ', book_title) or 'economy' in title_lower or 'business' in title_lower or 'marketing' in title_lower:
        return ("ê²½ì œê²½ì˜", "Business")
    # "ì—­ì‚¬" ê´€ë ¨ íŒ¨í„´
    elif re.search(r'ì—­ì‚¬', book_title) or 'history' in title_lower:
        return ("ì—­ì‚¬", "History")
    # "ë…¼í”½ì…˜" ê´€ë ¨ íŒ¨í„´
    elif 'ë…¼í”½ì…˜' in book_title or 'non-fiction' in title_lower or 'nonfiction' in title_lower:
        return ("ì¸ë¬¸í•™", "Humanities")
    
    # ê¸°ë³¸ê°’: ì†Œì„¤ (í•˜ìœ„ í˜¸í™˜ì„±)
    return ("ì†Œì„¤", "Novel")


def generate_episode_title(
    book_title: str,
    language: str = "ko",
    book_info: Optional[Dict] = None,
    author: Optional[str] = None
) -> str:
    """
    ì—í”¼ì†Œë“œ ì˜ìƒ ì œëª© ìƒì„±
    
    âš ï¸ ì¤‘ìš”: ê° ì–¸ì–´ë³„ë¡œ í•´ë‹¹ ì–¸ì–´ì˜ ì œëª©ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    YouTubeì˜ ë‹¤êµ­ì–´ ë©”íƒ€ë°ì´í„° ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì‹œì²­ìì˜ ì–¸ì–´ ì„¤ì •ì— ë”°ë¼ ìë™ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.
    
    ì œëª© í¬ë§· ê·œì¹™:
    - ì¼ë‹¹ë°± ìŠ¤íƒ€ì¼: [ì¼ë‹¹ë°±] ì±…ì œëª©: ì‘ê°€ (ë¶€ì œëª©)
    - ë¶€ì œëª©ì€ ìˆìœ¼ë©´ë§Œ ì¶”ê°€í•©ë‹ˆë‹¤.
    
    Args:
        book_title: ì±… ì œëª©
        language: ì–¸ì–´ ('ko' ë˜ëŠ” 'en')
        book_info: ì±… ì •ë³´ ë”•ì…”ë„ˆë¦¬ (ì„ íƒì‚¬í•­, ì¥ë¥´ ê°ì§€ìš©)
        author: ì‘ê°€ëª… (ì„ íƒì‚¬í•­, book_infoì— ì—†ì„ ë•Œ ì‚¬ìš©)
        
    Returns:
        ìƒì„±ëœ ì œëª© (í•´ë‹¹ ì–¸ì–´ë§Œ í¬í•¨)
    """
    import re

    def _truncate_with_ellipsis(text: str, max_len: int = 100) -> str:
        if len(text) <= max_len:
            return text
        if max_len <= 3:
            return text[:max_len]
        return text[: max_len - 3] + "..."

    def _shrink_subtitle_to_fit(prefix_: str, main_: str, author_part_: str, subtitle: Optional[str]) -> str:
        def _compose(sub: Optional[str]) -> str:
            sub_part = f" ({sub})" if sub else ""
            return f"{prefix_} {main_}{author_part_}{sub_part}"

        cand = _compose(subtitle)
        if len(cand) <= 100:
            return cand

        if subtitle and " Â· " in subtitle:
            parts = [p.strip() for p in subtitle.split(" Â· ") if p.strip()]
            for k in range(len(parts) - 1, 0, -1):
                sub2 = " Â· ".join(parts[:k])
                cand2 = _compose(sub2)
                if len(cand2) <= 100:
                    return cand2

        cand3 = _compose(None)
        if len(cand3) <= 100:
            return cand3

        return _truncate_with_ellipsis(cand3, 100)

    def _split_trailing_parenthetical(text: str):
        m = re.search(r"\s*\(([^)]+)\)\s*$", text or "")
        if not m:
            return (text or "").strip(), None
        main = re.sub(r"\s*\([^)]*\)\s*$", "", text or "").strip()
        sub = (m.group(1) or "").strip() or None
        return main, sub

    prefix = "[ì¼ë‹¹ë°±]" if language != "en" else "[1DANG100]"

    # 1) ì…ë ¥ì—ì„œ ë©”ì¸/ë¶€ì œëª© ë¶„ë¦¬
    book_title_main, subtitle_from_input = _split_trailing_parenthetical(book_title)

    # 2) ì‘ê°€ëª… ê²°ì • (book_info ìš°ì„ , ì—†ìœ¼ë©´ args.author)
    resolved_author = None
    if book_info:
        if isinstance(book_info.get("author"), str) and book_info.get("author").strip():
            resolved_author = book_info.get("author").strip()
        elif isinstance(book_info.get("authors"), list) and book_info.get("authors"):
            first = book_info.get("authors")[0]
            if isinstance(first, str) and first.strip():
                resolved_author = first.strip()
    if not resolved_author and author:
        resolved_author = author.strip()

    # 3) ì œëª©/ì‘ê°€ ë²ˆì—­ + SEO ë¶€ì œëª© ìƒì„±
    try:
        from src.utils.title_generator import generate_seo_subtitle
    except Exception:
        generate_seo_subtitle = None

    def _unique_keep_order(items: List[str]) -> List[str]:
        seen = set()
        out: List[str] = []
        for it in items:
            it2 = (it or "").strip()
            if not it2:
                continue
            if it2 in seen:
                continue
            seen.add(it2)
            out.append(it2)
        return out

    if language == "ko":
        if is_english_title(book_title_main):
            ko_title = translate_book_title_to_korean(book_title_main)
            en_title = book_title_main
        else:
            ko_title = book_title_main
            en_title = translate_book_title(book_title_main)
            if not is_english_title(en_title):
                raise ValueError(
                    f"ì±… ì œëª© '{book_title}'ì˜ ì˜ì–´ ë²ˆì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                    f"src/utils/translations.pyì— ë§¤í•‘ì„ ì¶”ê°€í•˜ê±°ë‚˜, ì˜ë¬¸ ì›ì œë¥¼ í•¨ê»˜ ì…ë ¥í•˜ì„¸ìš”."
                )

        ko_author = ""
        if resolved_author:
            if is_english_title(resolved_author):
                ko_author = translate_author_name_to_korean(resolved_author) or resolved_author
            else:
                ko_author = resolved_author

        # ë¶€ì œëª©(ko): ì‹¤ì œ ë¶€ì œ + ì˜ë¬¸ ì›ì œ + SEO í‚¤ì›Œë“œ
        subtitle_ko_parts: List[str] = []
        if subtitle_from_input:
            subtitle_ko_parts.append(subtitle_from_input)
        # í•œêµ­ì–´ ì œëª©ì—ì„œëŠ” "ì›ë˜ ë¶€ì œëª©ì´ ì—†ì„ ë•Œë§Œ" ì˜ë¬¸ ì›ì œë¥¼ í•¨ê»˜ ë„£ì–´ ê²€ìƒ‰ì„ ë•ìŠµë‹ˆë‹¤.
        if not subtitle_from_input:
            if en_title and en_title != ko_title and is_english_title(en_title):
                subtitle_ko_parts.append(en_title)
        if generate_seo_subtitle:
            subtitle_ko_parts.append(
                generate_seo_subtitle(
                    "ko",
                    ko_title,
                    author=ko_author or None,
                    book_info=book_info,
                    content_type="full_episode",
                )
            )
        subtitle_ko_parts = _unique_keep_order(subtitle_ko_parts)
        subtitle_ko = " Â· ".join(subtitle_ko_parts) if subtitle_ko_parts else None

        author_part = f": {ko_author}" if ko_author else ""
        title = _shrink_subtitle_to_fit(prefix, ko_title, author_part, subtitle_ko)
        return title

    # language == "en"
    if not is_english_title(book_title_main):
        en_title = translate_book_title(book_title_main)
        if not is_english_title(en_title):
            raise ValueError(
                f"ì±… ì œëª© '{book_title}'ì˜ ì˜ì–´ ë²ˆì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                f"src/utils/translations.pyì— ë§¤í•‘ì„ ì¶”ê°€í•˜ê±°ë‚˜, ì˜ë¬¸ ì›ì œë¥¼ í•¨ê»˜ ì…ë ¥í•˜ì„¸ìš”."
            )
        ko_title = book_title_main
    else:
        en_title = book_title_main
        ko_title = translate_book_title_to_korean(book_title_main)

    en_author = ""
    if resolved_author:
        if not is_english_title(resolved_author):
            en_author = translate_author_name(resolved_author) or resolved_author
        else:
            en_author = resolved_author
        # ì˜ë¬¸ ì œëª©ì€ "ì™„ì „ ì˜ì–´"ë§Œ í—ˆìš©
        if contains_korean(en_author):
            raise ValueError(
                "ì˜ë¬¸ ì‘ê°€ëª…ì— í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. "
                "ì €ì ì˜ë¬¸ í‘œê¸°ë¥¼ í•¨ê»˜ ì…ë ¥í•˜ê±°ë‚˜, src/utils/translations.pyì— ë§¤í•‘ì„ ì¶”ê°€í•˜ì„¸ìš”."
            )

    # ë¶€ì œëª©(en): ì˜ë¬¸ ë¶€ì œ(ìˆë‹¤ë©´) + SEO í‚¤ì›Œë“œ (ì™„ì „ ì˜ì–´ë§Œ)
    subtitle_en_parts: List[str] = []
    if subtitle_from_input and is_english_title(subtitle_from_input):
        subtitle_en_parts.append(subtitle_from_input)
    if generate_seo_subtitle:
        subtitle_en_parts.append(
            generate_seo_subtitle(
                "en",
                en_title,
                author=en_author or None,
                book_info=book_info,
                content_type="full_episode",
            )
        )
    subtitle_en_parts = _unique_keep_order(subtitle_en_parts)
    subtitle_en = " Â· ".join(subtitle_en_parts) if subtitle_en_parts else None

    author_part = f": {en_author}" if en_author else ""
    title = _shrink_subtitle_to_fit(prefix, en_title, author_part, subtitle_en)
    if contains_korean(title):
        raise ValueError(
            "ì˜ë¬¸ ì œëª©ì— í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. "
            "ì˜ë¬¸ ì›ì œ/ì €ì ì˜ë¬¸ í‘œê¸°ë¥¼ í•¨ê»˜ ì…ë ¥í•˜ê±°ë‚˜, src/utils/translations.pyì— ë²ˆì—­ ë§¤í•‘ì„ ì¶”ê°€í•˜ì„¸ìš”."
        )
    return title


def detect_part_count(book_title: str, language: str = "ko") -> int:
    """
    Part ê°œìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ê°ì§€

    Args:
        book_title: ì±… ì œëª©
        language: ì–¸ì–´ ('ko', 'kr' ë˜ëŠ” 'en')

    Returns:
        Part ê°œìˆ˜
    """
    safe_title = get_standard_safe_title(book_title)
    # ì–¸ì–´ ì •ê·œí™”
    normalized_language = "ko" if language in ["ko", "kr"] else "en"
    lang_suffix = "_kr" if language in ["ko", "kr"] else "_en"
    dir_lang = "kr" if language in ["ko", "kr"] else "en"
    input_dir = Path("assets/notebooklm") / safe_title / dir_lang
    
    part_count = 0
    part_num = 1
    while True:
        video_file = input_dir / f"part{part_num}_video{lang_suffix}.mp4"
        if video_file.exists():
            part_count += 1
            part_num += 1
        else:
            break
    
    return part_count


def get_actual_part_durations(book_title: str, language: str = "ko", infographic_duration: float = 30.0) -> List[float]:
    """
    ì‹¤ì œ Part ë¹„ë””ì˜¤ íŒŒì¼ì˜ ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ì—¬ ê° Partì˜ ì´ ê¸¸ì´ ë°˜í™˜
    (ë¹„ë””ì˜¤ ê¸¸ì´ + ì¸í¬ê·¸ë˜í”½ ê¸¸ì´)

    ìš°ì„ ìˆœìœ„:
    1. ë Œë”ë§ëœ ì˜ìƒì˜ timing.json íŒŒì¼ì—ì„œ ì½ê¸° (ê°€ì¥ ì •í™•)
    2. ì›ë³¸ ë¹„ë””ì˜¤ íŒŒì¼ì˜ duration ì‚¬ìš© (fallback)

    Args:
        book_title: ì±… ì œëª©
        language: ì–¸ì–´ ('ko', 'kr' ë˜ëŠ” 'en')
        infographic_duration: ì¸í¬ê·¸ë˜í”½ í‘œì‹œ ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 30.0)

    Returns:
        ê° Partì˜ ì´ ê¸¸ì´ ë¦¬ìŠ¤íŠ¸ (ì´ˆ ë‹¨ìœ„)
    """
    safe_title = get_standard_safe_title(book_title)

    # ì–¸ì–´ ì •ê·œí™” (íŒŒì¼ëª…ìš©)
    lang_suffix_file = "kr" if language in ["ko", "kr"] else "en"

    # 1. ë¨¼ì € ë Œë”ë§ëœ ì˜ìƒì˜ timing.json íŒŒì¼ í™•ì¸ (ê°€ì¥ ì •í™•)
    video_path = Path(f"output/{safe_title}_full_episode_{lang_suffix_file}.mp4")
    timing_info_path = video_path.with_suffix('.timing.json')
    
    if timing_info_path.exists():
        try:
            with open(timing_info_path, 'r', encoding='utf-8') as f:
                timing_info = json.load(f)
            
            part_clip_info = timing_info.get('part_clip_info', [])
            if part_clip_info:
                # Partë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ê° Partì˜ ì´ ê¸¸ì´ ê³„ì‚°
                part_durations = {}
                for clip_info in part_clip_info:
                    part_num = clip_info['part_num']
                    if part_num not in part_durations:
                        part_durations[part_num] = 0.0
                    part_durations[part_num] += clip_info['duration']
                
                # Part ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                sorted_parts = sorted(part_durations.keys())
                return [part_durations[p] for p in sorted_parts]
        except Exception as e:
            logger.warning(f"âš ï¸ timing.json íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            logger.warning("ì›ë³¸ ë¹„ë””ì˜¤ íŒŒì¼ì˜ durationì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # 2. Fallback: ì›ë³¸ ë¹„ë””ì˜¤ íŒŒì¼ì˜ duration ì‚¬ìš©
    part_durations = []

    # input í´ë”ì—ì„œ ë¨¼ì € í™•ì¸
    input_dir = Path("input")
    lang_suffix = "kr" if language in ["ko", "kr"] else "en"

    # assets/notebooklm í´ë” ê²½ë¡œ ë¯¸ë¦¬ ê³„ì‚°
    lang_suffix_alt = "_kr" if language in ["ko", "kr"] else "_en"
    dir_lang = "kr" if language in ["ko", "kr"] else "en"
    notebooklm_dir = Path("assets/notebooklm") / safe_title / dir_lang
    
    part_num = 1
    while True:
        video_file = input_dir / f"part{part_num}_video_{lang_suffix}.mp4"
        if not video_file.exists():
            # assets/notebooklm í´ë”ì—ì„œ í™•ì¸
            video_file = notebooklm_dir / f"part{part_num}_video{lang_suffix_alt}.mp4"
            
            if not video_file.exists():
                break
        
        try:
            from moviepy.editor import VideoFileClip
            clip = VideoFileClip(str(video_file))
            video_duration = clip.duration
            clip.close()
            
            # ì¸í¬ê·¸ë˜í”½ íŒŒì¼ í™•ì¸
            info_file = input_dir / f"part{part_num}_info_{lang_suffix}.png"
            if not info_file.exists():
                # assets/notebooklm í´ë”ì—ì„œ í™•ì¸
                info_file = notebooklm_dir / f"part{part_num}_info{lang_suffix_alt}.png"
            
            # ì¸í¬ê·¸ë˜í”½ì´ ìˆìœ¼ë©´ ê¸¸ì´ ì¶”ê°€
            total_duration = video_duration
            if info_file.exists():
                total_duration += infographic_duration
            
            part_durations.append(total_duration)
            part_num += 1
            
        except Exception as e:
            logger.warning(f"âš ï¸ Part {part_num} ë¹„ë””ì˜¤ ê¸¸ì´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            break
    
    return part_durations


def generate_episode_description(book_title: str, language: str = "ko", video_duration: Optional[float] = None, book_info: Optional[Dict] = None) -> str:
    """
    ì—í”¼ì†Œë“œ ì˜ìƒ ì„¤ëª… ìƒì„±
    
    Args:
        book_title: ì±… ì œëª©
        language: ì–¸ì–´ ('ko' ë˜ëŠ” 'en')
        video_duration: ì˜ìƒ ê¸¸ì´ (ì´ˆ, ì„ íƒì‚¬í•­)
        book_info: ì±… ì •ë³´ ë”•ì…”ë„ˆë¦¬ (ì„ íƒì‚¬í•­, ì¥ë¥´ ê°ì§€ìš©)
        
    Returns:
        ìƒì„±ëœ ì„¤ëª…
    """
    # ì¥ë¥´ ê°ì§€
    genre_ko, genre_en = detect_book_genre(book_title, book_info)
    
    # Part ê°œìˆ˜ ë™ì  ê°ì§€
    part_count = detect_part_count(book_title, language)
    if part_count == 0:
        part_count = 2  # ê¸°ë³¸ê°’ (í•˜ìœ„ í˜¸í™˜ì„±)
    
    # ì±… ì œëª© ë²ˆì—­
    if language == "ko":
        if is_english_title(book_title):
            ko_title = translate_book_title_to_korean(book_title)
            en_title = book_title
        else:
            ko_title = book_title
            en_title = translate_book_title(book_title)
        
        # ì‘ê°€ ì •ë³´
        author_info = ""
        if book_info and 'author' in book_info:
            author_info = f"ì €ì: {book_info['author']}"
            
        # Part ê°œìˆ˜ì— ë”°ë¼ ì„¤ëª… ë™ì  ìƒì„±
        part_desc_list = []
        if part_count == 1:
            part_desc_list.append("â€¢ Part 1: ì‘ê°€ì™€ ë°°ê²½ - ì‘ê°€ì˜ ìƒì• ì™€ ì‘í’ˆ ë°°ê²½")
        elif part_count == 2:
            part_desc_list.append("â€¢ Part 1: ì‘ê°€ì™€ ë°°ê²½ - ì‘ê°€ì˜ ìƒì• ì™€ ì‘í’ˆ ë°°ê²½")
            part_desc_list.append(f"â€¢ Part 2: {genre_ko} ì¤„ê±°ë¦¬ - ì „ì²´ ìŠ¤í† ë¦¬ì™€ ì£¼ìš” ì¸ë¬¼")
        elif part_count == 3:
            part_desc_list.append("â€¢ Part 1: ì‘ê°€ì™€ ë°°ê²½ - ì‘ê°€ì˜ ìƒì• ì™€ ì‘í’ˆ ë°°ê²½")
            part_desc_list.append(f"â€¢ Part 2: {genre_ko} ì¤„ê±°ë¦¬ (ìƒ) - ìŠ¤í† ë¦¬ ì „ë°˜ë¶€ì™€ ì£¼ìš” ì¸ë¬¼")
            part_desc_list.append(f"â€¢ Part 3: {genre_ko} ì¤„ê±°ë¦¬ (í•˜) - ìŠ¤í† ë¦¬ í›„ë°˜ë¶€ì™€ ê²°ë§")
        else:
            for i in range(1, part_count + 1):
                if i == 1:
                    part_desc_list.append(f"â€¢ Part {i}: ì‘ê°€ì™€ ë°°ê²½ - ì‘ê°€ì˜ ìƒì• ì™€ ì‘í’ˆ ë°°ê²½")
                else:
                    part_desc_list.append(f"â€¢ Part {i}: {genre_ko} ì¤„ê±°ë¦¬ - ìŠ¤í† ë¦¬ {i-1}ë¶€")
        part_description = "\n".join(part_desc_list)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ì‹¤ì œ Part ê¸¸ì´ ì‚¬ìš©)
        timestamps = ""
        if video_duration:
            # ì‹¤ì œ Part ë¹„ë””ì˜¤ íŒŒì¼ì˜ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
            actual_part_durations = get_actual_part_durations(book_title, language, infographic_duration=30.0)
            
            current_time = 0.0
            ts_lines = []
            
            # ì‹¤ì œ Part ê¸¸ì´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¹„ìœ¨ë¡œ ê³„ì‚°
            if actual_part_durations and len(actual_part_durations) == part_count:
                # ì‹¤ì œ Part ê¸¸ì´ ì‚¬ìš©
                for i in range(1, part_count + 1):
                    minutes = int(current_time // 60)
                    seconds = int(current_time % 60)
                    
                    if i == 1:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: ì‘ê°€ì™€ ë°°ê²½")
                    elif part_count == 2 and i == 2:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_ko} ì¤„ê±°ë¦¬")
                    elif part_count == 3:
                        if i == 2:
                            ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_ko} ì¤„ê±°ë¦¬ (ìƒ)")
                        elif i == 3:
                            ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_ko} ì¤„ê±°ë¦¬ (í•˜)")
                    else:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_ko} ì¤„ê±°ë¦¬ {i-1}ë¶€")
                    
                    # ë‹¤ìŒ Part ì‹œì‘ ì‹œê°„ ê³„ì‚°
                    if i < len(actual_part_durations):
                        current_time += actual_part_durations[i - 1]
            else:
                # ì‹¤ì œ Part ê¸¸ì´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë©´ ë¹„ìœ¨ë¡œ ê³„ì‚° (í•˜ìœ„ í˜¸í™˜ì„±)
                logger.warning("âš ï¸ ì‹¤ì œ Part ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¹„ìœ¨ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.")
                for i in range(1, part_count + 1):
                    if i == 1:
                        part_duration = video_duration * (0.35 if part_count >= 2 else 1.0)
                    elif i == part_count:
                        part_duration = video_duration - current_time
                    else:
                        remaining_time = video_duration - current_time
                        part_duration = remaining_time / (part_count - i + 1)
                    
                    minutes = int(current_time // 60)
                    seconds = int(current_time % 60)
                    
                    if i == 1:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: ì‘ê°€ì™€ ë°°ê²½")
                    elif part_count == 2 and i == 2:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_ko} ì¤„ê±°ë¦¬")
                    elif part_count == 3:
                        if i == 2:
                            ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_ko} ì¤„ê±°ë¦¬ (ìƒ)")
                        elif i == 3:
                            ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_ko} ì¤„ê±°ë¦¬ (í•˜)")
                    else:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_ko} ì¤„ê±°ë¦¬ {i-1}ë¶€")
                    
                    current_time += part_duration
            
            timestamps = "\n".join(ts_lines)

        description = f"""{timestamps}

ğŸ“š ì±… ë¦¬ë·° ì˜ìƒ | ë…ì„œ | ë¶íŠœë²„ | ì±…ì¶”ì²œ

ğŸ“– ì˜ìƒ êµ¬ì„±:
ì´ ì˜ìƒì€ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤:
{part_description}

ğŸ“˜ ì±… ì†Œê°œ:
{ko_title}
{author_info}

ì´ ì˜ìƒì€ ì¼ë‹¹ë°± ì±„ë„ì˜ {part_count}í¸ì˜ ì˜ìƒì„ í•˜ë‚˜ë¡œ í•©ì¹œ ì™„ì „íŒì…ë‹ˆë‹¤.
NotebookLMì˜ ì‹¬ì¸µ ë¶„ì„ê³¼ í•¨ê»˜ ì‘í’ˆì„ ê¹Šì´ ìˆê²Œ ì´í•´í•´ë³´ì„¸ìš”.

ğŸ¯ ì£¼ìš” ë‚´ìš©:
âœ“ ì‘ê°€ì˜ ìƒì• ì™€ ì‘í’ˆ ì„¸ê³„ ìƒì„¸ ë¶„ì„
âœ“ ì‘í’ˆì˜ ì‹œëŒ€ì  ë°°ê²½ê³¼ ìˆ¨ê²¨ì§„ ì˜ë¯¸
âœ“ {genre_ko}ì˜ ì „ì²´ ì¤„ê±°ë¦¬ì™€ í•µì‹¬ ë©”ì‹œì§€

ğŸ’¡ ì¼ë‹¹ë°± ì±„ë„ì—ì„œ ë” ë§ì€ ì‘í’ˆì„ ë§Œë‚˜ë³´ì„¸ìš”!
ğŸ”” êµ¬ë…ê³¼ ì¢‹ì•„ìš”ëŠ” ë‹¤ìŒ ì˜ìƒ ì œì‘ì— í° í˜ì´ ë©ë‹ˆë‹¤!
ğŸ’¬ ëŒ“ê¸€ë¡œ ì—¬ëŸ¬ë¶„ì˜ ìƒê°ì„ ê³µìœ í•´ì£¼ì„¸ìš”!
"""

        # ì œíœ´ ë§í¬ ì‚½ì… (í•œê¸€)
        # ì˜ë¬¸ ì±… ì œëª© ì¤€ë¹„
        if is_english_title(book_title):
            en_title_for_link = book_title
            ko_title_for_link = translate_book_title_to_korean(book_title)
        else:
            en_title_for_link = translate_book_title(book_title)
            ko_title_for_link = book_title

        # author ì •ë³´ ì¤€ë¹„
        ko_author = ""
        en_author = ""
        if book_info and 'author' in book_info:
            author_val = book_info['author']
            if is_english_title(author_val):
                en_author = author_val
                # ì˜ë¬¸ ì‘ê°€ëª…ì€ í•œê¸€ë¡œ ë²ˆì—­ (translate_author_nameì€ í•œê¸€â†’ì˜ë¬¸ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš© ë¶ˆê°€)
                ko_author = ""  # í˜„ì¬ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
            else:
                ko_author = author_val
                en_author = translate_author_name(author_val)

        affiliate_section = generate_affiliate_section(
            book_title_ko=ko_title_for_link,
            book_title_en=en_title_for_link,
            author_ko=ko_author,
            author_en=en_author,
            language='ko'
        )
        description += affiliate_section

        description += f"""
#ì¼ë‹¹ë°± #{ko_title.replace(' ', '')} #ì±…ë¦¬ë·° #ë¬¸í•™ #{genre_ko} #ì‘ê°€ #{author_info.replace('ì €ì: ', '').replace(' ', '') if author_info else 'ë¬¸í•™ì‘í’ˆ'}"""

    else:  # en
        if not is_english_title(book_title):
            en_title = translate_book_title(book_title)
            if not en_title or not is_english_title(en_title):
                en_title = "This Book" if not en_title else en_title
        else:
            en_title = book_title
        
        if not is_english_title(en_title):
            en_title = "This Book"

        # Author info
        author_info = ""
        if book_info and 'author' in book_info:
             author_val = book_info['author']
             if not is_english_title(author_val):
                 author_val = translate_author_name(author_val)
             if author_val and is_english_title(author_val):
                 author_info = f"Author: {author_val}"
        
        # Part description
        part_desc_list = []
        if part_count == 1:
            part_desc_list.append("â€¢ Part 1: Author & Background - Author's life and work context")
        elif part_count == 2:
            part_desc_list.append("â€¢ Part 1: Author & Background - Author's life and work context")
            part_desc_list.append(f"â€¢ Part 2: {genre_en} Summary - Full story and main characters")
        elif part_count == 3:
            part_desc_list.append("â€¢ Part 1: Author & Background - Author's life and work context")
            part_desc_list.append(f"â€¢ Part 2: {genre_en} Summary (Part 1) - First half of the story and main characters")
            part_desc_list.append(f"â€¢ Part 3: {genre_en} Summary (Part 2) - Second half of the story and conclusion")
        else:
            for i in range(1, part_count + 1):
                if i == 1:
                    part_desc_list.append(f"â€¢ Part {i}: Author & Background - Author's life and work context")
                else:
                    part_desc_list.append(f"â€¢ Part {i}: {genre_en} Summary - Story Part {i-1}")
        part_description = "\n".join(part_desc_list)

        # Timestamps (ì‹¤ì œ Part ê¸¸ì´ ì‚¬ìš©)
        timestamps = ""
        if video_duration:
            # ì‹¤ì œ Part ë¹„ë””ì˜¤ íŒŒì¼ì˜ ê¸¸ì´ ê°€ì ¸ì˜¤ê¸°
            actual_part_durations = get_actual_part_durations(book_title, language, infographic_duration=30.0)
            
            current_time = 0.0
            ts_lines = []
            
            # ì‹¤ì œ Part ê¸¸ì´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¹„ìœ¨ë¡œ ê³„ì‚°
            if actual_part_durations and len(actual_part_durations) == part_count:
                # ì‹¤ì œ Part ê¸¸ì´ ì‚¬ìš©
                for i in range(1, part_count + 1):
                    minutes = int(current_time // 60)
                    seconds = int(current_time % 60)
                    
                    if i == 1:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: Author & Background")
                    elif part_count == 2 and i == 2:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_en} Summary")
                    elif part_count == 3:
                        if i == 2:
                            ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_en} Summary (Part 1)")
                        elif i == 3:
                            ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_en} Summary (Part 2)")
                    else:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_en} Summary Part {i-1}")
                    
                    # ë‹¤ìŒ Part ì‹œì‘ ì‹œê°„ ê³„ì‚°
                    if i < len(actual_part_durations):
                        current_time += actual_part_durations[i - 1]
            else:
                # ì‹¤ì œ Part ê¸¸ì´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë©´ ë¹„ìœ¨ë¡œ ê³„ì‚° (í•˜ìœ„ í˜¸í™˜ì„±)
                logger.warning("âš ï¸ ì‹¤ì œ Part ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¹„ìœ¨ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.")
                for i in range(1, part_count + 1):
                    if i == 1:
                        part_duration = video_duration * (0.35 if part_count >= 2 else 1.0)
                    elif i == part_count:
                        part_duration = video_duration - current_time
                    else:
                        remaining_time = video_duration - current_time
                        part_duration = remaining_time / (part_count - i + 1)
                    
                    minutes = int(current_time // 60)
                    seconds = int(current_time % 60)
                    
                    if i == 1:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: Author & Background")
                    elif part_count == 2 and i == 2:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_en} Summary")
                    elif part_count == 3:
                        if i == 2:
                            ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_en} Summary (Part 1)")
                        elif i == 3:
                            ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_en} Summary (Part 2)")
                    else:
                        ts_lines.append(f"{minutes}:{seconds:02d} - Part {i}: {genre_en} Summary Part {i-1}")
                    
                    current_time += part_duration
            
            timestamps = "\n".join(ts_lines)
        
        safe_en_title = ensure_english_only(en_title.replace(' ', '').replace(':', '').replace('-', ''), "Book")
        safe_genre_en = ensure_english_only(genre_en.replace(' ', ''), "Work")
        
        description = f"""{timestamps}

ğŸ“š Book Review | Reading | BookTube | Recommendation

ğŸ“– Video Structure:
{part_description}

ğŸ“˜ Book Info:
{en_title}
{author_info}

This video combines {part_count} episodes from 1DANG100 channel into one complete guide.
Deep dive into the masterpiece with NotebookLM analysis.

ğŸ¯ What You'll Learn:
âœ“ Author's life and literary world
âœ“ Historical background and significance
âœ“ Complete story structure and plot

ğŸ’¡ Check out 1DANG100 channel for more literary works!
ğŸ”” Subscribe and like to support future videos!
ğŸ’¬ Share your thoughts in the comments!
"""

        # ì œíœ´ ë§í¬ ì‚½ì… (ì˜ë¬¸)
        # ì˜ë¬¸ ì±… ì œëª© ì¤€ë¹„
        if is_english_title(book_title):
            en_title_for_link = book_title
            ko_title_for_link = translate_book_title_to_korean(book_title)
        else:
            en_title_for_link = translate_book_title(book_title)
            ko_title_for_link = book_title

        # author ì •ë³´ ì¤€ë¹„
        ko_author = ""
        en_author = ""
        if book_info and 'author' in book_info:
            author_val = book_info['author']
            if is_english_title(author_val):
                en_author = author_val
                ko_author = ""  # ì˜ë¬¸â†’í•œê¸€ ë²ˆì—­ í•¨ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´
            else:
                ko_author = author_val
                en_author = translate_author_name(author_val)

        affiliate_section = generate_affiliate_section(
            book_title_ko=ko_title_for_link,
            book_title_en=en_title_for_link,
            author_ko=ko_author,
            author_en=en_author,
            language='en'
        )
        description += affiliate_section

        description += f"""
#{safe_en_title} #BookReview #Literature #{safe_genre_en} #Author #LiteraryWork"""

        # ìµœì¢… ê²€ì¦: descriptionì—ì„œ í•œêµ­ì–´ ì œê±°
        if language == "en":
            # description ì „ì²´ì—ì„œ í•œêµ­ì–´ê°€ í¬í•¨ëœ ë¶€ë¶„ ì œê±°
            lines = description.split('\n')
            cleaned_lines = []
            for line in lines:
                if contains_korean(line):
                    # í•œêµ­ì–´ê°€ í¬í•¨ëœ ë¼ì¸ì€ ì œê±°í•˜ê±°ë‚˜ í•œêµ­ì–´ë§Œ ì œê±°
                    cleaned_line = remove_korean_from_text(line)
                    if cleaned_line.strip():
                        cleaned_lines.append(cleaned_line)
                else:
                    cleaned_lines.append(line)
            description = '\n'.join(cleaned_lines)
    
    return description


def generate_episode_tags(book_title: str, language: str = "ko", book_info: Optional[Dict] = None) -> list:
    """
    ì—í”¼ì†Œë“œ ì˜ìƒ íƒœê·¸ ìƒì„± (YouTube ìµœëŒ€ì¹˜: 500ì, íƒœê·¸ë‹¹ 30ì)
    
    Args:
        book_title: ì±… ì œëª©
        language: ì–¸ì–´ ('ko' ë˜ëŠ” 'en')
        book_info: ì±… ì •ë³´ (Optional)
        
    Returns:
        ìƒì„±ëœ íƒœê·¸ ë¦¬ìŠ¤íŠ¸
    """
    from src.utils.file_utils import load_book_info
    
    # ì±… ì •ë³´ ë¡œë“œ ì‹œë„ (ì¸ìë¡œ ì „ë‹¬ë˜ì§€ ì•Šì€ ê²½ìš°)
    if not book_info:
        try:
            safe_title = get_standard_safe_title(book_title)
            book_info = load_book_info(safe_title)
        except:
            pass
            
    # ì¥ë¥´ ê°ì§€
    genre_ko, genre_en = detect_book_genre(book_title, book_info)
    
    # ì±… ì œëª© ë²ˆì—­
    if language == "ko":
        if is_english_title(book_title):
            ko_title = translate_book_title_to_korean(book_title)
            en_title = book_title
        else:
            ko_title = book_title
            en_title = translate_book_title(book_title)
        
        # ì‘ê°€ ì´ë¦„ ì¶”ì¶œ
        author_name = None
        if book_info and 'author' in book_info:
            author_name = book_info['author']
            # í•œê¸€ ì‘ê°€ ì´ë¦„ë„ ë²ˆì—­
            if author_name:
                author_ko = translate_author_name(author_name) if is_english_title(author_name) else author_name
        
        # ì±… ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (íƒœê·¸ìš©)
        # ì œëª©ì„ ê³µë°±/êµ¬ë‘ì ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ í•µì‹¬ ë‹¨ì–´ë§Œ ì¶”ì¶œ
        import re
        # í•œê¸€ ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (êµ¬ë‘ì , íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        ko_title_clean = re.sub(r'[:\-\(\)\[\]ã€Œã€]', ' ', ko_title)
        ko_keywords = [word.strip() for word in ko_title_clean.split() if len(word.strip()) > 1]
        # ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ ì„ íƒ (ì²˜ìŒ 2-3ê°œ ë‹¨ì–´)
        ko_main_keyword = ''.join(ko_keywords[:2]) if len(ko_keywords) >= 2 else ''.join(ko_keywords)
        ko_main_keyword = ko_main_keyword[:15]  # ìµœëŒ€ 15ìë¡œ ì œí•œ
        
        # ì˜ì–´ ì œëª©ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        en_title_clean = re.sub(r'[:\-\(\)\[\]ã€Œã€]', ' ', en_title)
        en_keywords = [word.strip() for word in en_title_clean.split() if len(word.strip()) > 1]
        en_main_keyword = ' '.join(en_keywords[:2]) if len(en_keywords) >= 2 else ' '.join(en_keywords)
        en_main_keyword = en_main_keyword[:20]  # ìµœëŒ€ 20ìë¡œ ì œí•œ
        
        tags = [
            # ì±„ë„ ë° ì‹œë¦¬ì¦ˆ
            "ì¼ë‹¹ë°±",
            "ì¼ë‹¹ë°±ì±…ë¦¬ë·°",
            "ì¼ë‹¹ë°±ë¬¸í•™",
            
            # ì±… ì œëª© í•µì‹¬ í‚¤ì›Œë“œ (ìì—°ìŠ¤ëŸ¬ìš´ íƒœê·¸)
            ko_main_keyword if ko_main_keyword else ko_title[:15],
            f"{ko_main_keyword}ë¦¬ë·°" if ko_main_keyword and len(ko_main_keyword) + 2 <= 30 else "ì±…ë¦¬ë·°",
            f"{ko_main_keyword}ë¶„ì„" if ko_main_keyword and len(ko_main_keyword) + 2 <= 30 else "ì±…ë¶„ì„",
            
            # ì‘ê°€ ê´€ë ¨
            "ì‘ê°€",
            "ì‘ê°€ë¶„ì„",
            "ì‘ê°€ì´ì•¼ê¸°",
            "ì‘ê°€ìƒì• ",
            "ë¬¸í•™ì‘ê°€",
        ]
        
        # ì‘ê°€ ì´ë¦„ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if author_name:
            if not is_english_title(author_name):
                author_ko = author_name
                author_en = translate_author_name(author_name)
            else:
                author_en = author_name
                author_ko = translate_author_name(author_name)
            
            tags.extend([
                f"{author_ko}",
                f"{author_ko}ì‘í’ˆ",
                f"{author_ko}{genre_ko}",
                f"{author_en}",
                f"{author_en}Book",
            ])
            
        # 1. ê¸°ë³¸ íƒœê·¸ (ê²€ìƒ‰ëŸ‰ ë†’ì€ ìˆœ)
        basic_tags = [
            "ì±…ë¦¬ë·°", "ë…ì„œ", "ë¶íŠœë²„", "ì±…ì¶”ì²œ", "ë…ì„œë²•", "ì±…ì½ê¸°", "ì„œí‰", "ë…í›„ê°",
            "BookReview", "Reading", "BookTube", "BookRecommendation"
        ]
        
        # 2. ì¥ë¥´ íƒœê·¸
        genre_tags = [
            f"{genre_ko}", f"{genre_ko}ì¶”ì²œ", f"{genre_ko}ë² ìŠ¤íŠ¸ì…€ëŸ¬",
            "ë² ìŠ¤íŠ¸ì…€ëŸ¬", "ìŠ¤í…Œë””ì…€ëŸ¬", "ì¶”ì²œë„ì„œ", "ê¶Œì¥ë„ì„œ",
            "ì¸ë¬¸í•™", "êµì–‘", "ì§€ì‹", "ê³µë¶€",
        ]
        if genre_ko == "ì†Œì„¤":
            genre_tags.extend(["ë¬¸í•™", "ì†Œì„¤ë¦¬ë·°", "ë¬¸í•™ì‘í’ˆ", "ê³ ì „ë¬¸í•™", "ì„¸ê³„ë¬¸í•™"])
        elif genre_ko == "ì² í•™":
            genre_tags.extend(["ì² í•™ì±…", "ì² í•™ì…ë¬¸", "ì¸ë¬¸í•™ê°•ì˜"])
        elif genre_ko == "ê²½ì œê²½ì˜":
            genre_tags.extend(["ê²½ì œê³µë¶€", "ì£¼ì‹", "íˆ¬ì", "ì„±ê³µ", "ë¶€ì"])
        elif genre_ko == "ìê¸°ê³„ë°œ":
            genre_tags.extend(["ë™ê¸°ë¶€ì—¬", "ì„±ì¥", "ì„±ê³µí•™", "ë§ˆì¸ë“œì…‹"])
            
        # í˜„ì¬ ì—°ë„ ê°€ì ¸ì˜¤ê¸°
        current_year = datetime.now().year
            
        # 3. ê¸°ê´€/íŠ¸ë Œë“œ íƒœê·¸
        trend_tags = [
            f"ì±…ì¶”ì²œ{current_year}", "ë…ì„œì±Œë¦°ì§€", "ë…ì„œëª¨ì„", "ì¼ë‹¹ë°±", "ì¼ë‹¹ë°±ì±…ë¦¬ë·°"
        ]
        
        # íƒœê·¸ í•©ì¹˜ê¸° (ìš°ì„ ìˆœìœ„ëŒ€ë¡œ)
        # ì´ë¯¸ ì¶”ê°€ëœ tags(ì œëª©/ì‘ê°€) + ê¸°ë³¸ + ì¥ë¥´ + íŠ¸ë Œë“œ
        tags = tags + basic_tags + genre_tags + trend_tags
        
    else:  # en
        if not is_english_title(book_title):
            en_title = translate_book_title(book_title)
            # ë²ˆì—­ì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ í•œêµ­ì–´ê°€ ê·¸ëŒ€ë¡œ ë‚¨ì•„ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if not en_title or not is_english_title(en_title):
                en_title = "Book"  # ê¸°ë³¸ê°’
        else:
            en_title = book_title
        
        # í•œêµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ì§€ ìµœì¢… í™•ì¸
        if not is_english_title(en_title):
            en_title = "Book"
        
        # ì‘ê°€ ì´ë¦„ ì¶”ì¶œ
        author_name = None
        if book_info and 'author' in book_info:
            author_name = book_info['author']
            # ì‘ê°€ ì´ë¦„ë„ ì˜ì–´ë¡œ ë³€í™˜
            if author_name and not is_english_title(author_name):
                author_name = translate_author_name(author_name)
                # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì„¤ì • (í•œêµ­ì–´ ì‘ê°€ ì´ë¦„ ì œê±°)
                if not author_name or not is_english_title(author_name):
                    author_name = None
        
        # ì±… ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (íƒœê·¸ìš©)
        import re
        # ì˜ì–´ ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œêµ­ì–´ ì œì™¸)
        en_title_clean = re.sub(r'[:\-\(\)\[\]ã€Œã€]', ' ', en_title)
        en_keywords = [word.strip() for word in en_title_clean.split() if len(word.strip()) > 1 and is_english_title(word.strip())]
        en_main_keyword = ' '.join(en_keywords[:2]) if len(en_keywords) >= 2 else ' '.join(en_keywords)
        en_main_keyword = en_main_keyword[:20]  # ìµœëŒ€ 20ìë¡œ ì œí•œ
        
        # en_main_keywordê°€ í•œêµ­ì–´ë¥¼ í¬í•¨í•˜ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        if not en_main_keyword or not is_english_title(en_main_keyword):
            en_main_keyword = None
        
        # en_titleë„ í•œêµ­ì–´ê°€ í¬í•¨ë˜ì§€ ì•Šë„ë¡ í™•ì¸
        safe_en_title = en_title[:20] if is_english_title(en_title) else "Book"
        
        tags = [
            # Channel & Series
            "1DANG100",
            "1DANG100BookReview",
            "1DANG100Literature",
            
            # Book Title (í•µì‹¬ í‚¤ì›Œë“œë§Œ, ì˜ì–´ë§Œ)
            en_main_keyword if en_main_keyword else safe_en_title,
            f"{en_main_keyword}Review" if en_main_keyword and len(en_main_keyword) + 6 <= 30 else "BookReview",
            f"{en_main_keyword}Analysis" if en_main_keyword and len(en_main_keyword) + 8 <= 30 else "BookAnalysis",
            f"{en_main_keyword}Summary" if en_main_keyword and len(en_main_keyword) + 7 <= 30 else "BookSummary",
            f"{en_main_keyword}Guide" if en_main_keyword and len(en_main_keyword) + 5 <= 30 else "BookGuide",
            
            # Author related
            "Author",
            "AuthorAnalysis",
            "AuthorBiography",
            "LiteraryAuthor",
        ]
        
        # Add author name if available (ì˜ì–´ë§Œ)
        if author_name and is_english_title(author_name):
            tags.extend([
                f"{author_name}",
                f"{author_name}Book",
                f"{author_name}{genre_en}",
            ])
        
        # 1. Basic Tags (High Volume)
        basic_tags = [
            "BookReview", "Reading", "BookTube", "BookRecommendation", "Books",
            "BookSummary", "Audiobook", "Literature", "Bestseller"
        ]
        
        # 2. Genre Tags
        genre_tags = [
            f"{genre_en}", f"{genre_en}Review", f"{genre_en}Recommendation",
            "ClassicLiterature", "ModernLiterature", "MustRead",
            "SelfImprovement" if genre_en == "SelfHelp" else "Education"
        ]
        if genre_en == "Novel":
            genre_tags.extend(["Fiction", "Story", "NovelReview", "LiteraryFiction"])
        elif genre_en == "Philosophy":
            genre_tags.extend(["PhilosophyBook", "Philosophical", "Wisdom"])
        elif genre_en == "Business":
            genre_tags.extend(["BusinessBook", "Finance", "Success", "Investment"])
        
        # í˜„ì¬ ì—°ë„ ê°€ì ¸ì˜¤ê¸°
        current_year = datetime.now().year
        
        # 3. Trending/Institution Tags
        trend_tags = [
            "BookTok", "BookLover", f"ReadingChallenge{current_year}", "1DANG100"
        ]
        
        tags = tags + basic_tags + genre_tags + trend_tags
    
    # íƒœê·¸ ì •ë¦¬: 30ì ì œí•œ, ì¤‘ë³µ ì œê±°, í•œêµ­ì–´ ì œê±° (ì˜ë¬¸ì¼ ê²½ìš°)
    cleaned_tags = []
    seen = set()
    for tag in tags:
        # ì˜ë¬¸ ë©”íƒ€ë°ì´í„°ì¸ ê²½ìš° í•œêµ­ì–´ ì œê±°
        if language == "en":
            if contains_korean(tag):
                # í•œêµ­ì–´ê°€ í¬í•¨ëœ íƒœê·¸ëŠ” ì œê±°
                continue
            # í•œêµ­ì–´ê°€ ì—†ëŠ” ê²½ìš°ì—ë„ í•œ ë²ˆ ë” í™•ì¸
            tag = ensure_english_only(tag, "")
            if not tag:
                continue
        
        # 30ìë¡œ ìë¥´ê¸°
        tag_cleaned = tag[:30] if len(tag) > 30 else tag
        # ì¤‘ë³µ ì œê±°
        tag_lower = tag_cleaned.lower()
        if tag_lower not in seen and tag_cleaned.strip():
            seen.add(tag_lower)
            cleaned_tags.append(tag_cleaned)
    
    # YouTube íƒœê·¸ ì´ ê¸¸ì´ ì œí•œ: 500ì (ì‰¼í‘œ í¬í•¨)
    # ê° íƒœê·¸ + ì‰¼í‘œ = íƒœê·¸ê¸¸ì´ + 1
    # ìµœëŒ€ 500ìê¹Œì§€ ê°€ëŠ¥
    final_tags = []
    total_length = 0
    
    for tag in cleaned_tags:
        # íƒœê·¸ + ì‰¼í‘œ ê¸¸ì´
        tag_length = len(tag) + 1  # +1 for comma
        if total_length + tag_length <= 500:
            final_tags.append(tag)
            total_length += tag_length
        else:
            break
    
    return final_tags


def create_episode_metadata(
    book_title: str,
    language: str = "ko",
    video_path: Optional[str] = None,
    thumbnail_path: Optional[str] = None,
    video_duration: Optional[float] = None,
    author: Optional[str] = None
) -> Dict:
    """
    ì—í”¼ì†Œë“œ ì˜ìƒ ë©”íƒ€ë°ì´í„° ìƒì„±

    Args:
        book_title: ì±… ì œëª©
        language: ì–¸ì–´ ('ko', 'kr' ë˜ëŠ” 'en')
        video_path: ì˜ìƒ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        thumbnail_path: ì¸ë„¤ì¼ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        video_duration: ì˜ìƒ ê¸¸ì´ (ì´ˆ, ì„ íƒì‚¬í•­)

    Returns:
        ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    safe_title = get_standard_safe_title(book_title)

    # ì–¸ì–´ ì •ê·œí™” (íŒŒì¼ëª…ìš©)
    normalized_language = "ko" if language in ["ko", "kr"] else "en"
    lang_suffix_file = "kr" if language in ["ko", "kr"] else "en"

    # ì˜ìƒ ê²½ë¡œê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì°¾ê¸°
    if not video_path:
        video_path = f"output/{safe_title}_full_episode_{lang_suffix_file}.mp4"
    
    video_path_obj = Path(video_path)
    
    # ì˜ìƒì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not video_path_obj.exists():
        logger.warning(f"âš ï¸ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        logger.warning("ë©”íƒ€ë°ì´í„°ëŠ” ìƒì„±ë˜ì§€ë§Œ ì˜ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì¸ë„¤ì¼ ê²½ë¡œê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ì°¾ê¸°
    if not thumbnail_path:
        thumbnail_path = f"output/{safe_title}_thumbnail_{lang_suffix_file}.jpg"
    
    thumbnail_path_obj = Path(thumbnail_path)
    
    # ì¸ë„¤ì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not thumbnail_path_obj.exists():
        logger.warning(f"âš ï¸ ì¸ë„¤ì¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {thumbnail_path}")
        thumbnail_path = None
    
    # ì˜ìƒ ê¸¸ì´ í™•ì¸ (video_durationì´ ì—†ìœ¼ë©´)
    if video_duration is None and video_path_obj.exists():
        try:
            from moviepy.editor import VideoFileClip
            clip = VideoFileClip(str(video_path_obj))
            video_duration = clip.duration
            clip.close()
        except Exception as e:
            logger.warning(f"âš ï¸ ì˜ìƒ ê¸¸ì´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            video_duration = None
    
    # ì±… ì •ë³´ ë¡œë“œ (ì¥ë¥´ ê°ì§€ìš©)
    book_info = None
    try:
        from src.utils.file_utils import load_book_info
        safe_title = get_standard_safe_title(book_title)
        book_info = load_book_info(safe_title)
    except:
        pass
    
    # ë©”íƒ€ë°ì´í„° ìƒì„± (í˜„ì¬ ì–¸ì–´) - normalized_language ì‚¬ìš©
    title = generate_episode_title(book_title, normalized_language, book_info, author=author)
    description = generate_episode_description(book_title, normalized_language, video_duration, book_info)
    tags = generate_episode_tags(book_title, normalized_language, book_info)

    # ì˜ë¬¸ ë©”íƒ€ë°ì´í„°ì¸ ê²½ìš° ìµœì¢… ê²€ì¦: descriptionê³¼ tagsì—ì„œ í•œêµ­ì–´ ì œê±°
    if normalized_language == "en":
        # descriptionì—ì„œ í•œêµ­ì–´ ì œê±°
        if contains_korean(description):
            logger.warning("âš ï¸ Descriptionì— í•œêµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì œê±°í•©ë‹ˆë‹¤.")
            lines = description.split('\n')
            cleaned_lines = []
            for line in lines:
                if contains_korean(line):
                    cleaned_line = remove_korean_from_text(line)
                    if cleaned_line.strip():
                        cleaned_lines.append(cleaned_line)
                else:
                    cleaned_lines.append(line)
            description = '\n'.join(cleaned_lines)
        
        # tagsì—ì„œ í•œêµ­ì–´ ì œê±°
        english_only_tags = []
        for tag in tags:
            if contains_korean(tag):
                logger.warning(f"âš ï¸ Tag '{tag}'ì— í•œêµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì œê±°í•©ë‹ˆë‹¤.")
                continue
            english_only_tags.append(tag)
        tags = english_only_tags
    
    # ì–‘ìª½ ì–¸ì–´ì˜ ì œëª©ê³¼ ì„¤ëª… ìƒì„± (ë‹¤êµ­ì–´ ë©”íƒ€ë°ì´í„°ìš©)
    other_language = "en" if normalized_language == "ko" else "ko"
    title_other = generate_episode_title(book_title, other_language, book_info, author=author)
    description_other = generate_episode_description(book_title, other_language, video_duration, book_info)
    
    # ì˜ë¬¸ ì„¤ëª…ì—ì„œ í•œêµ­ì–´ ì œê±° (ë‹¤êµ­ì–´ ë©”íƒ€ë°ì´í„°ìš©)
    if other_language == "en":
        if contains_korean(description_other):
            lines = description_other.split('\n')
            cleaned_lines = []
            for line in lines:
                if contains_korean(line):
                    cleaned_line = remove_korean_from_text(line)
                    if cleaned_line.strip():
                        cleaned_lines.append(cleaned_line)
                else:
                    cleaned_lines.append(line)
            description_other = '\n'.join(cleaned_lines)
    
    metadata = {
        'video_path': str(video_path_obj),
        'title': title,
        'description': description,
        'tags': tags,
        'language': normalized_language,
        'book_title': book_title,
        'video_duration': video_duration,
        # ë‹¤êµ­ì–´ ë©”íƒ€ë°ì´í„° (YouTube localizationsìš©)
        'localizations': {
            normalized_language: {
                'title': title,
                'description': description
            },
            other_language: {
                'title': title_other,
                'description': description_other
            }
        }
    }
    
    if thumbnail_path:
        metadata['thumbnail_path'] = str(thumbnail_path_obj)
    
    # Part 1 videoì™€ infographicì˜ ì¢…ë£Œ ì‹œê°„ ì¶”ê°€ (timing.jsonì—ì„œ ì½ê¸°)
    timing_info_path = video_path_obj.with_suffix('.timing.json')
    if timing_info_path.exists():
        try:
            with open(timing_info_path, 'r', encoding='utf-8') as f:
                timing_info = json.load(f)
            
            if timing_info.get('part1_video_end_time') is not None:
                metadata['part1_video_end_time'] = timing_info['part1_video_end_time']
            if timing_info.get('part1_info_end_time') is not None:
                metadata['part1_info_end_time'] = timing_info['part1_info_end_time']
            
            logger.info(f"ğŸ“Š Part 1 ì‹œê°„ ì •ë³´ ì¶”ê°€:")
            if metadata.get('part1_video_end_time'):
                minutes = int(metadata['part1_video_end_time'] // 60)
                seconds = int(metadata['part1_video_end_time'] % 60)
                logger.info(f"   Part 1 Video ì¢…ë£Œ: {minutes}:{seconds:02d} ({metadata['part1_video_end_time']:.2f}ì´ˆ)")
            if metadata.get('part1_info_end_time'):
                minutes = int(metadata['part1_info_end_time'] // 60)
                seconds = int(metadata['part1_info_end_time'] % 60)
                logger.info(f"   Part 1 Infographic ì¢…ë£Œ: {minutes}:{seconds:02d} ({metadata['part1_info_end_time']:.2f}ì´ˆ)")
        except Exception as e:
            logger.warning(f"âš ï¸ timing.json íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
    
    return metadata


def save_metadata(metadata: Dict, output_path: Optional[str] = None) -> Path:
    """
    ë©”íƒ€ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        metadata: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
        
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    if output_path is None:
        video_path = Path(metadata['video_path'])
        output_path = video_path.with_suffix('.metadata.json')
    else:
        output_path = Path(output_path)
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥: {output_path}")
    if metadata.get('thumbnail_path'):
        logger.info(f"   ğŸ“¸ ì¸ë„¤ì¼: {Path(metadata['thumbnail_path']).name}")
    
    return output_path


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ì¼ë‹¹ë°± ì—í”¼ì†Œë“œ ì˜ìƒ ë©”íƒ€ë°ì´í„° ìƒì„±',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python src/20_create_episode_metadata.py --title "ë§ˆí‚¤ì•„ë²¨ë¦¬ êµ°ì£¼ë¡ " --language ko
  python src/20_create_episode_metadata.py --title "The Prince" --language en
        """
    )
    
    parser.add_argument(
        '--title',
        type=str,
        required=True,
        help='ì±… ì œëª©'
    )

    parser.add_argument(
        '--author',
        type=str,
        default=None,
        help='ì‘ê°€ëª… (ì„ íƒ, book_info.jsonì— ì—†ì„ ë•Œ ì œëª©ì— ì‚¬ìš©)'
    )
    
    parser.add_argument(
        '--language',
        type=str,
        default='ko',
        choices=['ko', 'kr', 'en'],
        help='ì–¸ì–´ (ê¸°ë³¸ê°’: ko, krë„ koë¡œ ì²˜ë¦¬ë¨)'
    )
    
    parser.add_argument(
        '--video-path',
        type=str,
        default=None,
        help='ì˜ìƒ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: output/{ì±…ì œëª©}_full_episode_{ì–¸ì–´}.mp4)'
    )
    
    parser.add_argument(
        '--thumbnail-path',
        type=str,
        default=None,
        help='ì¸ë„¤ì¼ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: output/{ì±…ì œëª©}_thumbnail_{ì–¸ì–´}.jpg)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='ë©”íƒ€ë°ì´í„° ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: ì˜ìƒ íŒŒì¼ê³¼ ê°™ì€ ìœ„ì¹˜)'
    )
    
    parser.add_argument(
        '--preview',
        action='store_true',
        help='ë©”íƒ€ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°ë§Œ ì¶œë ¥ (ì €ì¥í•˜ì§€ ì•ŠìŒ)'
    )
    
    args = parser.parse_args()
    
    try:
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = create_episode_metadata(
            book_title=args.title,
            language=args.language,
            video_path=args.video_path,
            thumbnail_path=args.thumbnail_path,
            author=args.author
        )
        
        # ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥
        print("=" * 60)
        print("ğŸ“‹ ë©”íƒ€ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        print("=" * 60)
        print()
        print(f"ğŸ“– ì±… ì œëª©: {args.title}")
        print(f"ğŸŒ ì–¸ì–´: {args.language.upper()}")
        print()
        print("ğŸ“ ì œëª©:")
        print(f"   {metadata['title']}")
        print()
        print("ğŸ“„ ì„¤ëª… (ì²˜ìŒ 200ì):")
        print(f"   {metadata['description'][:200]}...")
        print()
        print(f"ğŸ·ï¸ íƒœê·¸ ({len(metadata['tags'])}ê°œ):")
        for i, tag in enumerate(metadata['tags'][:10], 1):  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            print(f"   {i}. {tag}")
        if len(metadata['tags']) > 10:
            print(f"   ... ì™¸ {len(metadata['tags']) - 10}ê°œ")
        print()
        
        if metadata.get('video_path'):
            print(f"ğŸ¬ ì˜ìƒ: {Path(metadata['video_path']).name}")
        if metadata.get('thumbnail_path'):
            print(f"ğŸ“¸ ì¸ë„¤ì¼: {Path(metadata['thumbnail_path']).name}")
        if metadata.get('video_duration'):
            minutes = int(metadata['video_duration'] // 60)
            seconds = int(metadata['video_duration'] % 60)
            print(f"â±ï¸ ê¸¸ì´: {minutes}ë¶„ {seconds}ì´ˆ")
        print()
        
        # ì €ì¥
        if not args.preview:
            output_path = save_metadata(metadata, args.output)
            print()
            print("=" * 60)
            print("âœ… ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ!")
            print("=" * 60)
            print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_path}")
        else:
            print("â„¹ï¸ ë¯¸ë¦¬ë³´ê¸° ëª¨ë“œ: ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ì €ì¥í•˜ë ¤ë©´ --preview ì˜µì…˜ì„ ì œê±°í•˜ì„¸ìš”.")
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())

