"""
ìµœì  ì—…ë¡œë“œ ì‹œê°„ëŒ€ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ì—…ë¡œë“œ ë¡œê·¸ì™€ YouTube Analytics ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬
ìµœì ì˜ ì—…ë¡œë“œ ì‹œê°„ëŒ€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

ë¶„ì„ í•­ëª©:
- ìš”ì¼ë³„ ì—…ë¡œë“œ ì„±ê³¼
- ì‹œê°„ëŒ€ë³„ ì—…ë¡œë“œ ì„±ê³¼
- ì—…ë¡œë“œ í›„ 24ì‹œê°„/48ì‹œê°„ ì¡°íšŒìˆ˜ ì„±ì¥ë¥ 
- ìµœì  ì—…ë¡œë“œ ì‹œê°„ëŒ€ ì¶”ì²œ
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from collections import defaultdict
from dotenv import load_dotenv

# ìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import importlib.util
    spec = importlib.util.spec_from_file_location("youtube_analytics", Path(__file__).parent / "15_youtube_analytics.py")
    youtube_analytics_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(youtube_analytics_module)
    YouTubeAnalytics = youtube_analytics_module.YouTubeAnalytics
    ANALYTICS_AVAILABLE = True
except Exception as e:
    ANALYTICS_AVAILABLE = False
    print(f"âš ï¸ YouTube Analytics ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")

from utils.logger import get_logger

load_dotenv()


class OptimalUploadTimeAnalyzer:
    """ìµœì  ì—…ë¡œë“œ ì‹œê°„ëŒ€ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        self.analytics = None
        if ANALYTICS_AVAILABLE:
            try:
                self.analytics = YouTubeAnalytics()
            except Exception as e:
                self.logger.warning(f"YouTube Analytics ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def load_upload_log(self, log_path: str = "output/upload_log.json") -> List[Dict]:
        """ì—…ë¡œë“œ ë¡œê·¸ ë¡œë“œ"""
        log_file = Path(log_path)
        if not log_file.exists():
            self.logger.warning(f"ì—…ë¡œë“œ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_path}")
            self.logger.info("ğŸ’¡ ì—…ë¡œë“œ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë°©ë²•ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
            self.logger.info("   1. YouTubeì— ì˜ìƒì„ ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤:")
            self.logger.info("      python src/09_upload_from_metadata.py --privacy private --auto")
            self.logger.info("   2. ë˜ëŠ” ê¸°ì¡´ ì—…ë¡œë“œ ë¡œê·¸ íŒŒì¼ì´ ë‹¤ë¥¸ ìœ„ì¹˜ì— ìˆëŠ” ê²½ìš°:")
            self.logger.info("      python src/18_analyze_optimal_upload_time.py --upload-log <ê²½ë¡œ>")
            return []
        
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì¸ì§€ ë”•ì…”ë„ˆë¦¬ í˜•ì‹ì¸ì§€ í™•ì¸
            if isinstance(data, list):
                uploads = data
            elif isinstance(data, dict) and 'uploads' in data:
                uploads = data['uploads']
            else:
                uploads = []
            
            self.logger.info(f"âœ… ì—…ë¡œë“œ ë¡œê·¸ ë¡œë“œ ì™„ë£Œ: {len(uploads)}ê°œ ì˜ìƒ")
            return uploads
        except Exception as e:
            self.logger.error(f"ì—…ë¡œë“œ ë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def parse_upload_time(self, upload_entry: Dict) -> Optional[Tuple[datetime, str, int]]:
        """
        ì—…ë¡œë“œ ì‹œê°„ íŒŒì‹±
        
        Returns:
            (datetime, video_id, weekday) íŠœí”Œ
            weekday: 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
        """
        # ì—¬ëŸ¬ í•„ë“œì—ì„œ ì—…ë¡œë“œ ì‹œê°„ ì°¾ê¸°
        upload_time_str = (
            upload_entry.get('uploaded_at') or
            upload_entry.get('published_at') or
            upload_entry.get('publishedAt') or
            upload_entry.get('timestamp')
        )
        
        if not upload_time_str:
            return None
        
        try:
            # ISO í˜•ì‹ íŒŒì‹±
            if 'T' in upload_time_str:
                upload_time = datetime.fromisoformat(upload_time_str.replace('Z', '+00:00'))
            else:
                upload_time = datetime.strptime(upload_time_str, '%Y-%m-%d %H:%M:%S')
            
            # ë¡œì»¬ ì‹œê°„ëŒ€ë¡œ ë³€í™˜ (UTCê°€ ì•„ë‹Œ ê²½ìš°)
            if upload_time.tzinfo is None:
                # íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                pass
            
            weekday = upload_time.weekday()  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
            video_id = upload_entry.get('video_id') or upload_entry.get('id', '')
            
            return (upload_time, video_id, weekday)
        except Exception as e:
            self.logger.warning(f"ì—…ë¡œë“œ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {upload_time_str}, {e}")
            return None
    
    def get_video_early_metrics(
        self,
        video_id: str,
        upload_time: datetime,
        hours: int = 24
    ) -> Optional[Dict]:
        """
        ì˜ìƒ ì—…ë¡œë“œ í›„ ì´ˆê¸° ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            video_id: ì˜ìƒ ID
            upload_time: ì—…ë¡œë“œ ì‹œê°„
            hours: ë¶„ì„í•  ì‹œê°„ (ê¸°ë³¸ê°’: 24ì‹œê°„)
        
        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ (views, likes, comments ë“±)
        """
        if not self.analytics:
            return None
        
        try:
            start_date = upload_time.strftime('%Y-%m-%d')
            end_time = upload_time + timedelta(hours=hours)
            end_date = end_time.strftime('%Y-%m-%d')
            
            metrics = self.analytics.get_video_metrics(
                video_id=video_id,
                start_date=start_date,
                end_date=end_date
            )
            
            return metrics
        except Exception as e:
            self.logger.warning(f"ì˜ìƒ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({video_id}): {e}")
            return None
    
    def analyze_by_weekday(self, uploads: List[Dict]) -> Dict:
        """ìš”ì¼ë³„ ì—…ë¡œë“œ ì„±ê³¼ ë¶„ì„"""
        weekday_stats = defaultdict(lambda: {
            'count': 0,
            'total_views_24h': 0,
            'total_views_48h': 0,
            'total_likes_24h': 0,
            'avg_views_24h': 0,
            'avg_views_48h': 0,
            'avg_likes_24h': 0,
            'videos': []
        })
        
        weekday_names = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼']
        
        for upload in uploads:
            parsed = self.parse_upload_time(upload)
            if not parsed:
                continue
            
            upload_time, video_id, weekday = parsed
            
            # ì´ˆê¸° ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
            metrics_24h = self.get_video_early_metrics(video_id, upload_time, hours=24)
            metrics_48h = self.get_video_early_metrics(video_id, upload_time, hours=48)
            
            stats = weekday_stats[weekday]
            stats['count'] += 1
            stats['videos'].append({
                'video_id': video_id,
                'title': upload.get('title', 'N/A'),
                'upload_time': upload_time.isoformat()
            })
            
            if metrics_24h:
                views_24h = metrics_24h.get('views', 0)
                likes_24h = metrics_24h.get('likes', 0)
                stats['total_views_24h'] += views_24h
                stats['total_likes_24h'] += likes_24h
            
            if metrics_48h:
                views_48h = metrics_48h.get('views', 0)
                stats['total_views_48h'] += views_48h
        
        # í‰ê·  ê³„ì‚°
        for weekday, stats in weekday_stats.items():
            if stats['count'] > 0:
                stats['avg_views_24h'] = stats['total_views_24h'] / stats['count']
                stats['avg_views_48h'] = stats['total_views_48h'] / stats['count']
                stats['avg_likes_24h'] = stats['total_likes_24h'] / stats['count']
                stats['weekday_name'] = weekday_names[weekday]
        
        return dict(weekday_stats)
    
    def analyze_by_hour(self, uploads: List[Dict]) -> Dict:
        """ì‹œê°„ëŒ€ë³„ ì—…ë¡œë“œ ì„±ê³¼ ë¶„ì„"""
        hour_stats = defaultdict(lambda: {
            'count': 0,
            'total_views_24h': 0,
            'total_views_48h': 0,
            'total_likes_24h': 0,
            'avg_views_24h': 0,
            'avg_views_48h': 0,
            'avg_likes_24h': 0,
            'videos': []
        })
        
        for upload in uploads:
            parsed = self.parse_upload_time(upload)
            if not parsed:
                continue
            
            upload_time, video_id, weekday = parsed
            hour = upload_time.hour
            
            # ì´ˆê¸° ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°
            metrics_24h = self.get_video_early_metrics(video_id, upload_time, hours=24)
            metrics_48h = self.get_video_early_metrics(video_id, upload_time, hours=48)
            
            stats = hour_stats[hour]
            stats['count'] += 1
            stats['videos'].append({
                'video_id': video_id,
                'title': upload.get('title', 'N/A'),
                'upload_time': upload_time.isoformat()
            })
            
            if metrics_24h:
                views_24h = metrics_24h.get('views', 0)
                likes_24h = metrics_24h.get('likes', 0)
                stats['total_views_24h'] += views_24h
                stats['total_likes_24h'] += likes_24h
            
            if metrics_48h:
                views_48h = metrics_48h.get('views', 0)
                stats['total_views_48h'] += views_48h
        
        # í‰ê·  ê³„ì‚°
        for hour, stats in hour_stats.items():
            if stats['count'] > 0:
                stats['avg_views_24h'] = stats['total_views_24h'] / stats['count']
                stats['avg_views_48h'] = stats['total_views_48h'] / stats['count']
                stats['avg_likes_24h'] = stats['total_likes_24h'] / stats['count']
        
        return dict(hour_stats)
    
    def generate_report(
        self,
        weekday_stats: Dict,
        hour_stats: Dict,
        output_path: str = "output/optimal_upload_time_analysis.md"
    ) -> str:
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_lines = []
        report_lines.append("# ìµœì  ì—…ë¡œë“œ ì‹œê°„ëŒ€ ë¶„ì„ ë¦¬í¬íŠ¸")
        report_lines.append("")
        report_lines.append(f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # ìš”ì¼ë³„ ë¶„ì„
        report_lines.append("## ğŸ“… ìš”ì¼ë³„ ì—…ë¡œë“œ ì„±ê³¼")
        report_lines.append("")
        report_lines.append("| ìš”ì¼ | ì—…ë¡œë“œ ìˆ˜ | í‰ê·  ì¡°íšŒìˆ˜ (24h) | í‰ê·  ì¡°íšŒìˆ˜ (48h) | í‰ê·  ì¢‹ì•„ìš” (24h) |")
        report_lines.append("|------|----------|------------------|------------------|------------------|")
        
        sorted_weekdays = sorted(weekday_stats.items(), key=lambda x: x[1]['avg_views_24h'], reverse=True)
        for weekday, stats in sorted_weekdays:
            if stats['count'] > 0:
                report_lines.append(
                    f"| {stats['weekday_name']} | {stats['count']}ê°œ | "
                    f"{stats['avg_views_24h']:.1f} | {stats['avg_views_48h']:.1f} | "
                    f"{stats['avg_likes_24h']:.1f} |"
                )
        
        report_lines.append("")
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        report_lines.append("## â° ì‹œê°„ëŒ€ë³„ ì—…ë¡œë“œ ì„±ê³¼")
        report_lines.append("")
        report_lines.append("| ì‹œê°„ | ì—…ë¡œë“œ ìˆ˜ | í‰ê·  ì¡°íšŒìˆ˜ (24h) | í‰ê·  ì¡°íšŒìˆ˜ (48h) | í‰ê·  ì¢‹ì•„ìš” (24h) |")
        report_lines.append("|------|----------|------------------|------------------|------------------|")
        
        sorted_hours = sorted(hour_stats.items(), key=lambda x: x[1]['avg_views_24h'], reverse=True)
        for hour, stats in sorted_hours:
            if stats['count'] > 0:
                report_lines.append(
                    f"| {hour:02d}:00 | {stats['count']}ê°œ | "
                    f"{stats['avg_views_24h']:.1f} | {stats['avg_views_48h']:.1f} | "
                    f"{stats['avg_likes_24h']:.1f} |"
                )
        
        report_lines.append("")
        
        # ìµœì  ì—…ë¡œë“œ ì‹œê°„ ì¶”ì²œ
        report_lines.append("## ğŸ¯ ìµœì  ì—…ë¡œë“œ ì‹œê°„ ì¶”ì²œ")
        report_lines.append("")
        
        if sorted_weekdays:
            best_weekday = sorted_weekdays[0]
            report_lines.append(f"### ìµœì  ì—…ë¡œë“œ ìš”ì¼: **{best_weekday[1]['weekday_name']}**")
            report_lines.append(f"- í‰ê·  ì¡°íšŒìˆ˜ (24h): {best_weekday[1]['avg_views_24h']:.1f}")
            report_lines.append(f"- í‰ê·  ì¡°íšŒìˆ˜ (48h): {best_weekday[1]['avg_views_48h']:.1f}")
            report_lines.append("")
        
        if sorted_hours:
            best_hour = sorted_hours[0]
            report_lines.append(f"### ìµœì  ì—…ë¡œë“œ ì‹œê°„: **{best_hour[0]:02d}:00**")
            report_lines.append(f"- í‰ê·  ì¡°íšŒìˆ˜ (24h): {best_hour[1]['avg_views_24h']:.1f}")
            report_lines.append(f"- í‰ê·  ì¡°íšŒìˆ˜ (48h): {best_hour[1]['avg_views_48h']:.1f}")
            report_lines.append("")
        
        # ìƒìœ„ 3ê°œ ì‹œê°„ëŒ€ ì¶”ì²œ
        report_lines.append("### ì¶”ì²œ ì—…ë¡œë“œ ì‹œê°„ëŒ€ (ìƒìœ„ 3ê°œ)")
        report_lines.append("")
        for i, (hour, stats) in enumerate(sorted_hours[:3], 1):
            if stats['count'] > 0:
                report_lines.append(f"{i}. **{hour:02d}:00** - í‰ê·  ì¡°íšŒìˆ˜ (24h): {stats['avg_views_24h']:.1f}")
        report_lines.append("")
        
        report_lines.append("---")
        report_lines.append("")
        report_lines.append("**ì°¸ê³ **: ì´ ë¶„ì„ì€ ì—…ë¡œë“œ í›„ 24ì‹œê°„/48ì‹œê°„ ë‚´ ì´ˆê¸° ì„±ê³¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤.")
        report_lines.append("ì‹¤ì œ ìµœì  ì‹œê°„ëŒ€ëŠ” ì±„ë„ì˜ íƒ€ê²Ÿ ì‹œì²­ìì¸µê³¼ ì½˜í…ì¸  íŠ¹ì„±ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        self.logger.info(f"âœ… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_path}")
        return output_path


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ìµœì  ì—…ë¡œë“œ ì‹œê°„ëŒ€ ë¶„ì„')
    parser.add_argument('--upload-log', type=str, default='output/upload_log.json', help='ì—…ë¡œë“œ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', type=str, default='output/optimal_upload_time_analysis.md', help='ì¶œë ¥ ë¦¬í¬íŠ¸ íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    analyzer = OptimalUploadTimeAnalyzer()
    
    # ì—…ë¡œë“œ ë¡œê·¸ ë¡œë“œ
    uploads = analyzer.load_upload_log(args.upload_log)
    
    if not uploads:
        print("âŒ ë¶„ì„í•  ì—…ë¡œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“Š {len(uploads)}ê°œ ì˜ìƒ ë¶„ì„ ì‹œì‘...")
    
    # ìš”ì¼ë³„ ë¶„ì„
    print("ğŸ“… ìš”ì¼ë³„ ì„±ê³¼ ë¶„ì„ ì¤‘...")
    weekday_stats = analyzer.analyze_by_weekday(uploads)
    
    # ì‹œê°„ëŒ€ë³„ ë¶„ì„
    print("â° ì‹œê°„ëŒ€ë³„ ì„±ê³¼ ë¶„ì„ ì¤‘...")
    hour_stats = analyzer.analyze_by_hour(uploads)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    print("ğŸ“ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    report_path = analyzer.generate_report(weekday_stats, hour_stats, args.output)
    
    print(f"âœ… ë¶„ì„ ì™„ë£Œ: {report_path}")


if __name__ == "__main__":
    main()

