"""
Phase 4: ì˜ìƒ í•©ì„± ë° í¸ì§‘ ìŠ¤í¬ë¦½íŠ¸
- ì˜¤ë””ì˜¤ ë¡œë“œ
- ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ ìƒì„± (ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶°)
- Ken Burns íš¨ê³¼ (ì¤Œì¸/íŒ¨ë‹)
- ì „í™˜ íš¨ê³¼ (í˜ì´ë“œ)
- ìë§‰ (OpenAI Whisper, ì„ íƒì‚¬í•­)
- ë Œë”ë§: 1080p, 30fps MP4
"""

import os
import random
import math
import numpy as np
from pathlib import Path
from typing import List, Optional, Tuple
from dotenv import load_dotenv

try:
    from moviepy.editor import ImageClip, AudioFileClip, CompositeVideoClip, TextClip, ColorClip, concatenate_videoclips, VideoFileClip
    from moviepy.video.fx.all import fadein, fadeout
    MOVIEPY_AVAILABLE = True
    MOVIEPY_VERSION_NEW = True
except ImportError as e:
    try:
        # êµ¬ë²„ì „ í˜¸í™˜ì„±
        from moviepy import ImageClip, AudioFileClip, CompositeVideoClip, TextClip, ColorClip, concatenate_videoclips, VideoFileClip
        from moviepy.video.fx import FadeIn, FadeOut, CrossFadeIn, CrossFadeOut
        MOVIEPY_AVAILABLE = True
        MOVIEPY_VERSION_NEW = False
    except ImportError:
        MOVIEPY_AVAILABLE = False
        MOVIEPY_VERSION_NEW = False
        # MoviePy import ì˜¤ë¥˜ëŠ” ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ë°œìƒí•˜ë¯€ë¡œ ë¡œê±°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
        # print ë¬¸ ìœ ì§€
        print(f"âš ï¸ MoviePy import ì˜¤ë¥˜: {e}")
        print("pip install moviepy")

try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False

try:
    from utils.logger import get_logger
except ImportError:
    from src.utils.logger import get_logger

load_dotenv()


class VideoMaker:
    """ì˜ìƒ ì œì‘ í´ë˜ìŠ¤"""
    
    def __init__(self, resolution: Tuple[int, int] = (1920, 1080), fps: int = 30, bitrate: str = "5000k", audio_bitrate: str = "320k"):
        """
        Args:
            resolution: í•´ìƒë„ (width, height)
            fps: í”„ë ˆì„ë ˆì´íŠ¸
            bitrate: ë¹„ë””ì˜¤ ë¹„íŠ¸ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: "5000k")
            audio_bitrate: ì˜¤ë””ì˜¤ ë¹„íŠ¸ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: "320k")
        """
        self.logger = get_logger(__name__)
        self.resolution = resolution
        self.fps = fps
        self.bitrate = bitrate
        self.audio_bitrate = audio_bitrate
        
        if not MOVIEPY_AVAILABLE:
            raise ImportError("MoviePyê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install moviepy")
    
    def load_audio(self, audio_path: str) -> AudioFileClip:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ"""
        audio_file = Path(audio_path)
        if not audio_file.exists():
            raise FileNotFoundError(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
        
        self.logger.info(f"ğŸµ ì˜¤ë””ì˜¤ ë¡œë“œ ì¤‘: {audio_path}")
        try:
            audio = AudioFileClip(audio_path)
            self.logger.info(f"   ê¸¸ì´: {audio.duration:.2f}ì´ˆ")
        except Exception as e:
            raise ValueError(f"ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return audio
    
    def concatenate_audios(
        self,
        audio_paths: List[str],
        output_path: str = None,
        fade_duration: float = 1.0,
        gap_duration: float = 2.0
    ) -> AudioFileClip:
        """
        ì—¬ëŸ¬ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—°ê²°
        
        Args:
            audio_paths: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            output_path: ì—°ê²°ëœ ì˜¤ë””ì˜¤ ì €ì¥ ê²½ë¡œ (ì„ íƒì‚¬í•­)
            fade_duration: ì „í™˜ í˜ì´ë“œ ì‹œê°„ (ì´ˆ)
            gap_duration: ì˜¤ë””ì˜¤ ê°„ ê°„ê²© ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 3.0)
            
        Returns:
            ì—°ê²°ëœ ì˜¤ë””ì˜¤ í´ë¦½
        """
        if not audio_paths:
            raise ValueError("ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        self.logger.info("ğŸ”— ì˜¤ë””ì˜¤ ì—°ê²° ì¤‘...")
        audio_clips = []
        
        for i, audio_path in enumerate(audio_paths):
            self.logger.info(f"[{i+1}/{len(audio_paths)}] ë¡œë“œ: {Path(audio_path).name}")
            audio_clip = self.load_audio(audio_path)
            
            # ì˜¤ë””ì˜¤ í´ë¦½ì— fade íš¨ê³¼ ì ìš© (ì˜¤ë””ì˜¤ ì „ìš© ë©”ì„œë“œ ì‚¬ìš©)
            if i > 0:
                # ì´ì „ í´ë¦½ì— fade out
                if audio_clips:
                    try:
                        from moviepy.audio.fx.all import audio_fadeout
                        audio_clips[-1] = audio_clips[-1].fx(audio_fadeout, fade_duration)
                    except ImportError:
                        # êµ¬ë²„ì „ í˜¸í™˜ì„± ë˜ëŠ” fade íš¨ê³¼ ì—†ì´ ì§„í–‰
                        pass
                
                # ì˜¤ë””ì˜¤ ê°„ ê°„ê²© ì¶”ê°€ (ì¡°ìš©í•œ êµ¬ê°„)
                if gap_duration > 0:
                    self.logger.info(f"â¸ï¸  {gap_duration}ì´ˆ ê°„ê²© ì¶”ê°€...")
                    try:
                        # ë¬´ìŒ ì˜¤ë””ì˜¤ í´ë¦½ ìƒì„±
                        from moviepy.audio.AudioClip import AudioArrayClip
                        import numpy as np
                        # ìƒ˜í”Œë ˆì´íŠ¸ ê°€ì ¸ì˜¤ê¸°
                        sample_rate = audio_clip.fps if hasattr(audio_clip, 'fps') else 44100
                        # ë¬´ìŒ ë°°ì—´ ìƒì„± (ìŠ¤í…Œë ˆì˜¤)
                        silence_array = np.zeros((int(sample_rate * gap_duration), 2))
                        silence = AudioArrayClip(silence_array, fps=sample_rate)
                        audio_clips.append(silence)
                    except Exception as e:
                        # AudioArrayClip ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
                        try:
                            from moviepy.editor import ColorClip
                            # ê²€ì€ìƒ‰ ë¹„ë””ì˜¤ í´ë¦½ ìƒì„± (ë¬´ìŒ ì˜¤ë””ì˜¤ í¬í•¨)
                            silence_video = ColorClip(size=(1, 1), color=(0, 0, 0), duration=gap_duration)
                            # ë¬´ìŒ ì˜¤ë””ì˜¤ ì¶”ê°€
                            from moviepy.audio.AudioClip import AudioClip
                            silence_audio = AudioClip(lambda t: [0, 0], duration=gap_duration, fps=44100)
                            silence_video = silence_video.set_audio(silence_audio)
                            audio_clips.append(silence_video)
                        except Exception as e2:
                            # ê°„ê²© ì¶”ê°€ ì‹¤íŒ¨ ì‹œ ê²½ê³ ë§Œ ì¶œë ¥í•˜ê³  ê³„ì† ì§„í–‰
                            self.logger.warning(f"ê°„ê²© ì¶”ê°€ ì‹¤íŒ¨: {e2}, ê°„ê²© ì—†ì´ ì—°ê²°í•©ë‹ˆë‹¤.")
                
                # í˜„ì¬ í´ë¦½ì— fade in
                try:
                    from moviepy.audio.fx.all import audio_fadein
                    audio_clip = audio_clip.fx(audio_fadein, fade_duration)
                except ImportError:
                    # êµ¬ë²„ì „ í˜¸í™˜ì„± ë˜ëŠ” fade íš¨ê³¼ ì—†ì´ ì§„í–‰
                    pass
            
            audio_clips.append(audio_clip)
        
        # ë§ˆì§€ë§‰ í´ë¦½ì— fade out
        if audio_clips:
            try:
                from moviepy.audio.fx.all import audio_fadeout
                audio_clips[-1] = audio_clips[-1].fx(audio_fadeout, fade_duration)
            except ImportError:
                pass
        
        # ì˜¤ë””ì˜¤ í´ë¦½ë“¤ì„ ì—°ê²°
        self.logger.info("ì—°ê²° ì¤‘...")
        try:
            from moviepy.audio.AudioClip import concatenate_audioclips
            final_audio = concatenate_audioclips(audio_clips)
        except ImportError:
            # êµ¬ë²„ì „ í˜¸í™˜ì„±: ë¹„ë””ì˜¤ í´ë¦½ìœ¼ë¡œ ë³€í™˜ í›„ ì—°ê²°
            from moviepy.editor import ColorClip
            video_clips = []
            for audio_clip in audio_clips:
                # ì˜¤ë””ì˜¤ ê¸¸ì´ë§Œí¼ì˜ ê²€ì€ìƒ‰ ë¹„ë””ì˜¤ í´ë¦½ ìƒì„±
                video_clip = ColorClip(size=(1, 1), color=(0, 0, 0), duration=audio_clip.duration)
                video_clip = video_clip.set_audio(audio_clip)
                video_clips.append(video_clip)
            concatenated = concatenate_videoclips(video_clips, method="compose")
            final_audio = concatenated.audio
        
        self.logger.info(f"âœ… ì—°ê²° ì™„ë£Œ: ì´ ê¸¸ì´ {final_audio.duration:.2f}ì´ˆ")
        
        # ì €ì¥ (ì„ íƒì‚¬í•­)
        if output_path:
            self.logger.info(f"ğŸ’¾ ì €ì¥ ì¤‘: {output_path}")
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            final_audio.write_audiofile(output_path, codec='aac', bitrate='192k')
            self.logger.info("âœ… ì €ì¥ ì™„ë£Œ")
        
        return final_audio
    
    def _ease_in_out(self, t: float) -> float:
        """
        ë¶€ë“œëŸ¬ìš´ easing í•¨ìˆ˜ (ease-in-out cubic)
        ì‹œì‘ê³¼ ëì—ì„œ ëŠë¦¬ê²Œ, ì¤‘ê°„ì—ì„œ ë¹ ë¥´ê²Œ
        """
        if t < 0.5:
            return 4 * t * t * t
        else:
            return 1 - pow(-2 * t + 2, 3) / 2
    
    def create_image_clip_with_ken_burns(
        self,
        image_path: str,
        duration: float,
        effect_type: str = "zoom_in",
        start_scale: float = 1.0,
        end_scale: float = 1.2,
        pan_direction: Optional[str] = None
    ) -> ImageClip:
        """
        Ken Burns íš¨ê³¼ê°€ ì ìš©ëœ ì´ë¯¸ì§€ í´ë¦½ ìƒì„± (ë¶€ë“œëŸ¬ìš´ ì• ë‹ˆë©”ì´ì…˜)
        
        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
            duration: í´ë¦½ ê¸¸ì´ (ì´ˆ)
            effect_type: íš¨ê³¼ íƒ€ì… ("zoom_in", "zoom_out")
            start_scale: ì‹œì‘ ìŠ¤ì¼€ì¼
            end_scale: ë ìŠ¤ì¼€ì¼
            pan_direction: íŒ¨ë‹ ë°©í–¥ ("left", "right", "up", "down")
        """
        from PIL import Image
        import numpy as np
        
        # ì´ë¯¸ì§€ ë¡œë“œ ë° ë¦¬ì‚¬ì´ì¦ˆ (í•´ìƒë„ë³´ë‹¤ í¬ê²Œ)
        img = Image.open(image_path)
        img_width, img_height = img.size
        
        # í•´ìƒë„ ë¹„ìœ¨ ê³„ì‚°
        target_width, target_height = self.resolution
        aspect_ratio = target_width / target_height
        img_aspect = img_width / img_height
        
        # ì´ë¯¸ì§€ë¥¼ í•´ìƒë„ë³´ë‹¤ í¬ê²Œ ë¦¬ì‚¬ì´ì¦ˆ (ì¤Œ íš¨ê³¼ë¥¼ ìœ„í•´)
        # ìµœëŒ€ ìŠ¤ì¼€ì¼ë³´ë‹¤ ë” í¬ê²Œ ë¦¬ì‚¬ì´ì¦ˆí•˜ì—¬ íŒ¨ë‹ ì—¬ìœ  ê³µê°„ í™•ë³´
        max_scale = max(end_scale, start_scale) * 1.2  # 20% ì—¬ìœ 
        scaled_width = int(target_width * max_scale)
        scaled_height = int(target_height * max_scale)
        
        # ì¢…íš¡ë¹„ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ
        if img_aspect > aspect_ratio:
            # ì´ë¯¸ì§€ê°€ ë” ë„“ìŒ
            scaled_height = int(scaled_width / img_aspect)
        else:
            # ì´ë¯¸ì§€ê°€ ë” ë†’ìŒ
            scaled_width = int(scaled_height * img_aspect)
        
        # ì´ë¯¸ì§€ ëª¨ë“œ ë³€í™˜ (RGBë¡œ í†µì¼)
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        # ê³ í’ˆì§ˆ ë¦¬ì‚¬ì´ì¦ˆ
        img = img.resize((scaled_width, scaled_height), Image.Resampling.LANCZOS)
        
        # ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (í•œ ë²ˆë§Œ)
        img_array = np.array(img)
        
        # ë°°ì—´ shape í™•ì¸ ë° ìˆ˜ì • (ë†’ì´, ë„ˆë¹„, ì±„ë„ ìˆœì„œ)
        if len(img_array.shape) == 2:
            # Grayscale ì´ë¯¸ì§€ì¸ ê²½ìš° RGBë¡œ ë³€í™˜
            img_array = np.stack([img_array, img_array, img_array], axis=-1)
        elif len(img_array.shape) == 3 and img_array.shape[2] != 3:
            # ì±„ë„ì´ 3ê°œê°€ ì•„ë‹Œ ê²½ìš° (ì˜ˆ: RGBA)
            if img_array.shape[2] == 4:
                # RGBA -> RGB ë³€í™˜
                img_array = img_array[:, :, :3]
            else:
                # ë‹¤ë¥¸ ì±„ë„ ìˆ˜ì¸ ê²½ìš° RGBë¡œ ë³€í™˜
                img = Image.fromarray(img_array).convert('RGB')
                img_array = np.array(img)
        
        # ì„¸ë¡œí˜• ì´ë¯¸ì§€ ì—¬ë¶€ í™•ì¸ (ë†’ì´ê°€ ë” ê¸´ ì´ë¯¸ì§€)
        is_portrait = img_aspect < aspect_ratio
        
        # Ken Burns íš¨ê³¼ ì ìš© (ë¶€ë“œëŸ¬ìš´ ì• ë‹ˆë©”ì´ì…˜)
        def make_frame(t):
            # ì§„í–‰ë¥  ê³„ì‚° (0.0 ~ 1.0)
            progress = t / duration if duration > 0 else 0
            progress = min(1.0, max(0.0, progress))
            
            # Easing ì ìš© (ë¶€ë“œëŸ¬ìš´ ì „í™˜)
            eased_progress = self._ease_in_out(progress)
            
            # ìŠ¤ì¼€ì¼ ê³„ì‚° (easing ì ìš©)
            if effect_type == "zoom_out":
                current_scale = start_scale + (end_scale - start_scale) * (1 - eased_progress)
            else:  # zoom_in or default
                current_scale = start_scale + (end_scale - start_scale) * eased_progress
            
            # íŒ¨ë‹ ê³„ì‚° (easing ì ìš©)
            pan_x = 0
            pan_y = 0
            if pan_direction:
                # íŒ¨ë‹ë„ easing ì ìš©í•˜ì—¬ ë¶€ë“œëŸ½ê²Œ
                pan_amount = 0.15 * eased_progress  # ìµœëŒ€ 15% ì´ë™
                if pan_direction == "left":
                    pan_x = -pan_amount
                elif pan_direction == "right":
                    pan_x = pan_amount
                elif pan_direction == "up":
                    pan_y = -pan_amount
                elif pan_direction == "down":
                    pan_y = pan_amount
            
            # ì„¸ë¡œí˜• ì´ë¯¸ì§€ ì²˜ë¦¬: ì›ë³¸ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë†’ì´ì— ë§ì¶° ì¤‘ì•™ ë°°ì¹˜
            if is_portrait:
                # ë†’ì´ë¥¼ target_heightì— ì •í™•íˆ ë§ì¶¤ (ìŠ¤ì¼€ì¼ íš¨ê³¼ ì—†ì´)
                display_height = target_height
                display_width = int(display_height * img_aspect)
                
                try:
                    # ì›ë³¸ ì´ë¯¸ì§€ ë°°ì—´ í™•ì¸
                    if len(img_array.shape) != 3 or img_array.shape[2] != 3:
                        from PIL import Image as PILImage
                        img_pil = PILImage.fromarray(img_array).convert('RGB')
                        img_array = np.array(img_pil)
                    
                    # ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì›ë³¸ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ
                    from PIL import Image as PILImage
                    img_pil = PILImage.fromarray(img_array)
                    resized_img = img_pil.resize((display_width, display_height), Image.Resampling.LANCZOS)
                    
                    # ê²€ì€ìƒ‰ ë°°ê²½ì— ì¤‘ì•™ ë°°ì¹˜ (letterbox íš¨ê³¼)
                    final_img = Image.new('RGB', (target_width, target_height), (0, 0, 0))
                    paste_x = (target_width - display_width) // 2
                    paste_y = 0  # ë†’ì´ì— ë§ì¶°ì„œ ìœ„ì—ì„œë¶€í„° ë°°ì¹˜
                    final_img.paste(resized_img, (paste_x, paste_y))
                    
                    return np.array(final_img)
                except (IndexError, ValueError, TypeError) as e:
                    # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš© (ë¹„ìœ¨ ìœ ì§€)
                    from PIL import Image as PILImage
                    img_pil = PILImage.fromarray(img_array).convert('RGB')
                    display_height = target_height
                    display_width = int(display_height * img_aspect)
                    resized = img_pil.resize((display_width, display_height), Image.Resampling.LANCZOS)
                    
                    # ê²€ì€ìƒ‰ ë°°ê²½ì— ì¤‘ì•™ ë°°ì¹˜
                    final_img = Image.new('RGB', (target_width, target_height), (0, 0, 0))
                    paste_x = (target_width - display_width) // 2
                    paste_y = 0
                    final_img.paste(resized, (paste_x, paste_y))
                    
                    return np.array(final_img)
            else:
                # ê°€ë¡œí˜• ì´ë¯¸ì§€: ê¸°ì¡´ ë¡œì§ ìœ ì§€
                # í˜„ì¬ í”„ë ˆì„ í¬ê¸° ê³„ì‚°
                current_width = int(target_width / current_scale)
                current_height = int(target_height / current_scale)
                
                # ì¤‘ì‹¬ì  ê³„ì‚° (ìŠ¤ì¼€ì¼ëœ ì´ë¯¸ì§€ ê¸°ì¤€)
                center_x = scaled_width // 2
                center_y = scaled_height // 2
                
                # íŒ¨ë‹ ì ìš© (ìŠ¤ì¼€ì¼ëœ ì´ë¯¸ì§€ í¬ê¸° ê¸°ì¤€)
                center_x += int(pan_x * scaled_width)
                center_y += int(pan_y * scaled_height)
                
                # í¬ë¡­ ì˜ì—­ ê³„ì‚° (ê²½ê³„ ì²´í¬ ê°•í™”)
                left = max(0, center_x - current_width // 2)
                top = max(0, center_y - current_height // 2)
                right = min(scaled_width, left + current_width)
                bottom = min(scaled_height, top + current_height)
                
                # ìœ íš¨ì„± ê²€ì‚¬
                if right <= left or bottom <= top:
                    # ì˜ëª»ëœ í¬ë¡­ ì˜ì—­ì´ë©´ ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©
                    from PIL import Image as PILImage
                    resized = PILImage.fromarray(img_array).resize((target_width, target_height), Image.Resampling.LANCZOS)
                    return np.array(resized)
                
                # í¬ë¡­ (numpy ë°°ì—´ ìŠ¬ë¼ì´ì‹± ì‚¬ìš© - ë” ë¹ ë¦„)
                try:
                    # ë°°ì—´ shape í™•ì¸: (height, width, channels)
                    if len(img_array.shape) != 3 or img_array.shape[2] != 3:
                        # RGBê°€ ì•„ë‹ˆë©´ PILë¡œ ë³€í™˜ í›„ ì²˜ë¦¬
                        from PIL import Image as PILImage
                        img_pil = PILImage.fromarray(img_array).convert('RGB')
                        img_array = np.array(img_pil)
                    
                    cropped = img_array[top:bottom, left:right]
                    
                    # ë¹ˆ ë°°ì—´ ì²´í¬
                    if cropped.size == 0 or len(cropped.shape) != 3:
                        from PIL import Image as PILImage
                        resized = PILImage.fromarray(img_array).resize((target_width, target_height), Image.Resampling.LANCZOS)
                        return np.array(resized)
                    
                    # ë¦¬ì‚¬ì´ì¦ˆ (ê³ í’ˆì§ˆ)
                    from PIL import Image as PILImage
                    cropped_img = PILImage.fromarray(cropped)
                    resized = cropped_img.resize((target_width, target_height), Image.Resampling.LANCZOS)
                    
                    return np.array(resized)
                except (IndexError, ValueError, TypeError) as e:
                    # í¬ë¡­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
                    from PIL import Image as PILImage
                    if len(img_array.shape) == 3:
                        img_pil = PILImage.fromarray(img_array).convert('RGB')
                    else:
                        img_pil = PILImage.fromarray(img_array).convert('RGB')
                    resized = img_pil.resize((target_width, target_height), Image.Resampling.LANCZOS)
                    return np.array(resized)
        
        # make_frame í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë¦½ ìƒì„±
        try:
            # fl ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë ˆì„ë³„ë¡œ íš¨ê³¼ ì ìš©
            clip = ImageClip(img_array, duration=duration)
            clip = clip.fl(lambda get_frame, t: make_frame(t), apply_to=['video'])
        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í´ë¦½ ë°˜í™˜ (íš¨ê³¼ ì—†ì´)
            self.logger.warning(f"Ken Burns íš¨ê³¼ ì ìš© ì‹¤íŒ¨, ê¸°ë³¸ í´ë¦½ ì‚¬ìš©: {e}")
            clip = ImageClip(img_array, duration=duration)
            clip = clip.resized(newsize=self.resolution)
        
        return clip
    
    def create_image_sequence(
        self,
        image_paths: List[str],
        total_duration: float,
        fade_duration: float = 1.5,  # í˜ì´ë“œ ì „í™˜ ì‹œê°„ (1.5ì´ˆ - ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜)
        use_ken_burns: bool = True  # Ken Burns ì¤Œ/íŒ¨ë‹ íš¨ê³¼ ì‚¬ìš© ì—¬ë¶€
    ) -> List[ImageClip]:
        """
        ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ ìƒì„± (ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶° ë°˜ë³µ)
        - ì´ë¯¸ì§€ 20ê°œë¥¼ ì˜ìƒì´ ëë‚  ë•Œê¹Œì§€ ê³„ì† ë°˜ë³µ
        - ìì—°ìŠ¤ëŸ¬ìš´ fade out/in ì „í™˜ íš¨ê³¼ ì ìš©
        
        Args:
            image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (20ê°œ)
            total_duration: ì „ì²´ ê¸¸ì´ (ì˜¤ë””ì˜¤ ê¸¸ì´)
            fade_duration: í˜ì´ë“œ ì „í™˜ ì‹œê°„ (ê¸°ë³¸ê°’: 1.5ì´ˆ - ìì—°ìŠ¤ëŸ¬ìš´ ì „í™˜)
        """
        if not image_paths:
            raise ValueError("ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        num_images = len(image_paths)
        
        # ì´ë¯¸ì§€ë‹¹ ìµœì  í‘œì‹œ ì‹œê°„ ê³„ì‚°
        # ì‹œì²­ì ê´€ì ì—ì„œ ìµœì : 4-5ì´ˆ
        optimal_duration_per_image = 4.5  # ìµœì  í‘œì‹œ ì‹œê°„: 4.5ì´ˆ
        min_duration_per_image = 4.0  # ìµœì†Œ í‘œì‹œ ì‹œê°„: 4ì´ˆ
        max_duration_per_image = 6.0  # ìµœëŒ€ í‘œì‹œ ì‹œê°„: 6ì´ˆ
        
        # ì „ì²´ ê¸¸ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ì´ë¯¸ì§€ë‹¹ í‘œì‹œ ì‹œê°„ ê³„ì‚°
        calculated_duration = total_duration / num_images
        
        # ìµœì  ë²”ìœ„ ë‚´ë¡œ ì¡°ì •
        if calculated_duration < min_duration_per_image:
            duration_per_image = min_duration_per_image
        elif calculated_duration > max_duration_per_image:
            duration_per_image = max_duration_per_image
        else:
            duration_per_image = calculated_duration
        
        # í˜ì´ë“œ ì „í™˜ ì‹œê°„ ì¡°ì • (ì´ë¯¸ì§€ í‘œì‹œ ì‹œê°„ì˜ 30% ì´í•˜ë¡œ ì œí•œ)
        fade_duration = min(fade_duration, duration_per_image * 0.3)
        
        # ì˜ìƒ ê¸¸ì´ì™€ ìƒê´€ì—†ì´ 100ê°œ ì´ë¯¸ì§€ë¥¼ ë²ˆê°ˆì•„ê°€ë©´ì„œ ì‚¬ìš©
        # ì´ë¯¸ì§€ ê²½ë¡œë¥¼ 100ê°œë¡œ ì œí•œ (ë” ë§ìœ¼ë©´ ì•ì—ì„œ 100ê°œë§Œ ì‚¬ìš©)
        max_images = 100
        if len(image_paths) > max_images:
            image_paths = image_paths[:max_images]
            self.logger.warning(f"ì´ë¯¸ì§€ê°€ {len(image_paths)}ê°œ ì´ìƒì…ë‹ˆë‹¤. ì•ì—ì„œ {max_images}ê°œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ì˜ìƒì´ ëë‚  ë•Œê¹Œì§€ í•„ìš”í•œ ì´ë¯¸ì§€ ê°œìˆ˜ ê³„ì‚°
        num_needed = math.ceil(total_duration / duration_per_image)
        num_cycles = math.ceil(num_needed / len(image_paths))
        
        self.logger.info(f"ğŸ“Š ì‚¬ìš©í•  ì´ë¯¸ì§€ ê°œìˆ˜: {len(image_paths)}ê°œ (ìµœëŒ€ 100ê°œ)")
        self.logger.info(f"ğŸ“Š í•„ìš”í•œ ì´ ì´ë¯¸ì§€ ê°œìˆ˜: {num_needed}ê°œ")
        self.logger.info(f"â±ï¸  ì´ë¯¸ì§€ë‹¹ í‘œì‹œ ì‹œê°„: {duration_per_image:.1f}ì´ˆ")
        self.logger.info(f"ğŸ¨ í˜ì´ë“œ ì „í™˜ ì‹œê°„: {fade_duration:.1f}ì´ˆ (fade out/in)")
        self.logger.info(f"ğŸ”„ ë°˜ë³µ íšŸìˆ˜: {num_cycles}íšŒ (100ê°œ ì´ë¯¸ì§€ë¥¼ ìˆœí™˜ ì‚¬ìš©)")
        self.logger.info("ğŸ’¡ ì‹œì²­ì ê´€ì  ê¶Œì¥: ì´ë¯¸ì§€ë‹¹ 4-5ì´ˆê°€ ê°€ì¥ ìì—°ìŠ¤ëŸ½ê³  ì ì ˆí•©ë‹ˆë‹¤")
        
        clips = []
        current_time = 0.0
        image_index = 0  # ì´ë¯¸ì§€ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘í•˜ì—¬ ìˆœí™˜)
        
        # ì˜ìƒì´ ëë‚  ë•Œê¹Œì§€ 100ê°œ ì´ë¯¸ì§€ë¥¼ ìˆœí™˜í•˜ë©´ì„œ ì‚¬ìš©
        while current_time < total_duration:
            # í˜„ì¬ ì‚¬ìš©í•  ì´ë¯¸ì§€ (ìˆœí™˜)
            image_path = image_paths[image_index % len(image_paths)]
            if current_time >= total_duration:
                break
            
            # í´ë¦½ ê¸¸ì´ ê³„ì‚° (ë§ˆì§€ë§‰ í´ë¦½ì€ ë‚¨ì€ ì‹œê°„ë§Œí¼ë§Œ)
            remaining_time = total_duration - current_time
            clip_duration = min(duration_per_image, remaining_time)
            
            if clip_duration <= 0:
                break
            
            # Ken Burns íš¨ê³¼ ë˜ëŠ” ì •ì  ì´ë¯¸ì§€ ì‚¬ìš©
            from PIL import Image as PILImage
            import numpy as np
            if use_ken_burns:
                # Ken Burns ì¤Œ/íŒ¨ë‹ íš¨ê³¼ ì ìš© (ì´íƒˆë¥  ê°ì†Œ íš¨ê³¼)
                effect_types = ["zoom_in", "zoom_out"]
                pan_directions = [None, "left", "right", None]
                effect_type = effect_types[image_index % len(effect_types)]
                pan_dir = pan_directions[image_index % len(pan_directions)]
                try:
                    clip = self.create_image_clip_with_ken_burns(
                        image_path=image_path,
                        duration=clip_duration,
                        effect_type=effect_type,
                        start_scale=1.0,
                        end_scale=1.15,
                        pan_direction=pan_dir
                    )
                except Exception as e:
                    self.logger.warning(f"Ken Burns íš¨ê³¼ ì ìš© ì‹¤íŒ¨ ({Path(image_path).name}): {e}, ì •ì  ì´ë¯¸ì§€ë¡œ ëŒ€ì²´")
                    use_ken_burns = False  # ì´í›„ ì´ë¯¸ì§€ëŠ” ì •ì ìœ¼ë¡œ
            if not use_ken_burns:
                try:
                    img = PILImage.open(image_path)
                    # RGBë¡œ ë³€í™˜
                    if img.mode != 'RGB':
                        img = img.convert('RGB')

                    # ì„¸ë¡œí˜• ì´ë¯¸ì§€ ì—¬ë¶€ í™•ì¸
                    img_width, img_height = img.size
                    target_width, target_height = self.resolution
                    aspect_ratio = target_width / target_height
                    img_aspect = img_width / img_height
                    is_portrait = img_aspect < aspect_ratio

                    if is_portrait:
                        # ì„¸ë¡œí˜• ì´ë¯¸ì§€: ë†’ì´ì— ë§ì¶”ê³  ì¢Œìš°ëŠ” ê²€ì€ìƒ‰
                        display_height = target_height
                        display_width = int(display_height * img_aspect)
                        resized_img = img.resize((display_width, display_height), PILImage.Resampling.LANCZOS)

                        # ê²€ì€ìƒ‰ ë°°ê²½ì— ì¤‘ì•™ ë°°ì¹˜
                        final_img = PILImage.new('RGB', (target_width, target_height), (0, 0, 0))
                        paste_x = (target_width - display_width) // 2
                        paste_y = 0
                        final_img.paste(resized_img, (paste_x, paste_y))
                        img_array = np.array(final_img)
                        clip = ImageClip(img_array, duration=clip_duration)
                    else:
                        # ê°€ë¡œí˜• ì´ë¯¸ì§€: ê¸°ì¡´ ë¡œì§ (í•´ìƒë„ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ)
                        img = img.resize(self.resolution, PILImage.Resampling.LANCZOS)
                        img_array = np.array(img)

                        # shape í™•ì¸: (height, width, channels) í˜•ì‹ì´ì–´ì•¼ í•¨
                        if len(img_array.shape) != 3 or img_array.shape[2] != 3:
                            img = PILImage.fromarray(img_array).convert('RGB')
                            img_array = np.array(img)
                        clip = ImageClip(img_array, duration=clip_duration)
                except Exception as e:
                    self.logger.warning(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ ({Path(image_path).name}): {e}, ê¸°ë³¸ ë°©ë²• ì‚¬ìš©")
                    try:
                        # ì˜ˆì™¸ ì²˜ë¦¬: ì„¸ë¡œí˜• ì´ë¯¸ì§€ ì²˜ë¦¬ í¬í•¨
                        img = PILImage.open(image_path)
                        if img.mode != 'RGB':
                            img = img.convert('RGB')

                        img_width, img_height = img.size
                        target_width, target_height = self.resolution
                        aspect_ratio = target_width / target_height
                        img_aspect = img_width / img_height
                        is_portrait = img_aspect < aspect_ratio

                        if is_portrait:
                            display_height = target_height
                            display_width = int(display_height * img_aspect)
                            resized_img = img.resize((display_width, display_height), PILImage.Resampling.LANCZOS)
                            final_img = PILImage.new('RGB', (target_width, target_height), (0, 0, 0))
                            paste_x = (target_width - display_width) // 2
                            paste_y = 0
                            final_img.paste(resized_img, (paste_x, paste_y))
                            clip = ImageClip(np.array(final_img), duration=clip_duration)
                        else:
                            clip = ImageClip(image_path, duration=clip_duration)
                            clip = clip.resized(newsize=self.resolution)
                    except Exception:
                        # ìµœí›„ì˜ ìˆ˜ë‹¨: ì„¸ë¡œí˜• ì´ë¯¸ì§€ ì²˜ë¦¬ í¬í•¨
                        img = PILImage.open(image_path).convert('RGB')
                        img_width, img_height = img.size
                        target_width, target_height = self.resolution
                        aspect_ratio = target_width / target_height
                        img_aspect = img_width / img_height
                        is_portrait = img_aspect < aspect_ratio

                        if is_portrait:
                            display_height = target_height
                            display_width = int(display_height * img_aspect)
                            resized_img = img.resize((display_width, display_height), PILImage.Resampling.LANCZOS)
                            final_img = PILImage.new('RGB', (target_width, target_height), (0, 0, 0))
                            paste_x = (target_width - display_width) // 2
                            paste_y = 0
                            final_img.paste(resized_img, (paste_x, paste_y))
                            clip = ImageClip(np.array(final_img), duration=clip_duration)
                        else:
                            img = img.resize(self.resolution, PILImage.Resampling.LANCZOS)
                            clip = ImageClip(np.array(img), duration=clip_duration)
            
            # fade out/in ì „í™˜ íš¨ê³¼ ì ìš©
            # ëª¨ë“  ì´ë¯¸ì§€ì— fade outê³¼ fade inì„ ëª¨ë‘ ì ìš©í•˜ì—¬ í¬ë¡œìŠ¤í˜ì´ë“œ íš¨ê³¼
            if MOVIEPY_AVAILABLE:
                if MOVIEPY_VERSION_NEW:
                    # MoviePy 1.0+ ë²„ì „
                    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë©´ fade in ì ìš©
                    # ë§ˆì§€ë§‰ ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë©´ fade out ì ìš©
                    # (ë°˜ë³µì´ë¯€ë¡œ ëª¨ë“  ì´ë¯¸ì§€ì— ì–‘ìª½ ëª¨ë‘ ì ìš©)
                    
                    # fade in: ì´ì „ ì´ë¯¸ì§€ì—ì„œ ì „í™˜ë  ë•Œ (ì²« ë²ˆì§¸ê°€ ì•„ë‹ˆë©´)
                    # fade out: ë‹¤ìŒ ì´ë¯¸ì§€ë¡œ ì „í™˜ë  ë•Œ (ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´)
                    is_first = (current_time == 0.0)
                    is_last = (current_time + clip_duration >= total_duration)
                    
                    # fade íš¨ê³¼ ì ìš© ì „ì— í´ë¦½ í¬ê¸° í™•ì¸ ë° ìˆ˜ì •
                    try:
                        # í´ë¦½ì˜ ì²« í”„ë ˆì„ì„ ê°€ì ¸ì™€ì„œ í¬ê¸° í™•ì¸
                        test_frame = clip.get_frame(0)
                        if len(test_frame.shape) == 3:
                            # RGB ì´ë¯¸ì§€ì¸ ê²½ìš°
                            expected_shape = (self.resolution[1], self.resolution[0], 3)
                            if test_frame.shape != expected_shape:
                                # í¬ê¸°ê°€ ë§ì§€ ì•Šìœ¼ë©´ ë¦¬ì‚¬ì´ì¦ˆ
                                clip = clip.resized(newsize=self.resolution)
                    except:
                        # í¬ê¸° í™•ì¸ ì‹¤íŒ¨ ì‹œ ë¦¬ì‚¬ì´ì¦ˆ ì‹œë„
                        try:
                            clip = clip.resized(newsize=self.resolution)
                        except:
                            pass
                    
                    if not is_first:
                        # fade in ì ìš©
                        try:
                            clip = clip.fx(fadein, fade_duration)
                        except Exception as e:
                            self.logger.warning(f"fade in ì ìš© ì‹¤íŒ¨: {e}, fade íš¨ê³¼ ì—†ì´ ì§„í–‰")
                    if not is_last:
                        # fade out ì ìš©
                        try:
                            clip = clip.fx(fadeout, fade_duration)
                        except Exception as e:
                            self.logger.warning(f"fade out ì ìš© ì‹¤íŒ¨: {e}, fade íš¨ê³¼ ì—†ì´ ì§„í–‰")
                else:
                    # êµ¬ë²„ì „ í˜¸í™˜ì„±
                    try:
                        if current_time > 0:
                            clip = clip.with_effects([FadeIn(fade_duration)])
                        if (current_time + clip_duration) < total_duration:
                            clip = clip.with_effects([FadeOut(fade_duration)])
                    except:
                        # í˜ì´ë“œ íš¨ê³¼ ì—†ì´ ì§„í–‰
                        pass
            
            clips.append(clip)
            current_time += clip_duration
            image_index += 1  # ë‹¤ìŒ ì´ë¯¸ì§€ë¡œ ì´ë™ (ìˆœí™˜)
        
        self.logger.info(f"âœ… ì´ {len(clips)}ê°œì˜ í´ë¦½ ìƒì„± ì™„ë£Œ")
        return clips
    
    def generate_subtitles(self, audio_path: str, language: str = "ko") -> Optional[List[dict]]:
        """
        OpenAI Whisperë¡œ ìë§‰ ìƒì„±
        
        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            language: ì–¸ì–´ ì½”ë“œ ("ko", "en" ë“±)
            
        Returns:
            ìë§‰ ë¦¬ìŠ¤íŠ¸ [{"start": float, "end": float, "text": str}, ...]
        """
        if not WHISPER_AVAILABLE:
            self.logger.warning("Whisperê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìë§‰ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
        
        self.logger.info("ğŸ“ ìë§‰ ìƒì„± ì¤‘ (Whisper)...")
        try:
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
            audio_file = Path(audio_path)
            if not audio_file.exists():
                self.logger.error(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
                return None
            
            self.logger.info(f"ğŸ“ ì˜¤ë””ì˜¤ íŒŒì¼: {audio_file.name}")
            model = whisper.load_model("base")
            result = model.transcribe(str(audio_path), language=language)
            
            if not result or "segments" not in result:
                self.logger.warning("Whisper ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return None
            
            subtitles = []
            for segment in result.get("segments", []):
                subtitles.append({
                    "start": segment["start"],
                    "end": segment["end"],
                    "text": segment["text"].strip()
                })
            
            self.logger.info(f"âœ… {len(subtitles)}ê°œì˜ ìë§‰ ìƒì„± ì™„ë£Œ")
            return subtitles
            
        except Exception as e:
            self.logger.error(f"ìë§‰ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def generate_subtitles_from_text(
        self,
        text: str,
        audio_duration: float,
        language: str = "ko",
        audio_path: Optional[str] = None
    ) -> Optional[List[dict]]:
        """
        Summary í…ìŠ¤íŠ¸ì™€ ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ìë§‰ ìƒì„±
        ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì œê³µë˜ë©´ Whisperë¡œ ì •í™•í•œ íƒ€ì´ë°ì„ ë¶„ì„í•˜ê³ ,
        ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ë§¤ì¹­í•˜ì—¬ ìë§‰ ìƒì„±
        
        Args:
            text: Summary í…ìŠ¤íŠ¸
            audio_duration: ì˜¤ë””ì˜¤ ê¸¸ì´ (ì´ˆ)
            language: ì–¸ì–´ ì½”ë“œ ("ko", "en" ë“±)
            audio_path: ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ìˆìœ¼ë©´ Whisperë¡œ íƒ€ì´ë° ë¶„ì„)
            
        Returns:
            ìë§‰ ë¦¬ìŠ¤íŠ¸ [{"start": float, "end": float, "text": str}, ...]
        """
        import re
        from difflib import SequenceMatcher
        
        # ì˜¤ë””ì˜¤ íŒŒì¼ì´ ìˆìœ¼ë©´ Whisperë¡œ ì •í™•í•œ íƒ€ì´ë° ë¶„ì„
        if audio_path and Path(audio_path).exists() and WHISPER_AVAILABLE:
            self.logger.info("ğŸ“ ìë§‰ ìƒì„± ì¤‘ (Whisper ë‹¨ì–´ ë‹¨ìœ„ íƒ€ì´ë° ë¶„ì„)...")
            try:
                audio_file = Path(audio_path)
                if not audio_file.exists():
                    self.logger.error(f"ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
                    return None
                
                self.logger.info(f"ğŸ“ ì˜¤ë””ì˜¤ íŒŒì¼: {audio_file.name}")
                # Whisperë¡œ ì˜¤ë””ì˜¤ ë¶„ì„ (ë‹¨ì–´ ë‹¨ìœ„ íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
                model = whisper.load_model("base")
                result = model.transcribe(
                    str(audio_path), 
                    language=language,
                    word_timestamps=True  # ë‹¨ì–´ ë‹¨ìœ„ íƒ€ì„ìŠ¤íƒ¬í”„ í™œì„±í™”
                )
                
                if not result:
                    self.logger.warning("Whisper ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    return None
                
                # ì›ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬ (ë§ˆí¬ë‹¤ìš´ ì œê±°)
                cleaned_text = self._clean_markdown_text(text)
                
                # ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                original_sentences = self._split_sentences(cleaned_text, language)
                
                # Whisper ê²°ê³¼ì˜ ì²« ë²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ ì‹œê°„ í™•ì¸ (íƒ€ì´ë° ë³´ì •ìš©)
                segments = result.get("segments", [])
                time_offset = 0.0
                if segments:
                    first_segment_start = segments[0].get("start", 0.0)
                    if first_segment_start > 0.1:  # 0.1ì´ˆ ì´ìƒ ì°¨ì´ë‚˜ë©´ ë³´ì •
                        time_offset = first_segment_start
                        self.logger.info(f"â±ï¸ íƒ€ì´ë° ë³´ì •: ì²« ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ ì‹œê°„ {first_segment_start:.2f}ì´ˆë§Œí¼ ì¡°ì •")
                
                # Whisper ë‹¨ì–´ ë‹¨ìœ„ íƒ€ì„ìŠ¤íƒ¬í”„ ìˆ˜ì§‘ (íƒ€ì´ë° ë³´ì • ì ìš©)
                whisper_words = []
                for segment in segments:
                    if "words" in segment:
                        for word_info in segment["words"]:
                            # íƒ€ì´ë° ë³´ì •: ì²« ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ ì‹œê°„ë§Œí¼ ë¹¼ê¸°
                            adjusted_start = max(0.0, word_info["start"] - time_offset)
                            adjusted_end = max(0.0, word_info["end"] - time_offset)
                            whisper_words.append({
                                "word": word_info["word"].strip(),
                                "start": adjusted_start,
                                "end": adjusted_end
                            })
                
                if not whisper_words:
                    self.logger.warning("Whisper ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìŠµë‹ˆë‹¤. ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    # ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìœ¼ë©´ ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ë¡œ í´ë°± (íƒ€ì´ë° ë³´ì • ì ìš©)
                    whisper_segments = []
                    for segment in segments:
                        # íƒ€ì´ë° ë³´ì •: ì²« ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ ì‹œê°„ë§Œí¼ ë¹¼ê¸°
                        adjusted_start = max(0.0, segment["start"] - time_offset)
                        adjusted_end = max(0.0, segment["end"] - time_offset)
                        whisper_segments.append({
                            "start": adjusted_start,
                            "end": adjusted_end,
                            "text": segment["text"].strip()
                        })
                    subtitles = self._match_sentences_to_whisper(
                        original_sentences, 
                        whisper_segments, 
                        language
                    )
                    if subtitles:
                        self.logger.info(f"âœ… {len(subtitles)}ê°œì˜ ìë§‰ ìƒì„± ì™„ë£Œ (Whisper ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì´ë° ì‚¬ìš©)")
                        return subtitles
                    else:
                        return self._generate_subtitles_from_text_fallback(text, audio_duration, language)
                
                # ë‹¨ì–´ ë‹¨ìœ„ ì •ë ¬ì„ ì‚¬ìš©í•˜ì—¬ ìë§‰ ìƒì„±
                subtitles = self._align_words_to_sentences(
                    original_sentences,
                    whisper_words,
                    language
                )
                
                if subtitles:
                    # íƒ€ì´ë° ê²€ì¦ ë° ë³´ì •: ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡
                    subtitles = self._validate_and_adjust_subtitle_timing(subtitles, audio_duration)
                    self.logger.info(f"âœ… {len(subtitles)}ê°œì˜ ìë§‰ ìƒì„± ì™„ë£Œ (Whisper ë‹¨ì–´ ë‹¨ìœ„ íƒ€ì´ë° ì‚¬ìš©)")
                    return subtitles
                else:
                    self.logger.warning("ë‹¨ì–´ ì •ë ¬ ì‹¤íŒ¨. ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                    # í´ë°±: ì„¸ê·¸ë¨¼íŠ¸ ë‹¨ìœ„ ë§¤ì¹­ (íƒ€ì´ë° ë³´ì • ì ìš©)
                    whisper_segments = []
                    for segment in segments:
                        # íƒ€ì´ë° ë³´ì •: ì²« ì„¸ê·¸ë¨¼íŠ¸ ì‹œì‘ ì‹œê°„ë§Œí¼ ë¹¼ê¸°
                        adjusted_start = max(0.0, segment["start"] - time_offset)
                        adjusted_end = max(0.0, segment["end"] - time_offset)
                        whisper_segments.append({
                            "start": adjusted_start,
                            "end": adjusted_end,
                            "text": segment["text"].strip()
                        })
                    subtitles = self._match_sentences_to_whisper(
                        original_sentences, 
                        whisper_segments, 
                        language
                    )
                    if subtitles:
                        # íƒ€ì´ë° ê²€ì¦ ë° ë³´ì •: ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡
                        subtitles = self._validate_and_adjust_subtitle_timing(subtitles, audio_duration)
                        self.logger.info(f"âœ… {len(subtitles)}ê°œì˜ ìë§‰ ìƒì„± ì™„ë£Œ (Whisper ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì´ë° ì‚¬ìš©)")
                        return subtitles
                    else:
                        return self._generate_subtitles_from_text_fallback(text, audio_duration, language)
                    
            except Exception as e:
                self.logger.warning(f"Whisper ë¶„ì„ ì‹¤íŒ¨: {e}. í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                import traceback
                traceback.print_exc()
                # Whisper ì‹¤íŒ¨ ì‹œ í´ë°±
                return self._generate_subtitles_from_text_fallback(text, audio_duration, language)
        else:
            # ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ê±°ë‚˜ Whisperê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            if not audio_path:
                self.logger.info("ğŸ“ ìë§‰ ìƒì„± ì¤‘ (Summary í…ìŠ¤íŠ¸ ê¸°ë°˜, ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ)...")
            elif not WHISPER_AVAILABLE:
                self.logger.info("ğŸ“ ìë§‰ ìƒì„± ì¤‘ (Summary í…ìŠ¤íŠ¸ ê¸°ë°˜, Whisper ë¯¸ì„¤ì¹˜)...")
            return self._generate_subtitles_from_text_fallback(text, audio_duration, language)
    
    def _clean_markdown_text(self, text: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì œê±° ë° ë©”íƒ€ë°ì´í„° í•„í„°ë§"""
        import re
        
        # HTML ì£¼ì„ ì œê±° (<!-- -->)
        text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)
        
        # íŒŒì¼ ì‹œì‘ ë¶€ë¶„ì˜ ë©”íƒ€ë°ì´í„° ì œê±°
        lines = text.split('\n')
        cleaned_lines = []
        skip_metadata = True
        metadata_patterns = [
            r'^ğŸ“˜',  # ì±… ì´ëª¨ì§€
            r'^ğŸ“–',  # ì±… ì´ëª¨ì§€
            r'^TTS ê¸°ì¤€',
            r'^ì„œë¨¸ë¦¬ ìŠ¤í¬ë¦½íŠ¸',
            r'^Summary script',
            r'^TTS ê¸°ì¤€.*ì„œë¨¸ë¦¬',
            r'^TTS ê¸°ì¤€.*ìŠ¤í¬ë¦½íŠ¸',
            r'^.*ì•½.*ë¶„.*ì„œë¨¸ë¦¬',
            r'^.*ì•½.*ë¶„.*ìŠ¤í¬ë¦½íŠ¸',
        ]
        
        for i, line in enumerate(lines):
            # ë¹ˆ ì¤„ì´ ë‚˜ì˜¤ë©´ ë©”íƒ€ë°ì´í„° êµ¬ê°„ ì¢…ë£Œë¡œ ê°„ì£¼
            if skip_metadata and line.strip() == '':
                if i + 1 < len(lines) and lines[i + 1].strip():
                    skip_metadata = False
                    continue
            
            # ë©”íƒ€ë°ì´í„° êµ¬ê°„ì—ì„œëŠ” íŒ¨í„´ ë§¤ì¹­í•˜ì—¬ ì œê±°
            if skip_metadata:
                is_metadata = False
                for pattern in metadata_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        is_metadata = True
                        break
                
                # ì²« 3ì¤„ ë‚´ì—ì„œ ì €ì ì´ë¦„ì´ë‚˜ ì±… ì œëª©ë§Œ ìˆëŠ” ê²½ìš°ë„ ë©”íƒ€ë°ì´í„°ë¡œ ê°„ì£¼
                if i < 3 and line.strip() and not any(tag in line for tag in ['[HOOK]', '[SUMMARY]', '[BRIDGE]', '[CLOSING]']):
                    if len(line.strip()) < 50 and not line.strip().startswith('['):
                        is_metadata = True
                
                if is_metadata:
                    continue
            
            cleaned_lines.append(line)
        
        text = '\n'.join(cleaned_lines)
        
        # êµ¬ì¡°ì  íƒœê·¸ ì œê±°
        text = re.sub(r'\[HOOK\s*â€“?\s*[^\]]*\]', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\[HOOK\]', '', text)
        text = re.sub(r'\[SUMMARY\s*â€“?\s*[^\]]*\]', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\[SUMMARY\]', '', text)
        text = re.sub(r'\[BRIDGE\s*â€“?\s*[^\]]*\]', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\[BRIDGE\]', '', text)
        text = re.sub(r'\[CLOSING\s*â€“?\s*[^\]]*\]', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\[CLOSING\]', '', text)
        
        # ê¸°íƒ€ êµ¬ì¡°ì  íƒœê·¸ ì œê±°
        text = re.sub(r'\[[^\]]+\]\s*$', '', text, flags=re.MULTILINE)
        
        text = re.sub(r'#+\s*', '', text)  # í—¤ë” ì œê±°
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)  # **ë³¼ë“œ** ì œê±°
        text = re.sub(r'\*([^*]+)\*', r'\1', text)  # *ì´íƒ¤ë¦­* ì œê±°
        text = re.sub(r'---+\s*', '\n', text)  # êµ¬ë¶„ì„  ì œê±°
        text = re.sub(r'^\s*[â‘ -â‘³]\s*', '', text, flags=re.MULTILINE)  # ë²ˆí˜¸ ê¸°í˜¸ ì œê±°
        text = re.sub(r'^\s*[0-9]+\.\s*', '', text, flags=re.MULTILINE)  # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸ ì œê±°
        text = re.sub(r'^\s*[-*]\s*', '', text, flags=re.MULTILINE)  # ë¦¬ìŠ¤íŠ¸ ë§ˆì»¤ ì œê±°
        text = re.sub(r'ã€([^ã€]+)ã€', r'"\1"', text)  # ã€ã€ë¥¼ ""ë¡œ ë³€í™˜
        text = re.sub(r'ã€Œ([^ã€]+)ã€', r'"\1"', text)  # ã€Œã€ë¥¼ ""ë¡œ ë³€í™˜
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
        return text.strip()
    
    def _split_sentences(self, text: str, language: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        import re
        if language == "ko":
            text = re.sub(r'\n+', ' ', text)
            sentences = re.split(r'([.!?ã€‚ï¼ï¼Ÿ]\s+)', text)
            sentences = [sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '') 
                         for i in range(0, len(sentences), 2) if sentences[i].strip()]
        else:
            text = re.sub(r'\n+', ' ', text)
            sentences = re.split(r'([.!?]\s+)', text)
            sentences = [sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '') 
                         for i in range(0, len(sentences), 2) if sentences[i].strip()]
        return [s.strip() for s in sentences if s.strip() and len(s.strip()) > 5]
    
    def _align_words_to_sentences(
        self,
        original_sentences: List[str],
        whisper_words: List[dict],
        language: str
    ) -> Optional[List[dict]]:
        """
        ë‹¨ì–´ ë‹¨ìœ„ ì •ë ¬ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ë³„ ìë§‰ ìƒì„±
        ì›ë³¸ ë¬¸ì¥ì˜ ë‹¨ì–´ë“¤ì„ Whisper ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ë§¤ì¹­
        """
        import re
        from difflib import SequenceMatcher
        
        subtitles = []
        whisper_word_idx = 0
        
        # Whisper ë‹¨ì–´ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ë§¤ì¹­ìš©)
        whisper_text = ' '.join([w["word"] for w in whisper_words])
        
        for orig_sentence in original_sentences:
            if whisper_word_idx >= len(whisper_words):
                break
            
            # ì›ë³¸ ë¬¸ì¥ì„ ë‹¨ì–´ë¡œ ë¶„í• 
            if language == "ko":
                # í•œêµ­ì–´: ê³µë°±ê³¼ êµ¬ë‘ì ìœ¼ë¡œ ë¶„í• 
                orig_words = re.findall(r'\S+', orig_sentence.lower())
            else:
                # ì˜ì–´: ê³µë°±ìœ¼ë¡œ ë¶„í• 
                orig_words = [w.lower().strip('.,!?;:') for w in orig_sentence.split() if w.strip()]
            
            if not orig_words:
                continue
            
            # í˜„ì¬ ìœ„ì¹˜ë¶€í„° ì‹œì‘í•˜ì—¬ ì›ë³¸ ë¬¸ì¥ì˜ ë‹¨ì–´ë“¤ì„ ì°¾ê¸°
            matched_word_indices = []
            search_start = whisper_word_idx
            
            # ê° ì›ë³¸ ë‹¨ì–´ë¥¼ Whisper ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
            for orig_word in orig_words:
                # ì›ë³¸ ë‹¨ì–´ ì •ë¦¬ (êµ¬ë‘ì  ì œê±°)
                clean_orig_word = re.sub(r'[^\w\s]', '', orig_word.lower())
                if not clean_orig_word:
                    continue
                
                # í˜„ì¬ ìœ„ì¹˜ë¶€í„° ìµœëŒ€ 20ê°œ ë‹¨ì–´ê¹Œì§€ ê²€ìƒ‰
                best_match_idx = -1
                best_similarity = 0.0
                
                for i in range(search_start, min(search_start + 20, len(whisper_words))):
                    whisper_word = re.sub(r'[^\w\s]', '', whisper_words[i]["word"].lower())
                    
                    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
                    if clean_orig_word == whisper_word:
                        best_match_idx = i
                        best_similarity = 1.0
                        break
                    
                    # ìœ ì‚¬ë„ ê³„ì‚°
                    sim = SequenceMatcher(None, clean_orig_word, whisper_word).ratio()
                    if sim > best_similarity and sim > 0.6:  # 60% ì´ìƒ ìœ ì‚¬ë„
                        best_similarity = sim
                        best_match_idx = i
                
                if best_match_idx >= 0:
                    matched_word_indices.append(best_match_idx)
                    search_start = best_match_idx + 1
                else:
                    # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ë‹¨ì–´ë¡œ ë„˜ì–´ê°
                    continue
            
            # ë§¤ì¹­ëœ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ìë§‰ ìƒì„±
            if matched_word_indices:
                # ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ë‹¨ì–´ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©
                start_time = whisper_words[matched_word_indices[0]]["start"]
                end_time = whisper_words[matched_word_indices[-1]]["end"]
                
                subtitles.append({
                    "start": start_time,
                    "end": end_time,
                    "text": orig_sentence  # ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                })
                
                # ë‹¤ìŒ ë¬¸ì¥ì„ ìœ„í•´ ë§ˆì§€ë§‰ ë§¤ì¹­ ë‹¨ì–´ ë‹¤ìŒìœ¼ë¡œ ì´ë™
                whisper_word_idx = matched_word_indices[-1] + 1
            else:
                # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ í˜„ì¬ ìœ„ì¹˜ì˜ Whisper ë‹¨ì–´ ì‹œê°„ ì‚¬ìš©
                if whisper_word_idx < len(whisper_words):
                    # ë‹¤ìŒ ëª‡ ê°œ ë‹¨ì–´ì˜ ì‹œê°„ ë²”ìœ„ ì‚¬ìš©
                    end_idx = min(whisper_word_idx + len(orig_words), len(whisper_words) - 1)
                    start_time = whisper_words[whisper_word_idx]["start"]
                    end_time = whisper_words[end_idx]["end"]
                    
                    subtitles.append({
                        "start": start_time,
                        "end": end_time,
                        "text": orig_sentence
                    })
                    
                    whisper_word_idx = end_idx + 1
        
        return subtitles if subtitles else None
    
    def _match_sentences_to_whisper(
        self, 
        original_sentences: List[str], 
        whisper_segments: List[dict],
        language: str
    ) -> Optional[List[dict]]:
        """ì›ë³¸ ë¬¸ì¥ê³¼ Whisper ê²°ê³¼ë¥¼ ë§¤ì¹­í•˜ì—¬ ìë§‰ ìƒì„±"""
        from difflib import SequenceMatcher
        
        subtitles = []
        whisper_text = ' '.join([seg["text"] for seg in whisper_segments])
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ ì „ì²´ì™€ Whisper í…ìŠ¤íŠ¸ ì „ì²´ì˜ ìœ ì‚¬ë„ í™•ì¸
        original_full = ' '.join(original_sentences)
        similarity = SequenceMatcher(None, original_full.lower(), whisper_text.lower()).ratio()
        
        if similarity < 0.3:  # ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ë§¤ì¹­ ì‹¤íŒ¨
            return None
        
        # ê° ì›ë³¸ ë¬¸ì¥ì„ Whisper ì„¸ê·¸ë¨¼íŠ¸ì™€ ë§¤ì¹­
        whisper_idx = 0
        for orig_sentence in original_sentences:
            if whisper_idx >= len(whisper_segments):
                break
            
            # í˜„ì¬ Whisper ì„¸ê·¸ë¨¼íŠ¸ë¶€í„° ì‹œì‘í•˜ì—¬ ë§¤ì¹­ ì‹œë„
            best_match_idx = whisper_idx
            best_similarity = 0.0
            best_end_idx = whisper_idx
            
            # ì—¬ëŸ¬ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ í•©ì³ì„œ ë§¤ì¹­ ì‹œë„ (ìµœëŒ€ 5ê°œ ì„¸ê·¸ë¨¼íŠ¸ê¹Œì§€)
            for end_idx in range(whisper_idx, min(whisper_idx + 5, len(whisper_segments))):
                combined_whisper = ' '.join([
                    whisper_segments[i]["text"] 
                    for i in range(whisper_idx, end_idx + 1)
                ])
                sim = SequenceMatcher(
                    None, 
                    orig_sentence.lower(), 
                    combined_whisper.lower()
                ).ratio()
                
                if sim > best_similarity:
                    best_similarity = sim
                    best_end_idx = end_idx
            
            # ìœ ì‚¬ë„ê°€ 0.4 ì´ìƒì´ë©´ ë§¤ì¹­ ì„±ê³µ
            if best_similarity >= 0.4:
                start_time = whisper_segments[whisper_idx]["start"]
                end_time = whisper_segments[best_end_idx]["end"]
                
                subtitles.append({
                    "start": start_time,
                    "end": end_time,
                    "text": orig_sentence  # ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                })
                
                whisper_idx = best_end_idx + 1
            else:
                # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì‚¬ìš©í•˜ê³  ë‹¤ìŒìœ¼ë¡œ
                if whisper_idx < len(whisper_segments):
                    subtitles.append({
                        "start": whisper_segments[whisper_idx]["start"],
                        "end": whisper_segments[whisper_idx]["end"],
                        "text": orig_sentence  # ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
                    })
                    whisper_idx += 1
        
        return subtitles if subtitles else None
    
    def _validate_and_adjust_subtitle_timing(
        self,
        subtitles: List[dict],
        audio_duration: float
    ) -> List[dict]:
        """
        ìë§‰ íƒ€ì´ë° ê²€ì¦ ë° ë³´ì •
        - ì˜¤ë””ì˜¤ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ì¡°ì •
        - ìŒìˆ˜ íƒ€ì´ë° ì œê±°
        - íƒ€ì´ë° ìˆœì„œ ì •ë ¬
        """
        if not subtitles:
            return subtitles
        
        validated_subtitles = []
        for subtitle in subtitles:
            start = max(0.0, subtitle.get("start", 0.0))
            end = min(audio_duration, subtitle.get("end", audio_duration))
            
            # ì‹œì‘ ì‹œê°„ì´ ë ì‹œê°„ë³´ë‹¤ í¬ë©´ ìŠ¤ì™‘
            if start > end:
                start, end = end, start
            
            # ìµœì†Œ ìë§‰ ê¸¸ì´ í™•ì¸ (0.5ì´ˆ)
            if end - start < 0.5:
                end = start + 0.5
                if end > audio_duration:
                    end = audio_duration
                    start = max(0.0, end - 0.5)
            
            validated_subtitles.append({
                "start": start,
                "end": end,
                "text": subtitle.get("text", "")
            })
        
        # ì‹œì‘ ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
        validated_subtitles.sort(key=lambda x: x["start"])
        
        # ì¤‘ë³µ ì œê±° ë° ê²¹ì¹˜ëŠ” ìë§‰ ë³‘í•©
        merged_subtitles = []
        for subtitle in validated_subtitles:
            if not merged_subtitles:
                merged_subtitles.append(subtitle)
            else:
                last = merged_subtitles[-1]
                # ì´ì „ ìë§‰ê³¼ ê²¹ì¹˜ê±°ë‚˜ ë„ˆë¬´ ê°€ê¹Œìš°ë©´ ë³‘í•©
                if subtitle["start"] <= last["end"] + 0.3:
                    last["end"] = max(last["end"], subtitle["end"])
                    last["text"] = last["text"] + " " + subtitle["text"]
                else:
                    merged_subtitles.append(subtitle)
        
        return merged_subtitles
    
    def _generate_subtitles_from_text_fallback(
        self,
        text: str,
        audio_duration: float,
        language: str = "ko"
    ) -> Optional[List[dict]]:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ìë§‰ ìƒì„± (í´ë°± ë°©ì‹)"""
        import re
        
        try:
            # í…ìŠ¤íŠ¸ ì •ë¦¬
            cleaned_text = self._clean_markdown_text(text)
            
            # ë¬¸ì¥ ë¶„í• 
            sentences = self._split_sentences(cleaned_text, language)
            
            if not sentences:
                self.logger.warning("ë¬¸ì¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ê° ë¬¸ì¥ì˜ ê¸¸ì´ì— ë¹„ë¡€í•˜ì—¬ ì‹œê°„ í• ë‹¹
            total_chars = sum(len(s) for s in sentences)
            if total_chars == 0:
                return None
            
            subtitles = []
            current_time = 0.0
            
            # ê° ë¬¸ì¥ì˜ ê¸°ë³¸ ì‹œê°„ ê³„ì‚°
            sentence_durations = []
            for sentence in sentences:
                base_duration = (len(sentence) / total_chars) * audio_duration
                min_duration = 2.0
                max_duration = 8.0
                base_duration = max(min_duration, min(max_duration, base_duration))
                sentence_durations.append(base_duration)
            
            # ì „ì²´ ì˜ˆìƒ ì‹œê°„ ê³„ì‚°
            total_estimated = sum(sentence_durations)
            
            # ì‹¤ì œ ì˜¤ë””ì˜¤ ì‹œê°„ê³¼ì˜ ë¹„ìœ¨ ê³„ì‚°
            if total_estimated > 0:
                time_ratio = audio_duration / total_estimated
            else:
                time_ratio = 1.0
            
            # ë¹„ìœ¨ì„ ì ìš©í•˜ì—¬ ì‹œê°„ í• ë‹¹
            for i, (sentence, base_duration) in enumerate(zip(sentences, sentence_durations)):
                sentence_duration = base_duration * time_ratio
                
                if i == len(sentences) - 1:
                    end_time = audio_duration
                else:
                    end_time = current_time + sentence_duration
                    if end_time > audio_duration:
                        end_time = audio_duration
                
                subtitles.append({
                    "start": current_time,
                    "end": end_time,
                    "text": sentence
                })
                
                current_time = end_time
                
                if current_time >= audio_duration:
                    break
            
            self.logger.info(f"âœ… {len(subtitles)}ê°œì˜ ìë§‰ ìƒì„± ì™„ë£Œ (í…ìŠ¤íŠ¸ ê¸°ë°˜)")
            return subtitles
            
        except Exception as e:
            self.logger.error(f"ìë§‰ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def add_subtitles(
        self,
        video_clip: CompositeVideoClip,
        subtitles: List[dict],
        font_size: int = 70,  # ê°œì„ : 60 -> 70 (ê°€ë…ì„± í–¥ìƒ)
        font_color: str = "white",
        stroke_color: str = "black",
        stroke_width: int = 3,  # ê°œì„ : 2 -> 3 (ê°€ë…ì„± í–¥ìƒ)
        language: str = "ko"
    ) -> CompositeVideoClip:
        """
        ìë§‰ ì˜¤ë²„ë ˆì´ ì¶”ê°€ (PIL/Pillow ì‚¬ìš©í•˜ì—¬ ImageMagick ì—†ì´ ìë§‰ ìƒì„±)
        
        Args:
            video_clip: ë¹„ë””ì˜¤ í´ë¦½
            subtitles: ìë§‰ ë¦¬ìŠ¤íŠ¸
            font_size: í°íŠ¸ í¬ê¸°
            font_color: í°íŠ¸ ìƒ‰ìƒ
            stroke_color: í…Œë‘ë¦¬ ìƒ‰ìƒ
            stroke_width: í…Œë‘ë¦¬ ë‘ê»˜
            language: ì–¸ì–´ ì½”ë“œ ("ko", "en" ë“±)
        """
        if not subtitles:
            self.logger.warning("ìë§‰ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            return video_clip
        
        # ì–¸ì–´ë³„ í°íŠ¸ ê²½ë¡œ ì„¤ì •
        font_path = None
        if language == "ko":
            # macOS í•œê¸€ í°íŠ¸ ê²½ë¡œ
            korean_font_paths = [
                '/System/Library/Fonts/Supplemental/AppleGothic.ttf',
                '/System/Library/Fonts/AppleGothic.ttf',
                '/Library/Fonts/AppleGothic.ttf',
            ]
            for path in korean_font_paths:
                if os.path.exists(path):
                    font_path = path
                    break
        else:
            # ì˜ì–´ í°íŠ¸ ê²½ë¡œ
            english_font_paths = [
                '/System/Library/Fonts/Supplemental/Arial Bold.ttf',
                '/System/Library/Fonts/Supplemental/Arial.ttf',
                '/Library/Fonts/Arial.ttf',
            ]
            for path in english_font_paths:
                if os.path.exists(path):
                    font_path = path
                    break
        
        subtitle_clips = []
        failed_count = 0
        
        # PIL/Pillowë¥¼ ì‚¬ìš©í•˜ì—¬ ìë§‰ ì´ë¯¸ì§€ ìƒì„±
        try:
            from PIL import Image, ImageDraw, ImageFont
            PIL_AVAILABLE = True
        except ImportError:
            PIL_AVAILABLE = False
            self.logger.error("PIL/Pillowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install Pillow")
            return video_clip
        
        self.logger.info(f"ğŸ“ {len(subtitles)}ê°œì˜ ìë§‰ í´ë¦½ ìƒì„± ì¤‘ (PIL ì‚¬ìš©)...")
        
        # í°íŠ¸ ë¡œë“œ
        font_obj = None
        if font_path and os.path.exists(font_path):
            try:
                font_obj = ImageFont.truetype(font_path, font_size)
                self.logger.info(f"ğŸ“ í°íŠ¸ ì‚¬ìš©: {os.path.basename(font_path)}")
            except Exception as e:
                self.logger.warning(f"í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}, ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")
                font_obj = ImageFont.load_default()
        else:
            self.logger.warning("í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")
            font_obj = ImageFont.load_default()
        
        for i, subtitle in enumerate(subtitles):
            try:
                # ìë§‰ í…ìŠ¤íŠ¸
                text = subtitle["text"]
                duration = subtitle["end"] - subtitle["start"]
                
                # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
                temp_img = Image.new('RGB', (100, 100), (0, 0, 0))
                temp_draw = ImageDraw.Draw(temp_img)
                
                # í…ìŠ¤íŠ¸ê°€ í™”ë©´ ë„ˆë¹„ì— ë§ë„ë¡ ì¤„ë°”ê¿ˆ ì²˜ë¦¬
                max_width = self.resolution[0] - 200  # ì¢Œìš° ì—¬ë°± 100pxì”©
                words = text.split()
                lines = []
                current_line = []
                
                for word in words:
                    test_line = ' '.join(current_line + [word])
                    bbox = temp_draw.textbbox((0, 0), test_line, font=font_obj)
                    text_width = bbox[2] - bbox[0]
                    
                    if text_width <= max_width:
                        current_line.append(word)
                    else:
                        if current_line:
                            lines.append(' '.join(current_line))
                        current_line = [word]
                
                if current_line:
                    lines.append(' '.join(current_line))
                
                if not lines:
                    lines = [text]
                
                # ìë§‰ ì´ë¯¸ì§€ ìƒì„± (ê°œì„ : ë°°ê²½ ë°˜íˆ¬ëª… ë°•ìŠ¤ ì¶”ê°€)
                line_height = font_size + 15  # ê°œì„ : 10 -> 15 (ì¤„ ê°„ê²© ì¦ê°€)
                padding = 20  # ì¢Œìš° ì—¬ë°±
                img_height = len(lines) * line_height + padding * 2
                subtitle_img = Image.new('RGBA', (self.resolution[0], img_height), (0, 0, 0, 0))
                draw = ImageDraw.Draw(subtitle_img)
                
                # ë°°ê²½ ë°˜íˆ¬ëª… ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ê°€ë…ì„± í–¥ìƒ)
                box_margin = 50  # ì¢Œìš° ì—¬ë°±
                box_y_start = 10
                box_y_end = img_height - 10
                box_alpha = 180  # ë°˜íˆ¬ëª…ë„ (0-255, 180 = ì•½ 70% ë¶ˆíˆ¬ëª…)
                box_color = (0, 0, 0, box_alpha)
                draw.rectangle(
                    [(box_margin, box_y_start), (self.resolution[0] - box_margin, box_y_end)],
                    fill=box_color
                )
                
                # ê° ì¤„ ê·¸ë¦¬ê¸°
                y_offset = padding
                # ê°œì„ : ë” ë°ì€ í°ìƒ‰ ì‚¬ìš© (ê°€ë…ì„± í–¥ìƒ)
                bright_white = (255, 255, 255)
                for line in lines:
                    bbox = draw.textbbox((0, 0), line, font=font_obj)
                    text_width = bbox[2] - bbox[0]
                    x = (self.resolution[0] - text_width) // 2
                    
                    # í…Œë‘ë¦¬ ê·¸ë¦¬ê¸° (stroke íš¨ê³¼) - ê°œì„ : ë” ë‘êº¼ìš´ í…Œë‘ë¦¬
                    if stroke_width > 0:
                        for adj_x in range(-stroke_width, stroke_width + 1):
                            for adj_y in range(-stroke_width, stroke_width + 1):
                                if adj_x != 0 or adj_y != 0:
                                    draw.text((x + adj_x, y_offset + adj_y), line, font=font_obj, fill=stroke_color)
                    
                    # ë©”ì¸ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (ë°ì€ í°ìƒ‰ ì‚¬ìš©)
                    draw.text((x, y_offset), line, font=font_obj, fill=bright_white)
                    y_offset += line_height
                
                # PIL ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
                import numpy as np
                img_array = np.array(subtitle_img)
                
                # ImageClip ìƒì„±
                text_clip = ImageClip(img_array, duration=duration)
                
                # ìœ„ì¹˜ ì„¤ì • (í™”ë©´ í•˜ë‹¨ ì¤‘ì•™) - ê°œì„ : ì•½ê°„ ìœ„ë¡œ ì´ë™ (ê°€ë…ì„± í–¥ìƒ)
                y_position = self.resolution[1] - img_height - 80  # 50 -> 80 (ìœ„ë¡œ ì´ë™)
                # MoviePy ë²„ì „ í˜¸í™˜ì„±: with_start/set_start, with_position/set_position
                try:
                    text_clip = text_clip.with_start(subtitle["start"]).with_position(('center', y_position))
                except AttributeError:
                    # êµ¬ë²„ì „ í˜¸í™˜ì„±
                    text_clip = text_clip.set_start(subtitle["start"]).set_position(('center', y_position))
                
                subtitle_clips.append(text_clip)
                
                if (i + 1) % 10 == 0:
                    self.logger.info(f"{i + 1}/{len(subtitles)}ê°œ ìƒì„±ë¨...")
                    
            except Exception as e:
                failed_count += 1
                if failed_count <= 3:  # ì²˜ìŒ 3ê°œ ì˜¤ë¥˜ë§Œ ìƒì„¸ ì¶œë ¥
                    self.logger.warning(f"ìë§‰ ìƒì„± ì˜¤ë¥˜ ({i+1}ë²ˆì§¸): {e}")
                    self.logger.warning(f"í…ìŠ¤íŠ¸: {subtitle['text'][:50]}...")
                    import traceback
                    traceback.print_exc()
                continue
        
        if failed_count > 0:
            self.logger.warning(f"{failed_count}ê°œì˜ ìë§‰ ìƒì„± ì‹¤íŒ¨")
        
        if subtitle_clips:
            self.logger.info(f"âœ… {len(subtitle_clips)}ê°œì˜ ìë§‰ í´ë¦½ ìƒì„± ì™„ë£Œ")
            try:
                result = CompositeVideoClip([video_clip] + subtitle_clips)
                self.logger.info("âœ… ìë§‰ ì˜¤ë²„ë ˆì´ í•©ì„± ì™„ë£Œ")
                return result
            except Exception as e:
                self.logger.error(f"ìë§‰ ì˜¤ë²„ë ˆì´ í•©ì„± ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                return video_clip
        
        self.logger.warning("ìƒì„±ëœ ìë§‰ í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤")
        return video_clip
    
    def create_video(
        self,
        audio_path: str,
        image_dir: str,
        output_path: str,
        add_subtitles_flag: bool = False,
        language: str = "ko",
        max_duration: Optional[float] = None,
        summary_audio_path: Optional[str] = None,
        notebooklm_video_path: Optional[str] = None,
        summary_audio_volume: float = 1.2,
        summary_text: Optional[str] = None
    ) -> str:
        """
        ìµœì¢… ì˜ìƒ ìƒì„± (Summary -> NotebookLM Video ìˆœì„œ)
        
        Args:
            audio_path: (ì‚¬ìš© ì•ˆ í•¨, í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
            image_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            add_subtitles_flag: ìë§‰ ì¶”ê°€ ì—¬ë¶€
            language: ìë§‰ ì–¸ì–´
            max_duration: ìµœëŒ€ ê¸¸ì´ ì œí•œ (ì‚¬ìš© ì•ˆ í•¨)
            summary_audio_path: ìš”ì•½ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ìˆìœ¼ë©´ Summary ë¶€ë¶„ ìƒì„±)
            notebooklm_video_path: NotebookLM ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ìˆìœ¼ë©´ ì¤‘ê°„ì— ì‚½ì…)
            summary_audio_volume: Summary ì˜¤ë””ì˜¤ ìŒëŸ‰ ë°°ìœ¨ (ê¸°ë³¸ê°’: 1.2, 20% ì¦ê°€)
            summary_text: Summary í…ìŠ¤íŠ¸ (ìë§‰ ìƒì„±ìš©, ì„ íƒì‚¬í•­)
        """
        self.logger.info("=" * 60)
        self.logger.info("ğŸ¬ ì˜ìƒ ì œì‘ ì‹œì‘")
        self.logger.info("=" * 60)
        
        # ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘
        image_dir_path = Path(image_dir)
        if not image_dir_path.exists():
            raise FileNotFoundError(f"ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_dir}")
        
        cover_path = image_dir_path / "cover.jpg"
        mood_images = sorted(image_dir_path.glob("mood_*.jpg"))
        
        if not mood_images:
            raise FileNotFoundError(f"ë¬´ë“œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_dir}")
        
        image_paths = []
        
        # âš ï¸ í‘œì§€ ì´ë¯¸ì§€ëŠ” ì €ì‘ê¶Œ ë¬¸ì œë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        if cover_path.exists():
            self.logger.warning(f"í‘œì§€ ì´ë¯¸ì§€ ë°œê²¬: {cover_path.name}")
            self.logger.info("â†’ ì €ì‘ê¶Œ ë¬¸ì œë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬´ë“œ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        for mood_img in mood_images:
            image_paths.append(str(mood_img))
        
        if not image_paths:
            raise FileNotFoundError(f"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_dir}")
        
        self.logger.info(f"ğŸ¨ ë¬´ë“œ ì´ë¯¸ì§€: {len(image_paths)}ê°œ")
        
        video_clips = []
        
        # 1. Summary ë¶€ë¶„: ìš”ì•½ ì˜¤ë””ì˜¤ + ì´ë¯¸ì§€ ìŠ¬ë¼ì´ë“œì‡¼
        if summary_audio_path and Path(summary_audio_path).exists():
            self.logger.info("ğŸ“š 1ë‹¨ê³„: Summary ë¶€ë¶„ ì˜ìƒ ìƒì„±")
            self.logger.info("-" * 60)
            summary_audio = self.load_audio(summary_audio_path)
            summary_duration = summary_audio.duration
            
            # Summary ì˜¤ë””ì˜¤ ìŒëŸ‰ ì¡°ì •
            if summary_audio_volume != 1.0:
                self.logger.info(f"ğŸ”Š Summary ì˜¤ë””ì˜¤ ìŒëŸ‰ ì¡°ì •: {summary_audio_volume}x")
                try:
                    from moviepy.audio.fx.all import volumex
                    summary_audio = summary_audio.fx(volumex, summary_audio_volume)
                except ImportError:
                    try:
                        # êµ¬ë²„ì „ í˜¸í™˜ì„±
                        summary_audio = summary_audio.volumex(summary_audio_volume)
                    except AttributeError:
                        self.logger.warning("ìŒëŸ‰ ì¡°ì • ì‹¤íŒ¨, ì›ë³¸ ìŒëŸ‰ ì‚¬ìš©")
            
            self.logger.info(f"ìš”ì•½ ì˜¤ë””ì˜¤ ê¸¸ì´: {summary_duration:.2f}ì´ˆ")
            
            # Summary ë¶€ë¶„ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ ìƒì„±
            summary_image_clips = self.create_image_sequence(
                image_paths=image_paths,
                total_duration=summary_duration,
                fade_duration=1.5
            )
            summary_video = concatenate_videoclips(summary_image_clips, method="compose")
            summary_video = summary_video.set_audio(summary_audio)
            
            # ì˜ìƒ ì‹œê°í™” ê°œì„ : ë™ì  ìë§‰, íŒŒí˜• ë“± ì¶”ê°€ (ì •ì§€ í™”ë©´ ë°©ì–´)
            try:
                import os
                from src.utils.video_enhancements import enhance_video_with_visuals
                self.logger.info("ğŸ¨ ì˜ìƒ ì‹œê°í™” ê°œì„  ì ìš© ì¤‘...")
                self.logger.info("   - ë™ì  ìë§‰ (Kinetic Typography): í•µì‹¬ í‚¤ì›Œë“œ ê°•ì¡°")
                self.logger.info("   - íŒŒí˜• ì‹œê°í™”: ì˜¤ë””ì˜¤ ìŠ¤í™íŠ¸ëŸ¼ í‘œì‹œ")
                enable_waveform = os.getenv("ENABLE_WAVEFORM", "1").lower() not in ("0", "false", "no")
                
                summary_video = enhance_video_with_visuals(
                    video_clip=summary_video,
                    audio_path=summary_audio_path,
                    text=summary_text,
                    language=language,
                    enable_kinetic_typography=True,  # ë™ì  ìë§‰ í™œì„±í™”
                    enable_waveform=enable_waveform,  # ê¸°ë³¸ ON (ENABLE_WAVEFORM=0 ë¡œ ë„ê¸°)
                    enable_footage=False  # í‘¸í‹°ì§€ëŠ” ì„ íƒì‚¬í•­ (Pexels API í‚¤ í•„ìš”)
                )
                self.logger.info("âœ… ì˜ìƒ ì‹œê°í™” ê°œì„  ì™„ë£Œ")
            except ImportError as e:
                self.logger.warning(f"ì˜ìƒ ì‹œê°í™” ê°œì„  ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                self.logger.warning("ê¸°ë³¸ ì˜ìƒë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            except Exception as e:
                self.logger.warning(f"ì˜ìƒ ì‹œê°í™” ê°œì„  ì‹¤íŒ¨ (ê¸°ë³¸ ì˜ìƒ ì‚¬ìš©): {e}")
                import traceback
                traceback.print_exc()
            
            # Summary ë¶€ë¶„ì— ìë§‰ ì¶”ê°€ (í…ìŠ¤íŠ¸ê°€ ìˆê³  ìë§‰ ì˜µì…˜ì´ ì¼œì ¸ ìˆëŠ” ê²½ìš°)
            self.logger.info(f"ğŸ” ìë§‰ ì˜µì…˜ í™•ì¸: add_subtitles_flag={add_subtitles_flag}, summary_text={'ìˆìŒ' if summary_text else 'ì—†ìŒ'}")
            if add_subtitles_flag and summary_text:
                self.logger.info("ğŸ“ Summary ìë§‰ ìƒì„± ì¤‘...")
                summary_subtitles = self.generate_subtitles_from_text(
                    text=summary_text,
                    audio_duration=summary_duration,
                    language=language,
                    audio_path=summary_audio_path  # ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì „ë‹¬
                )
                if summary_subtitles:
                    self.logger.info(f"ğŸ“ {len(summary_subtitles)}ê°œì˜ ìë§‰ ìƒì„±ë¨")
                    self.logger.info("ğŸ“ Summary ìë§‰ ì˜¤ë²„ë ˆì´ ì¶”ê°€ ì¤‘...")
                    summary_video = self.add_subtitles(
                        summary_video,
                        summary_subtitles,
                        font_size=70,  # ê°œì„ : 60 -> 70
                        font_color="white",
                        stroke_color="black",
                        stroke_width=3,  # ê°œì„ : 2 -> 3
                        language=language
                    )
                    self.logger.info("âœ… Summary ìë§‰ ì¶”ê°€ ì™„ë£Œ")
                else:
                    self.logger.warning("ìë§‰ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ìë§‰")
            else:
                if not add_subtitles_flag:
                    self.logger.warning("ìë§‰ ì˜µì…˜ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
                if not summary_text:
                    self.logger.warning("Summary í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            video_clips.append(summary_video)
            self.logger.info(f"âœ… Summary ë¶€ë¶„ ì™„ë£Œ ({summary_duration:.2f}ì´ˆ)")
        else:
            self.logger.info("ğŸ“š Summary ë¶€ë¶„: ìš”ì•½ ì˜¤ë””ì˜¤ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # 2. NotebookLM Video ë¶€ë¶„
        if notebooklm_video_path and Path(notebooklm_video_path).exists():
            self.logger.info("ğŸ¥ 2ë‹¨ê³„: NotebookLM Video ë¶€ë¶„")
            self.logger.info("-" * 60)
            self.logger.info(f"ë¹„ë””ì˜¤ ë¡œë“œ ì¤‘: {Path(notebooklm_video_path).name}")
            
            notebooklm_video = VideoFileClip(notebooklm_video_path)
            
            # í•´ìƒë„ ë° í”„ë ˆì„ë ˆì´íŠ¸ í†µì¼
            if notebooklm_video.size != self.resolution:
                self.logger.info(f"ğŸ”„ ë¦¬ì‚¬ì´ì¦ˆ ì¤‘: {notebooklm_video.size} -> {self.resolution}")
                notebooklm_video = notebooklm_video.resize(self.resolution)
            
            if notebooklm_video.fps != self.fps:
                self.logger.info(f"ğŸ”„ í”„ë ˆì„ë ˆì´íŠ¸ ì¡°ì • ì¤‘: {notebooklm_video.fps}fps -> {self.fps}fps")
                notebooklm_video = notebooklm_video.set_fps(self.fps)
            
            video_clips.append(notebooklm_video)
            self.logger.info(f"âœ… NotebookLM Video ë¶€ë¶„ ì™„ë£Œ ({notebooklm_video.duration:.2f}ì´ˆ)")
        else:
            self.logger.info("ğŸ¥ NotebookLM Video ë¶€ë¶„: ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # 3. ë‘ ë¶€ë¶„ ì—°ê²° (ê° ì„¹ì…˜ ì‚¬ì´ì— 2ì´ˆ silence ì¶”ê°€)
        if not video_clips:
            raise ValueError("ìƒì„±í•  ì˜ìƒ í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì „í™˜ íš¨ê³¼ ê°•í™”: ì„¹ì…˜ ì „í™˜ ì‹œ ëª…í™•í•œ ì‹ í˜¸ ì œê³µ
        def create_transition_clip(duration: float = 1.5, section_name: str = ""):
            """
            ì„¹ì…˜ ì „í™˜ í´ë¦½ ìƒì„± (ê°œì„ : í˜ì´ë“œ íš¨ê³¼ ê°•í™”)
            - ê²€ì€ìƒ‰ ë°°ê²½ì— í˜ì´ë“œ ì¸/ì•„ì›ƒ íš¨ê³¼
            - ì„¹ì…˜ ì´ë¦„ í‘œì‹œ (ì„ íƒì‚¬í•­)
            """
            transition_video = ColorClip(size=self.resolution, color=(0, 0, 0), duration=duration)
            
            # ë¬´ìŒ ì˜¤ë””ì˜¤ ì¶”ê°€
            try:
                from moviepy.audio.AudioClip import AudioArrayClip
                import numpy as np
                sample_rate = 44100
                silence_array = np.zeros((int(sample_rate * duration), 2))
                silence_audio = AudioArrayClip(silence_array, fps=sample_rate)
                transition_video = transition_video.set_audio(silence_audio)
            except Exception as e:
                pass
            
            # í˜ì´ë“œ íš¨ê³¼ ì ìš© (ì „í™˜ ê°•í™”)
            if MOVIEPY_AVAILABLE and MOVIEPY_VERSION_NEW:
                try:
                    fade_duration = min(0.5, duration / 3)  # ì „í™˜ ì‹œê°„ì˜ 1/3, ìµœëŒ€ 0.5ì´ˆ
                    transition_video = transition_video.fx(fadein, fade_duration).fx(fadeout, fade_duration)
                except Exception as e:
                    self.logger.warning(f"ì „í™˜ í˜ì´ë“œ íš¨ê³¼ ì ìš© ì‹¤íŒ¨: {e}")
            
            return transition_video
        
        # ì„¹ì…˜ ì‚¬ì´ì— ì „í™˜ í´ë¦½ ì¶”ê°€ (ê°œì„ : í˜ì´ë“œ íš¨ê³¼ ê°•í™”)
        final_clips = []
        transition_duration = 1.5  # ê°œì„ : 2ì´ˆ -> 1.5ì´ˆ (ë” ë¹ ë¥¸ ì „í™˜)
        
        for i, clip in enumerate(video_clips):
            # í´ë¦½ ëì— í˜ì´ë“œ ì•„ì›ƒ íš¨ê³¼ ì¶”ê°€ (ì „í™˜ ê°•í™”)
            if MOVIEPY_AVAILABLE and MOVIEPY_VERSION_NEW:
                try:
                    fade_duration = 0.5  # 0.5ì´ˆ í˜ì´ë“œ ì•„ì›ƒ
                    clip = clip.fx(fadeout, fade_duration)
                except Exception as e:
                    self.logger.warning(f"í˜ì´ë“œ ì•„ì›ƒ íš¨ê³¼ ì ìš© ì‹¤íŒ¨: {e}")
            
            final_clips.append(clip)
            
            # ë§ˆì§€ë§‰ í´ë¦½ì´ ì•„ë‹ˆë©´ ì „í™˜ í´ë¦½ ì¶”ê°€
            if i < len(video_clips) - 1:
                section_names = ["Summary", "NotebookLM Analysis", "Review"]
                section_name = section_names[i] if i < len(section_names) else ""
                self.logger.info(f"ğŸ”„ ì „í™˜ íš¨ê³¼ ì¶”ê°€ ({transition_duration}ì´ˆ, ì„¹ì…˜: {section_name})...")
                transition_clip = create_transition_clip(transition_duration, section_name)
                final_clips.append(transition_clip)
                
                # ë‹¤ìŒ í´ë¦½ì— í˜ì´ë“œ ì¸ íš¨ê³¼ ì¶”ê°€ (ì „í™˜ ê°•í™”)
                if i + 1 < len(video_clips):
                    next_clip = video_clips[i + 1]
                    if MOVIEPY_AVAILABLE and MOVIEPY_VERSION_NEW:
                        try:
                            fade_duration = 0.5  # 0.5ì´ˆ í˜ì´ë“œ ì¸
                            next_clip = next_clip.fx(fadein, fade_duration)
                            video_clips[i + 1] = next_clip
                        except Exception as e:
                            self.logger.warning(f"í˜ì´ë“œ ì¸ íš¨ê³¼ ì ìš© ì‹¤íŒ¨: {e}")
        
        self.logger.info("ğŸ”— ì „ì²´ ì˜ìƒ ì—°ê²° ì¤‘...")
        self.logger.info(f"ì´ {len(final_clips)}ê°œ í´ë¦½ ì—°ê²° (ì„¹ì…˜ {len(video_clips)}ê°œ + ì „í™˜ {len(final_clips) - len(video_clips)}ê°œ)")
        for i, clip in enumerate(final_clips, 1):
            if i <= len(video_clips):
                self.logger.info(f"[{i}] {clip.duration:.2f}ì´ˆ (ì„¹ì…˜)")
            else:
                self.logger.info(f"[{i}] {clip.duration:.2f}ì´ˆ (ì „í™˜)")
        
        # í˜ì´ë“œ íš¨ê³¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²° (ê°œì„ : ì „í™˜ íš¨ê³¼ ê°•í™”)
        final_video = concatenate_videoclips(final_clips, method="compose")
        total_duration = final_video.duration
        self.logger.info(f"âœ… ì—°ê²° ì™„ë£Œ: ì´ ê¸¸ì´ {total_duration:.2f}ì´ˆ ({total_duration/60:.2f}ë¶„)")
        
        # 5. ìë§‰ ì¶”ê°€ (ì„ íƒì‚¬í•­)
        # Note: Summary ë¶€ë¶„ì˜ ìë§‰ì€ ì´ë¯¸ ìœ„ì—ì„œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.
        # ì „ì²´ ì˜ìƒì— ëŒ€í•œ ì¶”ê°€ ìë§‰ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì—¬ê¸°ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        # í˜„ì¬ëŠ” Summary ë¶€ë¶„ì—ë§Œ ìë§‰ì„ ì¶”ê°€í•˜ë¯€ë¡œ ì´ ë¶€ë¶„ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        
        # 6. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path_obj = Path(output_path)
        output_path_obj.parent.mkdir(parents=True, exist_ok=True)
        
        # 7. ë Œë”ë§
        self.logger.info("ğŸï¸ ì˜ìƒ ë Œë”ë§ ì¤‘...")
        self.logger.info(f"í•´ìƒë„: {self.resolution[0]}x{self.resolution[1]}")
        self.logger.info(f"í”„ë ˆì„ë ˆì´íŠ¸: {self.fps}fps")
        self.logger.info(f"ì´ ê¸¸ì´: {total_duration:.2f}ì´ˆ ({total_duration/60:.2f}ë¶„)")
        
        final_video.write_videofile(
            output_path,
            fps=self.fps,
            codec='libx264',
            audio_codec='aac',
            bitrate=self.bitrate,
            audio_bitrate=self.audio_bitrate,
            preset='medium'
        )
        
        self.logger.info("=" * 60)
        self.logger.info("âœ… ì˜ìƒ ì œì‘ ì™„ë£Œ!")
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_path}")
        
        # ì •ë¦¬
        final_video.close()
        if summary_audio_path and Path(summary_audio_path).exists():
            summary_audio.close()
        if notebooklm_video_path and Path(notebooklm_video_path).exists():
            notebooklm_video.close()
        
        return output_path


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ì±… ë¦¬ë·° ì˜ìƒ ì œì‘')
    parser.add_argument('--audio', type=str, help='ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--summary-audio', type=str, help='ìš”ì•½ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--book-title', type=str, help='ì±… ì œëª©')
    parser.add_argument('--image-dir', type=str, help='ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', type=str, help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--subtitles', action='store_true', help='ìë§‰ ì¶”ê°€ (Whisper)')
    parser.add_argument('--language', type=str, default='ko', help='ìë§‰ ì–¸ì–´ (ê¸°ë³¸ê°’: ko)')
    parser.add_argument('--max-duration', type=float, help='ìµœëŒ€ ì˜ìƒ ê¸¸ì´ (ì´ˆ, í…ŒìŠ¤íŠ¸ìš©)')
    parser.add_argument('--bitrate', type=str, default="5000k", help='ë¹„ë””ì˜¤ ë¹„íŠ¸ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: 5000k)')
    parser.add_argument('--audio-bitrate', type=str, default="320k", help='ì˜¤ë””ì˜¤ ë¹„íŠ¸ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: 320k)')
    
    args = parser.parse_args()
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    if args.audio is None:
        # ìë™ìœ¼ë¡œ ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
        audio_dir = Path("assets/audio")
        audio_files = list(audio_dir.glob("*.m4a")) + list(audio_dir.glob("*.wav")) + list(audio_dir.glob("*.mp3"))
        if audio_files:
            # í•œê¸€ ì˜¤ë””ì˜¤ ìš°ì„  ì„ íƒ (íŒŒì¼ëª…ì— í•œê¸€ì´ í¬í•¨ëœ ê²ƒ)
            korean_audio = [f for f in audio_files if any(ord(c) > 127 for c in f.stem)]
            if korean_audio:
                args.audio = str(korean_audio[0])
                print(f"ğŸ“ í•œê¸€ ì˜¤ë””ì˜¤ íŒŒì¼ ìë™ ì„ íƒ: {args.audio}")
            else:
                args.audio = str(audio_files[0])
                print(f"ğŸ“ ì˜¤ë””ì˜¤ íŒŒì¼ ìë™ ì„ íƒ: {args.audio}")
        else:
            print("âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
    
    if args.book_title is None:
        # ì˜¤ë””ì˜¤ íŒŒì¼ëª…ì—ì„œ ì±… ì œëª© ì¶”ì¶œ
        audio_name = Path(args.audio).stem
        args.book_title = audio_name.replace("_review", "").replace("_Review", "")
        print(f"ğŸ“š ì±… ì œëª© ìë™ ì¶”ì¶œ: {args.book_title}")
    
    if args.image_dir is None:
        from utils.file_utils import safe_title
        safe_title_str = safe_title(args.book_title)
        args.image_dir = f"assets/images/{safe_title_str}"
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {args.image_dir}")
    
    if args.output is None:
        from utils.file_utils import safe_title
        safe_title_str = safe_title(args.book_title)
        args.output = f"output/{safe_title_str}_review.mp4"
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {args.output}")
    
    # ì˜ìƒ ì œì‘
    maker = VideoMaker(
        resolution=(1920, 1080), 
        fps=30,
        bitrate=args.bitrate,
        audio_bitrate=args.audio_bitrate
    )
    maker.create_video(
        audio_path=args.audio,
        image_dir=args.image_dir,
        output_path=args.output,
        add_subtitles_flag=args.subtitles,
        language=args.language,
        max_duration=args.max_duration,
        summary_audio_path=args.summary_audio
    )


if __name__ == "__main__":
    main()

