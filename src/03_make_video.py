"""
Phase 4: ì˜ìƒ í•©ì„± ë° í¸ì§‘ ìŠ¤í¬ë¦½íŠ¸
- ì˜¤ë””ì˜¤ ë¡œë“œ
- ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ ìƒì„± (ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶°)
- Ken Burns íš¨ê³¼ (ì¤Œì¸/íŒ¨ë‹)
- ì „í™˜ íš¨ê³¼ (í˜ì´ë“œ)
- ìë§‰ (OpenAI Whisper, ì„ íƒì‚¬í•­)
- ë Œë”ë§: 1080p, 30fps MP4
"""

import os
import random
import math
from pathlib import Path
from typing import List, Optional, Tuple
from dotenv import load_dotenv

try:
    from moviepy import ImageClip, AudioFileClip, CompositeVideoClip, TextClip, ColorClip, concatenate_videoclips
    from moviepy.video.fx import FadeIn, FadeOut, CrossFadeIn, CrossFadeOut
    MOVIEPY_AVAILABLE = True
except ImportError as e:
    MOVIEPY_AVAILABLE = False
    print(f"âš ï¸ MoviePy import ì˜¤ë¥˜: {e}")
    print("pip install moviepy")

try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False

load_dotenv()


class VideoMaker:
    """ì˜ìƒ ì œì‘ í´ë˜ìŠ¤"""
    
    def __init__(self, resolution: Tuple[int, int] = (1920, 1080), fps: int = 30):
        """
        Args:
            resolution: í•´ìƒë„ (width, height)
            fps: í”„ë ˆì„ë ˆì´íŠ¸
        """
        self.resolution = resolution
        self.fps = fps
        
        if not MOVIEPY_AVAILABLE:
            raise ImportError("MoviePyê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install moviepy")
    
    def load_audio(self, audio_path: str) -> AudioFileClip:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ"""
        print(f"ğŸµ ì˜¤ë””ì˜¤ ë¡œë“œ ì¤‘: {audio_path}")
        audio = AudioFileClip(audio_path)
        print(f"   ê¸¸ì´: {audio.duration:.2f}ì´ˆ")
        return audio
    
    def create_image_clip_with_ken_burns(
        self,
        image_path: str,
        duration: float,
        effect_type: str = "zoom_in",
        start_scale: float = 1.0,
        end_scale: float = 1.2,
        pan_direction: Optional[str] = None
    ) -> ImageClip:
        """
        Ken Burns íš¨ê³¼ê°€ ì ìš©ëœ ì´ë¯¸ì§€ í´ë¦½ ìƒì„± (ê°„ë‹¨í•œ ë²„ì „)
        
        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
            duration: í´ë¦½ ê¸¸ì´ (ì´ˆ)
            effect_type: íš¨ê³¼ íƒ€ì… ("zoom_in", "zoom_out", "static")
            start_scale: ì‹œì‘ ìŠ¤ì¼€ì¼
            end_scale: ë ìŠ¤ì¼€ì¼
            pan_direction: íŒ¨ë‹ ë°©í–¥ (í˜„ì¬ ë¯¸êµ¬í˜„)
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        clip = ImageClip(image_path, duration=duration)
        
        # í•´ìƒë„ì— ë§ê²Œ ë¦¬ì‚¬ì´ì¦ˆ
        try:
            clip = clip.resized(newsize=self.resolution)
        except:
            # ëŒ€ì²´ ë°©ë²•
            from PIL import Image
            import numpy as np
            img = Image.open(image_path)
            img = img.resize(self.resolution, Image.Resampling.LANCZOS)
            clip = ImageClip(np.array(img), duration=duration)
        
        return clip
    
    def create_image_sequence(
        self,
        image_paths: List[str],
        total_duration: float,
        fade_duration: float = 1.0
    ) -> List[ImageClip]:
        """
        ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ ìƒì„± (ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶°)
        
        Args:
            image_paths: ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            total_duration: ì „ì²´ ê¸¸ì´ (ì˜¤ë””ì˜¤ ê¸¸ì´)
            fade_duration: í˜ì´ë“œ ì „í™˜ ì‹œê°„
        """
        if not image_paths:
            raise ValueError("ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        num_images = len(image_paths)
        duration_per_image = total_duration / num_images
        
        clips = []
        effect_types = ["zoom_in", "zoom_out", "pan"]
        pan_directions = ["left", "right", "up", "down"]
        
        for i, image_path in enumerate(image_paths):
            # ëœë¤ íš¨ê³¼ ì„ íƒ
            effect_type = random.choice(effect_types)
            pan_direction = random.choice(pan_directions) if effect_type == "pan" else None
            
            # Ken Burns íš¨ê³¼ ì ìš©
            clip = self.create_image_clip_with_ken_burns(
                image_path=image_path,
                duration=duration_per_image,
                effect_type=effect_type,
                start_scale=1.0,
                end_scale=1.15 + random.uniform(0, 0.1),
                pan_direction=pan_direction
            )
            
            # í˜ì´ë“œ íš¨ê³¼ ì¶”ê°€
            if i == 0:
                # ì²« ë²ˆì§¸: í˜ì´ë“œì¸
                clip = clip.with_effects([FadeIn(fade_duration)])
            elif i == len(image_paths) - 1:
                # ë§ˆì§€ë§‰: í˜ì´ë“œì•„ì›ƒ
                clip = clip.with_effects([FadeOut(fade_duration)])
            else:
                # ì¤‘ê°„: í¬ë¡œìŠ¤í˜ì´ë“œ
                clip = clip.with_effects([CrossFadeIn(fade_duration)])
            
            clips.append(clip)
        
        return clips
    
    def generate_subtitles(self, audio_path: str, language: str = "ko") -> Optional[List[dict]]:
        """
        OpenAI Whisperë¡œ ìë§‰ ìƒì„±
        
        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            language: ì–¸ì–´ ì½”ë“œ ("ko", "en" ë“±)
            
        Returns:
            ìë§‰ ë¦¬ìŠ¤íŠ¸ [{"start": float, "end": float, "text": str}, ...]
        """
        if not WHISPER_AVAILABLE:
            print("âš ï¸ Whisperê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìë§‰ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
        
        print("ğŸ“ ìë§‰ ìƒì„± ì¤‘ (Whisper)...")
        try:
            model = whisper.load_model("base")
            result = model.transcribe(audio_path, language=language)
            
            subtitles = []
            for segment in result.get("segments", []):
                subtitles.append({
                    "start": segment["start"],
                    "end": segment["end"],
                    "text": segment["text"].strip()
                })
            
            print(f"   âœ… {len(subtitles)}ê°œì˜ ìë§‰ ìƒì„± ì™„ë£Œ")
            return subtitles
            
        except Exception as e:
            print(f"   âŒ ìë§‰ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def add_subtitles(
        self,
        video_clip: CompositeVideoClip,
        subtitles: List[dict],
        font_size: int = 60,
        font_color: str = "white",
        stroke_color: str = "black",
        stroke_width: int = 2
    ) -> CompositeVideoClip:
        """
        ìë§‰ ì˜¤ë²„ë ˆì´ ì¶”ê°€
        
        Args:
            video_clip: ë¹„ë””ì˜¤ í´ë¦½
            subtitles: ìë§‰ ë¦¬ìŠ¤íŠ¸
            font_size: í°íŠ¸ í¬ê¸°
            font_color: í°íŠ¸ ìƒ‰ìƒ
            stroke_color: í…Œë‘ë¦¬ ìƒ‰ìƒ
            stroke_width: í…Œë‘ë¦¬ ë‘ê»˜
        """
        if not subtitles:
            return video_clip
        
        subtitle_clips = []
        
        for subtitle in subtitles:
            try:
                text_clip = TextClip(
                    subtitle["text"],
                    fontsize=font_size,
                    color=font_color,
                    stroke_color=stroke_color,
                    stroke_width=stroke_width,
                    method='caption',
                    size=(self.resolution[0] - 100, None),
                    align='center'
                ).with_duration(subtitle["end"] - subtitle["start"]).with_start(subtitle["start"]).with_position(('center', self.resolution[1] - 150))
                
                subtitle_clips.append(text_clip)
            except Exception as e:
                print(f"   âš ï¸ ìë§‰ ìƒì„± ì˜¤ë¥˜: {e}")
                continue
        
        if subtitle_clips:
            return CompositeVideoClip([video_clip] + subtitle_clips)
        
        return video_clip
    
    def create_video(
        self,
        audio_path: str,
        image_dir: str,
        output_path: str,
        add_subtitles_flag: bool = False,
        language: str = "ko"
    ) -> str:
        """
        ìµœì¢… ì˜ìƒ ìƒì„±
        
        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            image_dir: ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            add_subtitles_flag: ìë§‰ ì¶”ê°€ ì—¬ë¶€
            language: ìë§‰ ì–¸ì–´
        """
        print("=" * 60)
        print("ğŸ¬ ì˜ìƒ ì œì‘ ì‹œì‘")
        print("=" * 60)
        print()
        
        # 1. ì˜¤ë””ì˜¤ ë¡œë“œ
        audio = self.load_audio(audio_path)
        audio_duration = audio.duration
        print()
        
        # 2. ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘
        image_dir_path = Path(image_dir)
        cover_path = image_dir_path / "cover.jpg"
        mood_images = sorted(image_dir_path.glob("mood_*.jpg"))
        
        image_paths = []
        
        # âš ï¸ í‘œì§€ ì´ë¯¸ì§€ëŠ” ì €ì‘ê¶Œ ë¬¸ì œë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        # ì €ì‘ê¶Œ ì—†ëŠ” ë¬´ë“œ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        if cover_path.exists():
            print(f"âš ï¸ í‘œì§€ ì´ë¯¸ì§€ ë°œê²¬: {cover_path.name}")
            print("   â†’ ì €ì‘ê¶Œ ë¬¸ì œë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬´ë“œ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        for mood_img in mood_images:
            image_paths.append(str(mood_img))
            print(f"ğŸ¨ ë¬´ë“œ ì´ë¯¸ì§€ ì¶”ê°€: {mood_img.name}")
        
        if not image_paths:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_dir}")
        
        print(f"\nì´ {len(image_paths)}ê°œì˜ ì´ë¯¸ì§€ ì‚¬ìš©")
        print()
        
        # 3. ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ ìƒì„±
        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        image_clips = self.create_image_sequence(
            image_paths=image_paths,
            total_duration=audio_duration,
            fade_duration=1.0
        )
        print(f"   âœ… {len(image_clips)}ê°œì˜ í´ë¦½ ìƒì„± ì™„ë£Œ")
        print()
        
        # 4. í´ë¦½ ì—°ê²°
        print("ğŸ”— í´ë¦½ ì—°ê²° ì¤‘...")
        video = concatenate_videoclips(image_clips, method="compose")
        print("   âœ… ì—°ê²° ì™„ë£Œ")
        print()
        
        # 5. ì˜¤ë””ì˜¤ ì¶”ê°€
        print("ğŸµ ì˜¤ë””ì˜¤ ì¶”ê°€ ì¤‘...")
        video = video.with_audio(audio)
        print("   âœ… ì˜¤ë””ì˜¤ ì¶”ê°€ ì™„ë£Œ")
        print()
        
        # 6. ìë§‰ ì¶”ê°€ (ì„ íƒì‚¬í•­)
        subtitles = None
        if add_subtitles_flag:
            subtitles = self.generate_subtitles(audio_path, language)
            if subtitles:
                print("ğŸ“ ìë§‰ ì˜¤ë²„ë ˆì´ ì¶”ê°€ ì¤‘...")
                video = self.add_subtitles(video, subtitles)
                print("   âœ… ìë§‰ ì¶”ê°€ ì™„ë£Œ")
                print()
        
        # 7. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path_obj = Path(output_path)
        output_path_obj.parent.mkdir(parents=True, exist_ok=True)
        
        # 8. ë Œë”ë§
        print("ğŸï¸ ì˜ìƒ ë Œë”ë§ ì¤‘...")
        print(f"   í•´ìƒë„: {self.resolution[0]}x{self.resolution[1]}")
        print(f"   í”„ë ˆì„ë ˆì´íŠ¸: {self.fps}fps")
        print(f"   ê¸¸ì´: {audio_duration:.2f}ì´ˆ")
        print()
        
        video.write_videofile(
            output_path,
            fps=self.fps,
            codec='libx264',
            audio_codec='aac',
            bitrate='8000k',
            preset='medium'
        )
        
        print()
        print("=" * 60)
        print("âœ… ì˜ìƒ ì œì‘ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_path}")
        print()
        
        # ì •ë¦¬
        audio.close()
        video.close()
        
        return output_path


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ì±… ë¦¬ë·° ì˜ìƒ ì œì‘')
    parser.add_argument('--audio', type=str, help='ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--book-title', type=str, help='ì±… ì œëª©')
    parser.add_argument('--image-dir', type=str, help='ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', type=str, help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--subtitles', action='store_true', help='ìë§‰ ì¶”ê°€ (Whisper)')
    parser.add_argument('--language', type=str, default='ko', help='ìë§‰ ì–¸ì–´ (ê¸°ë³¸ê°’: ko)')
    
    args = parser.parse_args()
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    if args.audio is None:
        # ìë™ìœ¼ë¡œ ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
        audio_dir = Path("assets/audio")
        audio_files = list(audio_dir.glob("*.m4a")) + list(audio_dir.glob("*.wav")) + list(audio_dir.glob("*.mp3"))
        if audio_files:
            # í•œê¸€ ì˜¤ë””ì˜¤ ìš°ì„  ì„ íƒ (íŒŒì¼ëª…ì— í•œê¸€ì´ í¬í•¨ëœ ê²ƒ)
            korean_audio = [f for f in audio_files if any(ord(c) > 127 for c in f.stem)]
            if korean_audio:
                args.audio = str(korean_audio[0])
                print(f"ğŸ“ í•œê¸€ ì˜¤ë””ì˜¤ íŒŒì¼ ìë™ ì„ íƒ: {args.audio}")
            else:
                args.audio = str(audio_files[0])
                print(f"ğŸ“ ì˜¤ë””ì˜¤ íŒŒì¼ ìë™ ì„ íƒ: {args.audio}")
        else:
            print("âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
    
    if args.book_title is None:
        # ì˜¤ë””ì˜¤ íŒŒì¼ëª…ì—ì„œ ì±… ì œëª© ì¶”ì¶œ
        audio_name = Path(args.audio).stem
        args.book_title = audio_name.replace("_review", "").replace("_Review", "")
        print(f"ğŸ“š ì±… ì œëª© ìë™ ì¶”ì¶œ: {args.book_title}")
    
    if args.image_dir is None:
        safe_title = "".join(c for c in args.book_title if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_title = safe_title.replace(' ', '_')
        args.image_dir = f"assets/images/{safe_title}"
        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {args.image_dir}")
    
    if args.output is None:
        safe_title = "".join(c for c in args.book_title if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_title = safe_title.replace(' ', '_')
        args.output = f"output/{safe_title}_review.mp4"
        print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {args.output}")
    
    print()
    
    # ì˜ìƒ ì œì‘
    maker = VideoMaker(resolution=(1920, 1080), fps=30)
    maker.create_video(
        audio_path=args.audio,
        image_dir=args.image_dir,
        output_path=args.output,
        add_subtitles_flag=args.subtitles,
        language=args.language
    )


if __name__ == "__main__":
    main()

